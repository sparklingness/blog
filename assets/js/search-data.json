{
  
    
        "post0": {
            "title": "Inspect Bert Vocabulary",
            "content": "Load the model . Install the huggingface implementation. . !pip install pytorch-pretrained-bert . Requirement already satisfied: pytorch-pretrained-bert in /usr/local/anaconda3/lib/python3.6/site-packages (0.6.2) Requirement already satisfied: tqdm in /usr/local/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.26.0) Requirement already satisfied: regex in /usr/local/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2017.4.5) Requirement already satisfied: torch&gt;=0.4.1 in /usr/local/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (0.4.1) Requirement already satisfied: requests in /usr/local/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.19.1) Requirement already satisfied: boto3 in /usr/local/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.7.78) Requirement already satisfied: numpy in /usr/local/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.15.4) Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/anaconda3/lib/python3.6/site-packages (from requests-&gt;pytorch-pretrained-bert) (3.0.4) Requirement already satisfied: idna&lt;2.8,&gt;=2.5 in /usr/local/anaconda3/lib/python3.6/site-packages (from requests-&gt;pytorch-pretrained-bert) (2.7) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/anaconda3/lib/python3.6/site-packages (from requests-&gt;pytorch-pretrained-bert) (2019.3.9) Requirement already satisfied: urllib3&lt;1.24,&gt;=1.21.1 in /usr/local/anaconda3/lib/python3.6/site-packages (from requests-&gt;pytorch-pretrained-bert) (1.23) Requirement already satisfied: botocore&lt;1.11.0,&gt;=1.10.78 in /usr/local/anaconda3/lib/python3.6/site-packages (from boto3-&gt;pytorch-pretrained-bert) (1.10.78) Requirement already satisfied: s3transfer&lt;0.2.0,&gt;=0.1.10 in /usr/local/anaconda3/lib/python3.6/site-packages (from boto3-&gt;pytorch-pretrained-bert) (0.1.13) Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in /usr/local/anaconda3/lib/python3.6/site-packages (from boto3-&gt;pytorch-pretrained-bert) (0.9.3) Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1; python_version &gt;= &#34;2.7&#34; in /usr/local/anaconda3/lib/python3.6/site-packages (from botocore&lt;1.11.0,&gt;=1.10.78-&gt;boto3-&gt;pytorch-pretrained-bert) (2.6.1) Requirement already satisfied: docutils&gt;=0.10 in /usr/local/anaconda3/lib/python3.6/site-packages (from botocore&lt;1.11.0,&gt;=1.10.78-&gt;boto3-&gt;pytorch-pretrained-bert) (0.14) Requirement already satisfied: six&gt;=1.5 in /usr/local/anaconda3/lib/python3.6/site-packages (from python-dateutil&lt;3.0.0,&gt;=2.1; python_version &gt;= &#34;2.7&#34;-&gt;botocore&lt;1.11.0,&gt;=1.10.78-&gt;boto3-&gt;pytorch-pretrained-bert) (1.11.0) . import torch from pytorch_pretrained_bert import BertTokenizer # Load pre-trained model tokenizer (vocabulary) tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;) . Inspect BERT Vocabulary . Vocab Dump . Retrieve the entire list of &quot;tokens&quot; and write these out to text files so we can peruse them. . with open(&quot;vocabulary.txt&quot;, &#39;w&#39;) as f: for token in tokenizer.vocab.keys(): f.write(token+&#39; n&#39;) . Single Characters . one_chars = [] one_chars_hashes = [] # For each token in the vocabulary, for token in tokenizer.vocab.keys(): # Record any single-character tokens. if len(token) == 1: one_chars.append(token) # Record single-character tokens preceded by the two hashes. elif len(token) == 3 and token[0:2] == &#39;##&#39;: one_chars_hashes.append(token) . print(&#39;Number of single character tokens:&#39;, len(one_chars), &#39; n&#39;) # Print all of the single characters, 40 per row. # For every batch of 40 tokens, for i in range(0, len(one_chars), 40): # Limit the end index so we don&#39;t go past the end of the list. end = min(i+40, len(one_chars)+1) # Print out the tokens, seperated by a space. print(&#39; &#39;.join(one_chars[i:end])) . Number of single character tokens: 997 ! &#34; # $ % &amp; &#39; ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; &lt; = &gt; ? @ [ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z { | } ~ ¡ ¢ £ ¤ ¥ ¦ § ¨ © ª « ¬ ® ° ± ² ³ ´ µ ¶ · ¹ º » ¼ ½ ¾ ¿ × ß æ ð ÷ ø þ đ ħ ı ł ŋ œ ƒ ɐ ɑ ɒ ɔ ɕ ə ɛ ɡ ɣ ɨ ɪ ɫ ɬ ɯ ɲ ɴ ɹ ɾ ʀ ʁ ʂ ʃ ʉ ʊ ʋ ʌ ʎ ʐ ʑ ʒ ʔ ʰ ʲ ʳ ʷ ʸ ʻ ʼ ʾ ʿ ˈ ː ˡ ˢ ˣ ˤ α β γ δ ε ζ η θ ι κ λ μ ν ξ ο π ρ ς σ τ υ φ χ ψ ω а б в г д е ж з и к л м н о п р с т у ф х ц ч ш щ ъ ы ь э ю я ђ є і ј љ њ ћ ӏ ա բ գ դ ե թ ի լ կ հ մ յ ն ո պ ս վ տ ր ւ ք ־ א ב ג ד ה ו ז ח ט י ך כ ל ם מ ן נ ס ע ף פ ץ צ ק ר ש ת ، ء ا ب ة ت ث ج ح خ د ذ ر ز س ش ص ض ط ظ ع غ ـ ف ق ك ل م ن ه و ى ي ٹ پ چ ک گ ں ھ ہ ی ے अ आ उ ए क ख ग च ज ट ड ण त थ द ध न प ब भ म य र ल व श ष स ह ा ि ी ो । ॥ ং অ আ ই উ এ ও ক খ গ চ ছ জ ট ড ণ ত থ দ ধ ন প ব ভ ম য র ল শ ষ স হ া ি ী ে க ச ட த ந ன ப ம ய ர ல ள வ ா ி ு ே ை ನ ರ ಾ ක ය ර ල ව ා ก ง ต ท น พ ม ย ร ล ว ส อ า เ ་ ། ག ང ད ན པ བ མ འ ར ལ ས မ ა ბ გ დ ე ვ თ ი კ ლ მ ნ ო რ ს ტ უ ᄀ ᄂ ᄃ ᄅ ᄆ ᄇ ᄉ ᄊ ᄋ ᄌ ᄎ ᄏ ᄐ ᄑ ᄒ ᅡ ᅢ ᅥ ᅦ ᅧ ᅩ ᅪ ᅭ ᅮ ᅯ ᅲ ᅳ ᅴ ᅵ ᆨ ᆫ ᆯ ᆷ ᆸ ᆼ ᴬ ᴮ ᴰ ᴵ ᴺ ᵀ ᵃ ᵇ ᵈ ᵉ ᵍ ᵏ ᵐ ᵒ ᵖ ᵗ ᵘ ᵢ ᵣ ᵤ ᵥ ᶜ ᶠ ‐ ‑ ‒ – — ― ‖ ‘ ’ ‚ “ ” „ † ‡ • … ‰ ′ ″ › ‿ ⁄ ⁰ ⁱ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ⁺ ⁻ ⁿ ₀ ₁ ₂ ₃ ₄ ₅ ₆ ₇ ₈ ₉ ₊ ₍ ₎ ₐ ₑ ₒ ₓ ₕ ₖ ₗ ₘ ₙ ₚ ₛ ₜ ₤ ₩ € ₱ ₹ ℓ № ℝ ™ ⅓ ⅔ ← ↑ → ↓ ↔ ↦ ⇄ ⇌ ⇒ ∂ ∅ ∆ ∇ ∈ − ∗ ∘ √ ∞ ∧ ∨ ∩ ∪ ≈ ≡ ≤ ≥ ⊂ ⊆ ⊕ ⊗ ⋅ ─ │ ■ ▪ ● ★ ☆ ☉ ♠ ♣ ♥ ♦ ♭ ♯ ⟨ ⟩ ⱼ ⺩ ⺼ ⽥ 、 。 〈 〉 《 》 「 」 『 』 〜 あ い う え お か き く け こ さ し す せ そ た ち っ つ て と な に ぬ ね の は ひ ふ へ ほ ま み む め も や ゆ よ ら り る れ ろ を ん ァ ア ィ イ ウ ェ エ オ カ キ ク ケ コ サ シ ス セ タ チ ッ ツ テ ト ナ ニ ノ ハ ヒ フ ヘ ホ マ ミ ム メ モ ャ ュ ョ ラ リ ル レ ロ ワ ン ・ ー 一 三 上 下 不 世 中 主 久 之 也 事 二 五 井 京 人 亻 仁 介 代 仮 伊 会 佐 侍 保 信 健 元 光 八 公 内 出 分 前 劉 力 加 勝 北 区 十 千 南 博 原 口 古 史 司 合 吉 同 名 和 囗 四 国 國 土 地 坂 城 堂 場 士 夏 外 大 天 太 夫 奈 女 子 学 宀 宇 安 宗 定 宣 宮 家 宿 寺 將 小 尚 山 岡 島 崎 川 州 巿 帝 平 年 幸 广 弘 張 彳 後 御 德 心 忄 志 忠 愛 成 我 戦 戸 手 扌 政 文 新 方 日 明 星 春 昭 智 曲 書 月 有 朝 木 本 李 村 東 松 林 森 楊 樹 橋 歌 止 正 武 比 氏 民 水 氵 氷 永 江 沢 河 治 法 海 清 漢 瀬 火 版 犬 王 生 田 男 疒 発 白 的 皇 目 相 省 真 石 示 社 神 福 禾 秀 秋 空 立 章 竹 糹 美 義 耳 良 艹 花 英 華 葉 藤 行 街 西 見 訁 語 谷 貝 貴 車 軍 辶 道 郎 郡 部 都 里 野 金 鈴 镇 長 門 間 阝 阿 陳 陽 雄 青 面 風 食 香 馬 高 龍 龸 ﬁ ﬂ ！ （ ） ， － ． ／ ： ？ ～ . print(&#39;Number of single character tokens with hashes:&#39;, len(one_chars_hashes), &#39; n&#39;) # Print all of the single characters, 40 per row. # Strip the hash marks, since they just clutter the display. tokens = [token.replace(&#39;##&#39;, &#39;&#39;) for token in one_chars_hashes] # For every batch of 40 tokens... for i in range(0, len(tokens), 40): # Limit the end index so we don&#39;t go past the end of the list. end = min(i+40, len(tokens)+1) # Print out the tokens, seperated by a space. print(&#39; &#39;.join(tokens[i:end])) . Number of single character tokens with hashes: 997 s a e i n o d r y t l m u h k c g p 2 z 1 b 3 f 4 6 7 x v 8 5 9 0 w j q ° ₂ а и ² ₃ ı ₁ ⁺ ½ о ه ي α е د ن ν ø р ₄ ₀ ر я ³ ι ł н ᵢ ₙ ß ة ς م − т ː ل ь к ♭ η ی в ا × ¹ ы ה ɛ л ! &#34; # $ % &amp; &#39; ( ) * + , - . / : ; &lt; = &gt; ? @ [ ] ^ _ ` { | } ~ ¡ ¢ £ ¤ ¥ ¦ § ¨ © ª « ¬ ® ± ´ µ ¶ · º » ¼ ¾ ¿ æ ð ÷ þ đ ħ ŋ œ ƒ ɐ ɑ ɒ ɔ ɕ ə ɡ ɣ ɨ ɪ ɫ ɬ ɯ ɲ ɴ ɹ ɾ ʀ ʁ ʂ ʃ ʉ ʊ ʋ ʌ ʎ ʐ ʑ ʒ ʔ ʰ ʲ ʳ ʷ ʸ ʻ ʼ ʾ ʿ ˈ ˡ ˢ ˣ ˤ β γ δ ε ζ θ κ λ μ ξ ο π ρ σ τ υ φ χ ψ ω б г д ж з м п с у ф х ц ч ш щ ъ э ю ђ є і ј љ њ ћ ӏ ա բ գ դ ե թ ի լ կ հ մ յ ն ո պ ս վ տ ր ւ ք ־ א ב ג ד ו ז ח ט י ך כ ל ם מ ן נ ס ע ף פ ץ צ ק ר ש ת ، ء ب ت ث ج ح خ ذ ز س ش ص ض ط ظ ع غ ـ ف ق ك و ى ٹ پ چ ک گ ں ھ ہ ے अ आ उ ए क ख ग च ज ट ड ण त थ द ध न प ब भ म य र ल व श ष स ह ा ि ी ो । ॥ ং অ আ ই উ এ ও ক খ গ চ ছ জ ট ড ণ ত থ দ ধ ন প ব ভ ম য র ল শ ষ স হ া ি ী ে க ச ட த ந ன ப ம ய ர ல ள வ ா ி ு ே ை ನ ರ ಾ ක ය ර ල ව ා ก ง ต ท น พ ม ย ร ล ว ส อ า เ ་ ། ག ང ད ན པ བ མ འ ར ལ ས မ ა ბ გ დ ე ვ თ ი კ ლ მ ნ ო რ ს ტ უ ᄀ ᄂ ᄃ ᄅ ᄆ ᄇ ᄉ ᄊ ᄋ ᄌ ᄎ ᄏ ᄐ ᄑ ᄒ ᅡ ᅢ ᅥ ᅦ ᅧ ᅩ ᅪ ᅭ ᅮ ᅯ ᅲ ᅳ ᅴ ᅵ ᆨ ᆫ ᆯ ᆷ ᆸ ᆼ ᴬ ᴮ ᴰ ᴵ ᴺ ᵀ ᵃ ᵇ ᵈ ᵉ ᵍ ᵏ ᵐ ᵒ ᵖ ᵗ ᵘ ᵣ ᵤ ᵥ ᶜ ᶠ ‐ ‑ ‒ – — ― ‖ ‘ ’ ‚ “ ” „ † ‡ • … ‰ ′ ″ › ‿ ⁄ ⁰ ⁱ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ⁻ ⁿ ₅ ₆ ₇ ₈ ₉ ₊ ₍ ₎ ₐ ₑ ₒ ₓ ₕ ₖ ₗ ₘ ₚ ₛ ₜ ₤ ₩ € ₱ ₹ ℓ № ℝ ™ ⅓ ⅔ ← ↑ → ↓ ↔ ↦ ⇄ ⇌ ⇒ ∂ ∅ ∆ ∇ ∈ ∗ ∘ √ ∞ ∧ ∨ ∩ ∪ ≈ ≡ ≤ ≥ ⊂ ⊆ ⊕ ⊗ ⋅ ─ │ ■ ▪ ● ★ ☆ ☉ ♠ ♣ ♥ ♦ ♯ ⟨ ⟩ ⱼ ⺩ ⺼ ⽥ 、 。 〈 〉 《 》 「 」 『 』 〜 あ い う え お か き く け こ さ し す せ そ た ち っ つ て と な に ぬ ね の は ひ ふ へ ほ ま み む め も や ゆ よ ら り る れ ろ を ん ァ ア ィ イ ウ ェ エ オ カ キ ク ケ コ サ シ ス セ タ チ ッ ツ テ ト ナ ニ ノ ハ ヒ フ ヘ ホ マ ミ ム メ モ ャ ュ ョ ラ リ ル レ ロ ワ ン ・ ー 一 三 上 下 不 世 中 主 久 之 也 事 二 五 井 京 人 亻 仁 介 代 仮 伊 会 佐 侍 保 信 健 元 光 八 公 内 出 分 前 劉 力 加 勝 北 区 十 千 南 博 原 口 古 史 司 合 吉 同 名 和 囗 四 国 國 土 地 坂 城 堂 場 士 夏 外 大 天 太 夫 奈 女 子 学 宀 宇 安 宗 定 宣 宮 家 宿 寺 將 小 尚 山 岡 島 崎 川 州 巿 帝 平 年 幸 广 弘 張 彳 後 御 德 心 忄 志 忠 愛 成 我 戦 戸 手 扌 政 文 新 方 日 明 星 春 昭 智 曲 書 月 有 朝 木 本 李 村 東 松 林 森 楊 樹 橋 歌 止 正 武 比 氏 民 水 氵 氷 永 江 沢 河 治 法 海 清 漢 瀬 火 版 犬 王 生 田 男 疒 発 白 的 皇 目 相 省 真 石 示 社 神 福 禾 秀 秋 空 立 章 竹 糹 美 義 耳 良 艹 花 英 華 葉 藤 行 街 西 見 訁 語 谷 貝 貴 車 軍 辶 道 郎 郡 部 都 里 野 金 鈴 镇 長 門 間 阝 阿 陳 陽 雄 青 面 風 食 香 馬 高 龍 龸 ﬁ ﬂ ！ （ ） ， － ． ／ ： ？ ～ . print(&#39;Are the two sets identical?&#39;, set(one_chars) == set(tokens)) . Are the two sets identical? True . Subwords vs. Whole-words . Let&#39;s gather some statistics on the vocabulary. . import matplotlib.pyplot as plt import seaborn as sns import numpy as np sns.set(style=&#39;darkgrid&#39;) # Increase the plot size and font size. sns.set(font_scale=1.5) plt.rcParams[&quot;figure.figsize&quot;] = (10,5) # Measure the length of every token in the vocab. token_lengths = [len(token) for token in tokenizer.vocab.keys()] # Plot the number of tokens of each length. sns.countplot(token_lengths) plt.title(&#39;Vocab Token Lengths&#39;) plt.xlabel(&#39;Token Length&#39;) plt.ylabel(&#39;# of Tokens&#39;) print(&#39;Maximum token length:&#39;, max(token_lengths)) . Maximum token length: 18 . Let&#39;s look at just the tokens which begin with &#39;##&#39;. . num_subwords = 0 subword_lengths = [] # For each token in the vocabulary, for token in tokenizer.vocab.keys(): # If it&#39;s a subword, if len(token) &gt;=2 and token[0:2] == &#39;##&#39;: # Tally all wubwords num_subwords += 1 # Measure the subword length (without the hashes) length = len(token) - 2 # Record the lengths. subword_lengths.append(length) . How many &#39;##&#39; tokens are there vs. the full vocab? . vocab_size = len(tokenizer.vocab.keys()) print(&#39;Number of subwords: {:,} of {:,}&#39;.format(num_subwords, vocab_size)) # Calculate the percentage of words that are &#39;##&#39; subwords. prcnt = float(num_subwords) / vocab_size * 100.0 print(&#39;%.1f%%&#39; % prcnt) . Number of subwords: 5,828 of 30,522 19.1% . Plot the subword lengths (not including the two &#39;##&#39; characters). . sns.countplot(subword_lengths) plt.title(&#39;Subword Token Lengths (w/o &quot;##&quot;)&#39;) plt.xlabel(&#39;Subword Length&#39;) plt.ylabel(&#39;# of ## Subwords&#39;) . Text(0,0.5,&#39;# of ## Subwords&#39;) . Misspellings . &#39;misspelled&#39; in tokenizer.vocab # Right . False . &#39;mispelled&#39; in tokenizer.vocab # Wrong . False . &#39;government&#39; in tokenizer.vocab # Right . True . &#39;goverment&#39; in tokenizer.vocab # Wrong . False . &#39;beginning&#39; in tokenizer.vocab # Right . True . &#39;begining&#39; in tokenizer.vocab # Wrong . False . &#39;separate&#39; in tokenizer.vocab # Right . True . &#39;seperate&#39; in tokenizer.vocab # Wrong . False . What about contractions? . &quot;can&#39;t&quot; in tokenizer.vocab . False . &quot;cant&quot; in tokenizer.vocab . False . Start vs. Mid Subwords . For single characters, there are both the individual character and the &#39;##&#39; version for every character. Is the same true of subwords? . # For each token in the vocabulary, for token in tokenizer.vocab.keys(): # If it&#39;s a subword, if len(token) &gt;= 2 and token[0:2] == &#39;##&#39;: if not token[2:] in tokenizer.vocab: print(&#39;Did not find a token for&#39;, token[2:]) break . Did not find a token for ly . &#39;##ly&#39; in tokenizer.vocab . True . &#39;ly&#39; in tokenizer.vocab . False . Names . !pip install wget . Collecting wget Downloading wget-3.2.zip (10 kB) Building wheels for collected packages: wget Building wheel for wget (setup.py) ... done Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=11569 sha256=05f2900f19cc29250982d62b189f1a0ca18cc0423a8f86496ce1225086688918 Stored in directory: /Users/sparkling/Library/Caches/pip/wheels/90/1d/93/c863ee832230df5cfc25ca497b3e88e0ee3ea9e44adc46ac62 Successfully built wget Installing collected packages: wget Successfully installed wget-3.2 . import wget import random print(&#39;Beginning file download with wget module&#39;) url = &#39;http://www.gutenberg.org/files/3201/files/NAMES.TXT&#39; wget.download(url, &#39;first-names.txt&#39;) . Beginning file download with wget module . &#39;first-names.txt&#39; . # Read them in. with open(&#39;first-names.txt&#39;, &#39;rb&#39;) as f: names_encoded = f.readlines() names = [] # Decode the names, convert to lowercase, and strip newlines. for name in names_encoded: try: names.append(name.rstrip().lower().decode(&#39;utf-8&#39;)) except: continue print(&#39;Number of nmaes: {:,}&#39;.format(len(names))) print(&#39;Example:&#39;, random.choice(names)) . Number of nmaes: 21,985 Example: basset . num_names = 0 # For each name in our list, for name in names: # If it&#39;s in the vocab, if name in tokenizer.vocab: # Tally it. num_names += 1 print(&#39;{:,} names in the vocabulary&#39;.format(num_names)) . 3,869 names in the vocabulary . Numbers . # Count how many numbers are in the vocabulary. count = 0 # For each token in the vocabulary, for token in tokenizer.vocab.keys(): # Tally if it&#39;s a number. if token.isdigit(): count += 1 # Any numbers &gt;= 10,000? if len(token) &gt; 4: print(token) print(&#39;Vocab includes {:,} numbers.&#39;.format(count)) . Vocab includes 881 numbers. . # Count how many dates between 1600 and 2021 are included. count = 0 for i in range(1600, 2021): if str(i) in tokenizer.vocab: count += 1 print(&#39;Vocab includes {:,} of 421 dates from 1600 - 2021&#39;.format(count)) . Vocab includes 384 of 421 dates from 1600 - 2021 .",
            "url": "https://sparklingness.github.io/blog/bert/jupyter/2020/04/18/Inspect-Bert-Vocabulary.html",
            "relUrl": "/bert/jupyter/2020/04/18/Inspect-Bert-Vocabulary.html",
            "date": " • Apr 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Bert Notebook Blog Post(2)",
            "content": "ref: http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/ . 1. Loading Pre-Trained BERT . ! pip install pytorch-pretrained-bert . Collecting pytorch-pretrained-bert Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB) |████████████████████████████████| 133kB 2.8MB/s Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.2) Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.12.38) Requirement already satisfied: torch&gt;=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0) Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.38.0) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0) Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;pytorch-pretrained-bert) (0.9.5) Requirement already satisfied: botocore&lt;1.16.0,&gt;=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;pytorch-pretrained-bert) (1.15.38) Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;pytorch-pretrained-bert) (0.3.3) Requirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;pytorch-pretrained-bert) (1.24.3) Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;pytorch-pretrained-bert) (2.8) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;pytorch-pretrained-bert) (2020.4.5.1) Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;pytorch-pretrained-bert) (3.0.4) Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore&lt;1.16.0,&gt;=1.15.38-&gt;boto3-&gt;pytorch-pretrained-bert) (2.8.1) Requirement already satisfied: docutils&lt;0.16,&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore&lt;1.16.0,&gt;=1.15.38-&gt;boto3-&gt;pytorch-pretrained-bert) (0.15.2) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.16.0,&gt;=1.15.38-&gt;boto3-&gt;pytorch-pretrained-bert) (1.12.0) Installing collected packages: pytorch-pretrained-bert Successfully installed pytorch-pretrained-bert-0.6.2 . import torch from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM # OPTIONAL: if you want to have more information on what&#39;s happening, activate the logger as follows import logging # logging.basicConfig(level=logging.INFO) import matplotlib.pyplot as plt % matplotlib inline # Load pre-trained model tokenizer (vocabulary) tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;) . 100%|██████████| 231508/231508 [00:00&lt;00:00, 315280.35B/s] . 2. Input Formatting . 2.1. Special Tokens . Special tokens to mark the beginning ([CLS]) and seprataion/end of sentences ([SEP]) . BERT can take as input either one or two sentences, and expects special tokens to mark the beginning and end of each one: . 2 Sentence Input: . [CLS] The man wnent to the store. [SEP] He bought a gallon of milk. [SEP] . 1 Sentence Input: . [CLS] The man went to the store. [SEP] . 2.2. Tokenization . text = &quot;Here is the sentence I want embeddings for.&quot; marked_text = &quot;[CLS] &quot; + text + &quot; [SEP]&quot; # Tokenize our sentence with the BERT tokenizer. tokenized_text = tokenizer.tokenize(marked_text) # Print out the tokens. print(tokenized_text) . [&#39;[CLS]&#39;, &#39;here&#39;, &#39;is&#39;, &#39;the&#39;, &#39;sentence&#39;, &#39;i&#39;, &#39;want&#39;, &#39;em&#39;, &#39;##bed&#39;, &#39;##ding&#39;, &#39;##s&#39;, &#39;for&#39;, &#39;.&#39;, &#39;[SEP]&#39;] . # Define a new example sentence with multiple meanings of the word &quot;bank&quot; text = &quot;After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.&quot; # Add the special tokens. marked_text = &quot;[CLS] &quot; + text + &quot; [SEP]&quot; # Split the sentence into tokens. tokenized_text = tokenizer.tokenize(marked_text) # Map the token strings to their vocabulary indeces. indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) # Display the words with their indeces. for tup in zip(tokenized_text, indexed_tokens): print(&#39;{:&lt;12} {:&gt;6,}&#39;.format(tup[0], tup[1])) . [CLS] 101 after 2,044 stealing 11,065 money 2,769 from 2,013 the 1,996 bank 2,924 vault 11,632 , 1,010 the 1,996 bank 2,924 robber 27,307 was 2,001 seen 2,464 fishing 5,645 on 2,006 the 1,996 mississippi 5,900 river 2,314 bank 2,924 . 1,012 [SEP] 102 . 2.3. Segment ID . # Mark each of the 22 tokens as belonging to sentence &quot;1&quot;. segments_ids = [1] * len(tokenized_text) print(segments_ids) . [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] . 3. Extracting Embeddings . 3.1. Running BERT on our text . model.eval() evaluation mode turns off dropout regularization which is used in training . # Convert inputs to PyTorch tensors tokens_tensor = torch.tensor([indexed_tokens]) print(tokens_tensor) segments_tensors = torch.tensor([segments_ids]) # Load pre-trained model (weights) model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;) # Put the model in &quot;evaluation&quot; mode, meaning feed-forward operation. model.eval() . tensor([[ 101, 2044, 11065, 2769, 2013, 1996, 2924, 11632, 1010, 1996, 2924, 27307, 2001, 2464, 5645, 2006, 1996, 5900, 2314, 2924, 1012, 102]]) . 100%|██████████| 407873900/407873900 [00:35&lt;00:00, 11382845.79B/s] . BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) . torch.no_grad deactivates the gradient calculations, saves memory, and speeds up computation. (we don&#39;t need gradients or backpropagation since we&#39;re just running a forward pass). . # Predict hidden states features for each layer with torch.no_grad(): encoded_layers, _ = model(tokens_tensor, segments_tensors) . 3.2. Understanding the Output . print(&quot;Number of layers:&quot;, len(encoded_layers)) layer_i = 0 print(&quot;Number of batches:&quot;, len(encoded_layers[layer_i])) batch_i = 0 print(&quot;Number of tokens:&quot;, len(encoded_layers[layer_i][batch_i])) token_i = 0 print(&quot;Number of hidden units:&quot;, len(encoded_layers[layer_i][batch_i][token_i])) . Number of layers: 12 Number of batches: 1 Number of tokens: 22 Number of hidden units: 768 . # For the 5th token in our sentence, select its feature values from layer 5. token_i = 5 layer_i = 5 vec = encoded_layers[layer_i][batch_i][token_i] # Plot the values as a histogram to show their distribution. plt.figure(figsize=(10,10)) plt.hist(vec, bins=200) plt.show() . # `encoded_layers` is a Python list. print(&#39; Type of encoded_layers: &#39;, type(encoded_layers), len(encoded_layers)) # Each layer in the list is a torch tensor. print(&#39;Tensor shape for each layer: &#39;, encoded_layers[0].size()) . Type of encoded_layers: &lt;class &#39;list&#39;&gt; 12 Tensor shape for each layer: torch.Size([1, 22, 768]) . # Concatenate the tensors for all layers. We use `stack` here to # create a new dimension in the tensor. token_embeddings = torch.stack(encoded_layers, dim=0) token_embeddings.size() . torch.Size([12, 1, 22, 768]) . # Remove dimension 1, the &quot;batches&quot;. token_embeddings = torch.squeeze(token_embeddings, dim=1) token_embeddings.size() . torch.Size([12, 22, 768]) . # Swap dimensions 0 and 1. token_embeddings = token_embeddings.permute(1, 0, 2) token_embeddings.size() . torch.Size([22, 12, 768]) . 3.3. Creating word and sentence vectors from hidden states . Word Vectors . # Stores the token vectors, with shape [22 x 3,072] (4 x 768 = 3,072) token_vecs_cat = [] # `token_embeddings` is a [22 x 12 x 768] tensor. # For each token in the sentence... for token in token_embeddings: # `token` is a [12 x 768] tensor # Concatenate the vectors (that is, append them together) from the last four layers. # Each layer vector is 768 values, so `cat_vec` is length 3,072. cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0) # Use `cat_vec` to represent `token`. token_vecs_cat.append(cat_vec) print(&#39;Shape is: %d x %d&#39; % (len(token_vecs_cat), len(token_vecs_cat[0]))) . Shape is: 22 x 3072 . # Stores the token vectors, with shape [22 x 768] token_vecs_sum = [] # `token_embeddings` is a [22 x 12 x 768] tensor. # For each token in the sentence... for token in token_embeddings: # `token` is a [12 x 768] tensor # Sum the vectors from the last four layers. sum_vec = torch.sum(token[-4:], dim=0) # Use `sum_vec` to represent `token`. token_vecs_sum.append(sum_vec) print(&#39;Shape is: %d x %d&#39; % (len(token_vecs_sum), len(token_vecs_sum[0]))) . Shape is: 22 x 768 . Sentence Vectors . # `encoded_layers` has shape [12 x 1 x 22 x 768] # `token_vecs` is a tensor with shape [22 x 768] token_vecs = encoded_layers[11][0] # Calculate the average of all 22 token vectors. sentence_embedding = torch.mean(token_vecs, dim=0) . print(&quot;Our final sentence embedding vector of shape:&quot;, sentence_embedding.size()) . Our final sentence embedding vector of shape: torch.Size([768]) . 3.4. Confirming contextually dependent vectors . for i, token_str in enumerate(tokenized_text): print(i, token_str) . 0 [CLS] 1 after 2 stealing 3 money 4 from 5 the 6 bank 7 vault 8 , 9 the 10 bank 11 robber 12 was 13 seen 14 fishing 15 on 16 the 17 mississippi 18 river 19 bank 20 . 21 [SEP] . print(&#39;First 5 vector values for each instance of &quot;bank&quot;.&#39;) print(&#39;&#39;) print(&quot;bank vault &quot;, str(token_vecs_sum[6][:5])) print(&quot;bank robber &quot;, str(token_vecs_sum[10][:5])) print(&quot;river bank &quot;, str(token_vecs_sum[19][:5])) . First 5 vector values for each instance of &#34;bank&#34;. bank vault tensor([ 2.1319, -2.1413, -1.6260, 0.8638, 3.3173]) bank robber tensor([ 1.1868, -1.5298, -1.3770, 1.0648, 3.1446]) river bank tensor([ 1.1295, -1.4725, -0.7296, -0.0901, 2.4970]) . from scipy.spatial.distance import cosine # Calculate the cosine similarity between the word bank # in &quot;bank robber&quot; vs &quot;river bank&quot; (different meanings). diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19]) # Calculate the cosine similarity between the word bank # in &quot;bank robber&quot; vs &quot;bank vault&quot; (same meaning). same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6]) print(&#39;Vector similarity for *similar* meanings: %.2f&#39; % same_bank) print(&#39;Vector similarity for *different* meanings: %.2f&#39; % diff_bank) . Vector similarity for *similar* meanings: 0.95 Vector similarity for *different* meanings: 0.68 .",
            "url": "https://sparklingness.github.io/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html",
            "relUrl": "/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html",
            "date": " • Apr 11, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sparklingness.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}