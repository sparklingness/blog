{
  
    
        "post0": {
            "title": "BERT Fine-Tuning Sentence Classification",
            "content": "1. Setup . 1.1. Using Colab GPU for Training . import tensorflow as tf # Get the GPU device name. device_name = tf.test.gpu_device_name() # The device name should look like the following: if device_name == &#39;/device:GPU:0&#39;: print(&#39;Found GPU at: {}&#39;.format(device_name)) else: raise SystemError(&#39;GPU device not found&#39;) . Found GPU at: /device:GPU:0 . import torch # If there&#39;s a GPU available, if torch.cuda.is_available(): # Tell PyTorch to user the GPU. device = torch.device(&quot;cuda&quot;) print(&#39;There are %d GPU(s) available.&#39; % torch.cuda.device_count()) print(&#39;We will user the GPU:&#39;, torch.cuda.get_device_name(0)) # If not, else: print(&#39;No GPU available, using the CPU instead.&#39;) device = torch.device(&quot;cpu&quot;) . There are 1 GPU(s) available. We will user the GPU: Tesla K80 . 1.2. Installing the Hugging Face Library . ! pip install transformers -q . ! pip install wget -q . import wget import os print(&#39;Downloading dataset...&#39;) # The URL for the dataset zip file. url = &#39;https://nyu-mll.github.io/CoLA/cola_public_1.1.zip&#39; # Download the file (if we haven&#39;t already) if not os.path.exists(&#39;./cola_public_1.1.zip&#39;): wget.download(url, &#39;./cola_public_1.1.zip&#39;) . Downloading dataset... . # Unzip the dataset (if we haven&#39;t already) if not os.path.exists(&#39;./cola_public/&#39;): ! unzip cola_public_1.1.zip . ! head cola_public/*/* . ==&gt; cola_public/raw/in_domain_dev.tsv &lt;== gj04 1 The sailors rode the breeze clear of the rocks. gj04 1 The weights made the rope stretch over the pulley. gj04 1 The mechanical doll wriggled itself loose. cj99 1 If you had eaten more, you would want less. cj99 0 * As you eat the most, you want the least. cj99 0 * The more you would want, the less you would eat. cj99 0 * I demand that the more John eat, the more he pays. cj99 1 Mary listens to the Grateful Dead, she gets depressed. cj99 1 The angrier Mary got, the more she looked at pictures. cj99 1 The higher the stakes, the lower his expectations are. ==&gt; cola_public/raw/in_domain_train.tsv &lt;== gj04 1 Our friends won&#39;t buy this analysis, let alone the next one we propose. gj04 1 One more pseudo generalization and I&#39;m giving up. gj04 1 One more pseudo generalization or I&#39;m giving up. gj04 1 The more we study verbs, the crazier they get. gj04 1 Day by day the facts are getting murkier. gj04 1 I&#39;ll fix you a drink. gj04 1 Fred watered the plants flat. gj04 1 Bill coughed his way out of the restaurant. gj04 1 We&#39;re dancing the night away. gj04 1 Herman hammered the metal flat. ==&gt; cola_public/raw/out_of_domain_dev.tsv &lt;== clc95 1 Somebody just left - guess who. clc95 1 They claimed they had settled on something, but it wasn&#39;t clear what they had settled on. clc95 1 If Sam was going, Sally would know where. clc95 1 They&#39;re going to serve the guests something, but it&#39;s unclear what. clc95 1 She&#39;s reading. I can&#39;t imagine what. clc95 1 John said Joan saw someone from her graduating class. clc95 0 * John ate dinner but I don&#39;t know who. clc95 0 * She mailed John a letter, but I don&#39;t know to whom. clc95 1 I served leek soup to my guests. clc95 1 I served my guests. ==&gt; cola_public/tokenized/in_domain_dev.tsv &lt;== gj04 1 the sailors rode the breeze clear of the rocks . gj04 1 the weights made the rope stretch over the pulley . gj04 1 the mechanical doll wriggled itself loose . cj99 1 if you had eaten more , you would want less . cj99 0 * as you eat the most , you want the least . cj99 0 * the more you would want , the less you would eat . cj99 0 * i demand that the more john eat , the more he pays . cj99 1 mary listens to the grateful dead , she gets depressed . cj99 1 the angrier mary got , the more she looked at pictures . cj99 1 the higher the stakes , the lower his expectations are . ==&gt; cola_public/tokenized/in_domain_train.tsv &lt;== gj04 1 our friends wo n&#39;t buy this analysis , let alone the next one we propose . gj04 1 one more pseudo generalization and i &#39;m giving up . gj04 1 one more pseudo generalization or i &#39;m giving up . gj04 1 the more we study verbs , the crazier they get . gj04 1 day by day the facts are getting murkier . gj04 1 i &#39;ll fix you a drink . gj04 1 fred watered the plants flat . gj04 1 bill coughed his way out of the restaurant . gj04 1 we &#39;re dancing the night away . gj04 1 herman hammered the metal flat . ==&gt; cola_public/tokenized/out_of_domain_dev.tsv &lt;== clc95 1 somebody just left - guess who . clc95 1 they claimed they had settled on something , but it was n&#39;t clear what they had settled on . clc95 1 if sam was going , sally would know where . clc95 1 they &#39;re going to serve the guests something , but it &#39;s unclear what . clc95 1 she &#39;s reading . i ca n&#39;t imagine what . clc95 1 john said joan saw someone from her graduating class . clc95 0 * john ate dinner but i do n&#39;t know who . clc95 0 * she mailed john a letter , but i do n&#39;t know to whom . clc95 1 i served leek soup to my guests . clc95 1 i served my guests . . 2.2. Parse . import pandas as pd # Load the dataset into a pandas dataframe. df = pd.read_csv(&quot;./cola_public/raw/in_domain_train.tsv&quot;, delimiter=&#39; t&#39;, header=None, names=[&#39;sentence_source&#39;, &#39;label&#39;, &#39;label_notes&#39;, &#39;sentence&#39;]) # Report the number of sentences. print(&#39;Number of training sentences: {:,} n&#39;.format(df.shape[0])) # Display 10 random rows from the data df.sample(10) . Number of training sentences: 8,551 . sentence_source label label_notes sentence . 1337 r-67 | 1 | NaN | I want to peruse that contract before filing a... | . 1670 r-67 | 1 | NaN | This hat Tom said Al thought you wanted me to ... | . 6952 m_02 | 1 | NaN | Jane visits Emma. | . 449 bc01 | 0 | ?? | Who is he reading a book that criticizes? | . 4988 ks08 | 1 | NaN | The fact that scientists have now established ... | . 1830 r-67 | 0 | * | That informers they never use is claimed by th... | . 6508 d_98 | 0 | * | Every cat doesn&#39;t like mice, but Felix doesn&#39;t. | . 8329 ad03 | 1 | NaN | I asked did Medea poison Jason. | . 1358 r-67 | 0 | * | They will give me a hat that I won&#39;t like whic... | . 5503 b_73 | 1 | NaN | Her mother wants Mary to be such an eminent wo... | . df[df.label == 0].sample(5)[[&#39;sentence&#39;, &#39;label&#39;]] . sentence label . 6544 The table, I put Kim on which supported the book. | 0 | . 5993 Has Calvin a bowl? | 0 | . 379 How do you wonder who could solve this problem. | 0 | . 588 the branch dropped bare of its apple. | 0 | . 7194 Your desk before, this girl in the red coat wi... | 0 | . # Get the lists of sentences and their labels. sentences = df.sentence.values labels = df.label.values . 3. Tokenization &amp; Input Formatting . 3.1. BERT Tokenizer . from transformers import BertTokenizer # Load the BERT tokenizer. print(&#39;Loading BERT tokenizer...&#39;) tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-multilingual-uncased&#39;, do_lower_case=True) . Loading BERT tokenizer... . # Print the original sentence. print(&#39; Original: &#39;, sentences[0]) # Print the sentence split into tokens. print(&#39;Tokenized: &#39;, tokenizer.tokenize(sentences[0])) # Print the sentence mapped to token ids. print(&#39;Token IDs: &#39;, tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0]))) . Original: Our friends won&#39;t buy this analysis, let alone the next one we propose. Tokenized: [&#39;our&#39;, &#39;friends&#39;, &#39;won&#39;, &#34;&#39;&#34;, &#39;t&#39;, &#39;buy&#39;, &#39;this&#39;, &#39;analysis&#39;, &#39;,&#39;, &#39;let&#39;, &#39;alone&#39;, &#39;the&#39;, &#39;next&#39;, &#39;one&#39;, &#39;we&#39;, &#39;propose&#39;, &#39;.&#39;] Token IDs: [14008, 16119, 11441, 112, 162, 35172, 10372, 15559, 117, 12421, 19145, 10103, 12878, 10399, 11312, 25690, 119] . korean = &quot;안녕하세요. 반갑습니다. 너는 이름이 뭐니? 오늘 날씨가 맑고 좋구나.&quot; # Print the original sentence. print(&#39; Original: &#39;, korean) # Print the sentence split into tokens. print(&#39;Tokenized: &#39;, tokenizer.tokenize(korean)) # Print the sentence mapped to token ids. print(&#39;Token IDs: &#39;, tokenizer.convert_tokens_to_ids(tokenizer.tokenize(korean))) . Original: 안녕하세요. 반갑습니다. 너는 이름이 뭐니? 오늘 날씨가 맑고 좋구나. Tokenized: [&#39;ᄋ&#39;, &#39;##ᅡᆫ&#39;, &#39;##녀&#39;, &#39;##ᆼ&#39;, &#39;##하&#39;, &#39;##세&#39;, &#39;##요&#39;, &#39;.&#39;, &#39;ᄇ&#39;, &#39;##ᅡᆫ&#39;, &#39;##가&#39;, &#39;##ᆸ&#39;, &#39;##스&#39;, &#39;##ᆸ니다&#39;, &#39;.&#39;, &#39;ᄂ&#39;, &#39;##ᅥ&#39;, &#39;##는&#39;, &#39;이&#39;, &#39;##름이&#39;, &#39;ᄆ&#39;, &#39;##ᅯ&#39;, &#39;##니&#39;, &#39;?&#39;, &#39;ᄋ&#39;, &#39;##ᅩ&#39;, &#39;##ᄂ&#39;, &#39;##ᅳᆯ&#39;, &#39;날&#39;, &#39;##씨&#39;, &#39;##가&#39;, &#39;ᄆ&#39;, &#39;##ᅡ&#39;, &#39;##ᆰ&#39;, &#39;##고&#39;, &#39;ᄌ&#39;, &#39;##ᅩ&#39;, &#39;##ᇂ&#39;, &#39;##구&#39;, &#39;##나&#39;, &#39;.&#39;] Token IDs: [1174, 26646, 49345, 13045, 35132, 25169, 47024, 119, 1170, 26646, 11376, 17360, 13212, 79427, 119, 1165, 33645, 11192, 12398, 89420, 1169, 97090, 25536, 136, 1174, 29347, 97071, 63277, 76818, 47928, 11376, 1169, 25539, 97098, 12300, 1175, 29347, 97109, 16336, 16801, 119] . When we actually convert all of our sentences, we&#39;ll use the tokenize.encode functio to handle both steps, rather than calling tokenize and convert_tokens_to_ids seperately. . 3.2. Required Formatting . 3.3. Tokenize Dataset . max_len = 0 # For every sentence, for sent in sentences: # Tokenize the text and add `[CLS]` and `[SEP]` tokens. input_ids = tokenizer.encode(sent, add_special_tokens=True) # Update the maximum sentence langth. max_len = max(max_len, len(input_ids)) print(&#39;Max sentence length: &#39;, max_len) . Max sentence length: 48 . # Tokenize all of the sentences and map the tokens to their word IDs. input_ids = [] attention_masks = [] # For every sentence, for sent in sentences: # `encode_plus` will: # (1) Tokenize the sentence. # (2) Prepend the `[CLS]` token to the start. # (3) Append the `[SEP]` token to the end. # (4) Map tokens to their IDs. # (5) Pad or truncate the sentence to `max_length` # (6) Create attention masks for [PAD] tokens. encoded_dict = tokenizer.encode_plus( sent, # Sentence to encode. add_special_tokens = True, # Add &#39;[CLS]&#39; and &#39;[SEP]&#39; max_length = 64, # Pad &amp; truncate all sentences. pad_to_max_length = True, return_attention_mask = True, # Construct attn. masks. return_Tensors = &#39;pt&#39;, # Return pytorch tensors. ) # Add the encoded sentence to the list. input_ids.append(encoded_dict[&#39;input_ids&#39;]) # And its attention mask (simply differentiates padding from non-padding.) attention_masks.append(encoded_dict[&#39;attention_mask&#39;]) # Convert the lists into tensors. input_ids = torch.FloatTensor(input_ids) attention_masks = torch.FloatTensor(attention_masks) labels = torch.tensor(labels) # Print sentence 0, now as a list of IDs. print(&#39;Original: &#39;, sentences[0]) print(&#39;Token IDs: &#39;, input_ids[0]) . Original: Our friends won&#39;t buy this analysis, let alone the next one we propose. Token IDs: tensor([ 101., 14008., 16119., 11441., 112., 162., 35172., 10372., 15559., 117., 12421., 19145., 10103., 12878., 10399., 11312., 25690., 119., 102., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). . 3.4. Training &amp; Validation Split . from torch.utils.data import TensorDataset, random_split # Combine the training inputs into a TensorDataset. dataset = TensorDataset(input_ids, attention_masks, labels) # Create a 90-10 train-validation split. # Calculate the number of samples to include in each set. train_size = int(0.9 * len(dataset)) val_size = len(dataset) - train_size # Devide the dataset by randomly selecting samples. train_dataset, val_dataset = random_split(dataset, [train_size, val_size]) print(&#39;{:&gt;5} training samples&#39;.format(train_size)) print(&#39;{:&gt;5} validation samples&#39;.format(val_size)) . 7695 training samples 856 validation samples . from torch.utils.data import DataLoader, RandomSampler, SequentialSampler # The DataLoader needs to know our batch size for training, so we specify it here. # For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32. batch_size = 32 # Create the DataLoaders for our training and validation sets. # We&#39;ll take training samples in random order. train_dataloader = DataLoader( train_dataset, # The training samples. sampler = RandomSampler(train_dataset), # Select batches randomly batch_size = batch_size # Trains with this batch size. ) # For validation the order doesn&#39;t matter, so we&#39;ll just read them sequentially. validation_dataloader = DataLoader( val_dataset, # The validation samples. sampler = SequentialSampler(val_dataset), # Pull out batches sequentially. batch_size = batch_size # Evaluate with this batch size. ) .",
            "url": "https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification.html",
            "relUrl": "/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "BERT Fine-Tuning Sentence Classification",
            "content": "1. Setup . 1.1. Using Colab GPU for Training . import tensorflow as tf # Get the GPU device name. device_name = tf.test.gpu_device_name() # The device name should look like the following: if device_name == &#39;/device:GPU:0&#39;: print(&#39;Found GPU at: {}&#39;.format(device_name)) else: raise SystemError(&#39;GPU device not found&#39;) . Found GPU at: /device:GPU:0 . import torch # If there&#39;s a GPU available, if torch.cuda.is_available(): # Tell PyTorch to user the GPU. device = torch.device(&quot;cuda&quot;) print(&#39;There are %d GPU(s) available.&#39; % torch.cuda.device_count()) print(&#39;We will user the GPU:&#39;, torch.cuda.get_device_name(0)) # If not, else: print(&#39;No GPU available, using the CPU instead.&#39;) device = torch.device(&quot;cpu&quot;) . There are 1 GPU(s) available. We will user the GPU: Tesla T4 . 1.2. Installing the Hugging Face Library . ! pip install transformers -q . ! pip install wget -q . import wget import os print(&#39;Downloading dataset...&#39;) # The URL for the dataset zip file. url = &#39;https://nyu-mll.github.io/CoLA/cola_public_1.1.zip&#39; # Download the file (if we haven&#39;t already) if not os.path.exists(&#39;./cola_public_1.1.zip&#39;): wget.download(url, &#39;./cola_public_1.1.zip&#39;) . Downloading dataset... . # Unzip the dataset (if we haven&#39;t already) if not os.path.exists(&#39;./cola_public/&#39;): ! unzip cola_public_1.1.zip . ! head cola_public/*/* . ==&gt; cola_public/raw/in_domain_dev.tsv &lt;== gj04 1 The sailors rode the breeze clear of the rocks. gj04 1 The weights made the rope stretch over the pulley. gj04 1 The mechanical doll wriggled itself loose. cj99 1 If you had eaten more, you would want less. cj99 0 * As you eat the most, you want the least. cj99 0 * The more you would want, the less you would eat. cj99 0 * I demand that the more John eat, the more he pays. cj99 1 Mary listens to the Grateful Dead, she gets depressed. cj99 1 The angrier Mary got, the more she looked at pictures. cj99 1 The higher the stakes, the lower his expectations are. ==&gt; cola_public/raw/in_domain_train.tsv &lt;== gj04 1 Our friends won&#39;t buy this analysis, let alone the next one we propose. gj04 1 One more pseudo generalization and I&#39;m giving up. gj04 1 One more pseudo generalization or I&#39;m giving up. gj04 1 The more we study verbs, the crazier they get. gj04 1 Day by day the facts are getting murkier. gj04 1 I&#39;ll fix you a drink. gj04 1 Fred watered the plants flat. gj04 1 Bill coughed his way out of the restaurant. gj04 1 We&#39;re dancing the night away. gj04 1 Herman hammered the metal flat. ==&gt; cola_public/raw/out_of_domain_dev.tsv &lt;== clc95 1 Somebody just left - guess who. clc95 1 They claimed they had settled on something, but it wasn&#39;t clear what they had settled on. clc95 1 If Sam was going, Sally would know where. clc95 1 They&#39;re going to serve the guests something, but it&#39;s unclear what. clc95 1 She&#39;s reading. I can&#39;t imagine what. clc95 1 John said Joan saw someone from her graduating class. clc95 0 * John ate dinner but I don&#39;t know who. clc95 0 * She mailed John a letter, but I don&#39;t know to whom. clc95 1 I served leek soup to my guests. clc95 1 I served my guests. ==&gt; cola_public/tokenized/in_domain_dev.tsv &lt;== gj04 1 the sailors rode the breeze clear of the rocks . gj04 1 the weights made the rope stretch over the pulley . gj04 1 the mechanical doll wriggled itself loose . cj99 1 if you had eaten more , you would want less . cj99 0 * as you eat the most , you want the least . cj99 0 * the more you would want , the less you would eat . cj99 0 * i demand that the more john eat , the more he pays . cj99 1 mary listens to the grateful dead , she gets depressed . cj99 1 the angrier mary got , the more she looked at pictures . cj99 1 the higher the stakes , the lower his expectations are . ==&gt; cola_public/tokenized/in_domain_train.tsv &lt;== gj04 1 our friends wo n&#39;t buy this analysis , let alone the next one we propose . gj04 1 one more pseudo generalization and i &#39;m giving up . gj04 1 one more pseudo generalization or i &#39;m giving up . gj04 1 the more we study verbs , the crazier they get . gj04 1 day by day the facts are getting murkier . gj04 1 i &#39;ll fix you a drink . gj04 1 fred watered the plants flat . gj04 1 bill coughed his way out of the restaurant . gj04 1 we &#39;re dancing the night away . gj04 1 herman hammered the metal flat . ==&gt; cola_public/tokenized/out_of_domain_dev.tsv &lt;== clc95 1 somebody just left - guess who . clc95 1 they claimed they had settled on something , but it was n&#39;t clear what they had settled on . clc95 1 if sam was going , sally would know where . clc95 1 they &#39;re going to serve the guests something , but it &#39;s unclear what . clc95 1 she &#39;s reading . i ca n&#39;t imagine what . clc95 1 john said joan saw someone from her graduating class . clc95 0 * john ate dinner but i do n&#39;t know who . clc95 0 * she mailed john a letter , but i do n&#39;t know to whom . clc95 1 i served leek soup to my guests . clc95 1 i served my guests . . 2.2. Parse . import pandas as pd # Load the dataset into a pandas dataframe. df = pd.read_csv(&quot;./cola_public/raw/in_domain_train.tsv&quot;, delimiter=&#39; t&#39;, header=None, names=[&#39;sentence_source&#39;, &#39;label&#39;, &#39;label_notes&#39;, &#39;sentence&#39;]) # Report the number of sentences. print(&#39;Number of training sentences: {:,} n&#39;.format(df.shape[0])) # Display 10 random rows from the data df.sample(10) . Number of training sentences: 8,551 . sentence_source label label_notes sentence . 1643 r-67 | 1 | NaN | I brought a razor with which to shave myself. | . 558 bc01 | 1 | NaN | A bear inhabits the cave. | . 7840 ad03 | 1 | NaN | The therapist&#39;s analysis of Lucy | . 4713 ks08 | 1 | NaN | This noise cannot be put up with. | . 5501 b_73 | 1 | NaN | Such a scholar as you were speaking of just no... | . 568 bc01 | 1 | NaN | The rolling stone avoided the river. | . 1920 r-67 | 1 | NaN | I would prefer it for there to be no talking. | . 2492 l-93 | 1 | NaN | He worked his way through the book. | . 5364 b_73 | 1 | NaN | Enough is going on to keep them confused. | . 5344 b_73 | 1 | NaN | Mary is taller than six feet. | . df[df.label == 0].sample(5)[[&#39;sentence&#39;, &#39;label&#39;]] . sentence label . 444 Mickey slips all the time up. | 0 | . 187 The more books I ask to whom he will give, the... | 0 | . 4866 This needs investigating the problem. | 0 | . 7780 We believed to be omnipotent. | 0 | . 1267 What table will he put the chair between and s... | 0 | . # Get the lists of sentences and their labels. sentences = df.sentence.values labels = df.label.values . 3. Tokenization &amp; Input Formatting . 3.1. BERT Tokenizer . from transformers import BertTokenizer # Load the BERT tokenizer. print(&#39;Loading BERT tokenizer...&#39;) tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-multilingual-uncased&#39;, do_lower_case=True) . Loading BERT tokenizer... . # Print the original sentence. print(&#39; Original: &#39;, sentences[0]) # Print the sentence split into tokens. print(&#39;Tokenized: &#39;, tokenizer.tokenize(sentences[0])) # Print the sentence mapped to token ids. print(&#39;Token IDs: &#39;, tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0]))) . Original: Our friends won&#39;t buy this analysis, let alone the next one we propose. Tokenized: [&#39;our&#39;, &#39;friends&#39;, &#39;won&#39;, &#34;&#39;&#34;, &#39;t&#39;, &#39;buy&#39;, &#39;this&#39;, &#39;analysis&#39;, &#39;,&#39;, &#39;let&#39;, &#39;alone&#39;, &#39;the&#39;, &#39;next&#39;, &#39;one&#39;, &#39;we&#39;, &#39;propose&#39;, &#39;.&#39;] Token IDs: [14008, 16119, 11441, 112, 162, 35172, 10372, 15559, 117, 12421, 19145, 10103, 12878, 10399, 11312, 25690, 119] . korean = &quot;안녕하세요. 반갑습니다. 너는 이름이 뭐니? 오늘 날씨가 맑고 좋구나.&quot; # Print the original sentence. print(&#39; Original: &#39;, korean) # Print the sentence split into tokens. print(&#39;Tokenized: &#39;, tokenizer.tokenize(korean)) # Print the sentence mapped to token ids. print(&#39;Token IDs: &#39;, tokenizer.convert_tokens_to_ids(tokenizer.tokenize(korean))) . Original: 안녕하세요. 반갑습니다. 너는 이름이 뭐니? 오늘 날씨가 맑고 좋구나. Tokenized: [&#39;ᄋ&#39;, &#39;##ᅡᆫ&#39;, &#39;##녀&#39;, &#39;##ᆼ&#39;, &#39;##하&#39;, &#39;##세&#39;, &#39;##요&#39;, &#39;.&#39;, &#39;ᄇ&#39;, &#39;##ᅡᆫ&#39;, &#39;##가&#39;, &#39;##ᆸ&#39;, &#39;##스&#39;, &#39;##ᆸ니다&#39;, &#39;.&#39;, &#39;ᄂ&#39;, &#39;##ᅥ&#39;, &#39;##는&#39;, &#39;이&#39;, &#39;##름이&#39;, &#39;ᄆ&#39;, &#39;##ᅯ&#39;, &#39;##니&#39;, &#39;?&#39;, &#39;ᄋ&#39;, &#39;##ᅩ&#39;, &#39;##ᄂ&#39;, &#39;##ᅳᆯ&#39;, &#39;날&#39;, &#39;##씨&#39;, &#39;##가&#39;, &#39;ᄆ&#39;, &#39;##ᅡ&#39;, &#39;##ᆰ&#39;, &#39;##고&#39;, &#39;ᄌ&#39;, &#39;##ᅩ&#39;, &#39;##ᇂ&#39;, &#39;##구&#39;, &#39;##나&#39;, &#39;.&#39;] Token IDs: [1174, 26646, 49345, 13045, 35132, 25169, 47024, 119, 1170, 26646, 11376, 17360, 13212, 79427, 119, 1165, 33645, 11192, 12398, 89420, 1169, 97090, 25536, 136, 1174, 29347, 97071, 63277, 76818, 47928, 11376, 1169, 25539, 97098, 12300, 1175, 29347, 97109, 16336, 16801, 119] . When we actually convert all of our sentences, we&#39;ll use the tokenize.encode functio to handle both steps, rather than calling tokenize and convert_tokens_to_ids seperately. . 3.2. Required Formatting . 3.3. Tokenize Dataset . max_len = 0 # For every sentence, for sent in sentences: # Tokenize the text and add `[CLS]` and `[SEP]` tokens. input_ids = tokenizer.encode(sent, add_special_tokens=True) # Update the maximum sentence langth. max_len = max(max_len, len(input_ids)) print(&#39;Max sentence length: &#39;, max_len) . Max sentence length: 48 . # Tokenize all of the sentences and map the tokens to their word IDs. input_ids = [] attention_masks = [] # For every sentence, for sent in sentences: # `encode_plus` will: # (1) Tokenize the sentence. # (2) Prepend the `[CLS]` token to the start. # (3) Append the `[SEP]` token to the end. # (4) Map tokens to their IDs. # (5) Pad or truncate the sentence to `max_length` # (6) Create attention masks for [PAD] tokens. encoded_dict = tokenizer.encode_plus( sent, # Sentence to encode. add_special_tokens = True, # Add &#39;[CLS]&#39; and &#39;[SEP]&#39; max_length = 64, # Pad &amp; truncate all sentences. pad_to_max_length = True, return_attention_mask = True, # Construct attn. masks. return_Tensors = &#39;pt&#39;, # Return pytorch tensors. ) # Add the encoded sentence to the list. input_ids.append(encoded_dict[&#39;input_ids&#39;]) # And its attention mask (simply differentiates padding from non-padding.) attention_masks.append(encoded_dict[&#39;attention_mask&#39;]) # Convert the lists into tensors. input_ids = torch.LongTensor(input_ids) attention_masks = torch.FloatTensor(attention_masks) labels = torch.tensor(labels) # Print sentence 0, now as a list of IDs. print(&#39;Original: &#39;, sentences[0]) print(&#39;Token IDs: &#39;, input_ids[0]) . Original: Our friends won&#39;t buy this analysis, let alone the next one we propose. Token IDs: tensor([ 101, 14008, 16119, 11441, 112, 162, 35172, 10372, 15559, 117, 12421, 19145, 10103, 12878, 10399, 11312, 25690, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) . 3.4. Training &amp; Validation Split . from torch.utils.data import TensorDataset, random_split # Combine the training inputs into a TensorDataset. dataset = TensorDataset(input_ids, attention_masks, labels) # Create a 90-10 train-validation split. # Calculate the number of samples to include in each set. train_size = int(0.9 * len(dataset)) val_size = len(dataset) - train_size # Devide the dataset by randomly selecting samples. train_dataset, val_dataset = random_split(dataset, [train_size, val_size]) print(&#39;{:&gt;5} training samples&#39;.format(train_size)) print(&#39;{:&gt;5} validation samples&#39;.format(val_size)) . 7695 training samples 856 validation samples . from torch.utils.data import DataLoader, RandomSampler, SequentialSampler # The DataLoader needs to know our batch size for training, so we specify it here. # For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32. batch_size = 32 # Create the DataLoaders for our training and validation sets. # We&#39;ll take training samples in random order. train_dataloader = DataLoader( train_dataset, # The training samples. sampler = RandomSampler(train_dataset), # Select batches randomly batch_size = batch_size # Trains with this batch size. ) # For validation the order doesn&#39;t matter, so we&#39;ll just read them sequentially. validation_dataloader = DataLoader( val_dataset, # The validation samples. sampler = SequentialSampler(val_dataset), # Pull out batches sequentially. batch_size = batch_size # Evaluate with this batch size. ) . train_dataloader.batch_size # 7695/32 = 240.46875 . 32 . 4. Train Our Classification Model . 4.1. BertForSequenceClassification . BertModel | BertForPreTraining | BertForMaskedLM | BertForNextSentencePredicion | BertForSequenceClassification | BertForTokenClassification | BertForQuestionAnswering | . device . device(type=&#39;cuda&#39;) . from transformers import BertForSequenceClassification, AdamW, BertConfig # Load BertForSequenceClassification, the pretrained BERT model with # a single linear classification layer on top. model = BertForSequenceClassification.from_pretrained( &quot;bert-base-uncased&quot;, # Use the 12-layer BERT model, with an uncased vocab. num_labels = 2, # The number of output labels--2 for binary classification. # You can increase this for multi-class tasks. output_attentions = False, # Whether the model returns attentions weights. output_hidden_states = False, # Whether the model returns all hidden-states. ) # Tell pytorch to run this model on the GPU. # if device == &#39;cuda&#39;: model.cuda() . BertForSequenceClassification( (bert): BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) (dropout): Dropout(p=0.1) (classifier): Linear(in_features=768, out_features=2, bias=True) ) . # Get all of the model&#39;s parameters as a list of tuples. params = list(model.named_parameters()) print(&#39;The BERT model has {:} different named parameters. n&#39;.format(len(params))) print(&#39;==== Embedding Layer ==== n&#39;) for p in params[0:5]: print(&quot;{:&lt;55} {:&gt;12}&quot;.format(p[0], str(tuple(p[1].size())))) print(&#39; n==== First Transformer ==== n&#39;) for p in params[5:21]: print(&quot;{:&lt;55} {:&gt;12}&quot;.format(p[0], str(tuple(p[1].size())))) print(&#39; n==== Output Layer ==== n&#39;) for p in params[-4:]: print(&quot;{:&lt;55} {:&gt;12}&quot;.format(p[0], str(tuple(p[1].size())))) . The BERT model has 201 different named parameters. ==== Embedding Layer ==== bert.embeddings.word_embeddings.weight (30522, 768) bert.embeddings.position_embeddings.weight (512, 768) bert.embeddings.token_type_embeddings.weight (2, 768) bert.embeddings.LayerNorm.weight (768,) bert.embeddings.LayerNorm.bias (768,) ==== First Transformer ==== bert.encoder.layer.0.attention.self.query.weight (768, 768) bert.encoder.layer.0.attention.self.query.bias (768,) bert.encoder.layer.0.attention.self.key.weight (768, 768) bert.encoder.layer.0.attention.self.key.bias (768,) bert.encoder.layer.0.attention.self.value.weight (768, 768) bert.encoder.layer.0.attention.self.value.bias (768,) bert.encoder.layer.0.attention.output.dense.weight (768, 768) bert.encoder.layer.0.attention.output.dense.bias (768,) bert.encoder.layer.0.attention.output.LayerNorm.weight (768,) bert.encoder.layer.0.attention.output.LayerNorm.bias (768,) bert.encoder.layer.0.intermediate.dense.weight (3072, 768) bert.encoder.layer.0.intermediate.dense.bias (3072,) bert.encoder.layer.0.output.dense.weight (768, 3072) bert.encoder.layer.0.output.dense.bias (768,) bert.encoder.layer.0.output.LayerNorm.weight (768,) bert.encoder.layer.0.output.LayerNorm.bias (768,) ==== Output Layer ==== bert.pooler.dense.weight (768, 768) bert.pooler.dense.bias (768,) classifier.weight (2, 768) classifier.bias (2,) . 4.2. Optimizer &amp; Learning Rate Scheduler . # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the &#39;W&#39; stands for &#39;Weight Decay fix&#39; optimizer = AdamW(model.parameters(), lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5 eps = 1e-8 # args.dam_epsilon - default is 1e-8 ) . from transformers import get_linear_schedule_with_warmup # Number of training epochs. The BERT authors recommend between 2 and 4. # We chose to run for 4, but we&#39;ll see later that this my be over-fitting the training data. epochs = 4 # Total number of training steps is [number of batches] x [number of epochs]. # (Note that this is not the same as the number of training samples). total_steps = len(train_dataloader) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, # Default value in run_glue.py num_training_steps = total_steps) . 4.3. Training Loop . Training: . Unpack our data inputs and labels | Load data onto the GPU for acceleration | Clear out the gradients calculated in the previous pass. In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out. | . | Forward pass (feed input data through the network) | Backward pass (backpropagation) | Tell the network to update parameters with optimizer.step() | Track variables for monitoring progress | . Evaluation: . Unpack our data inputs and labels | Load data onto the GPU for acceleration | Forward pass (feed input data through the network) | Compute loss on our validation data and track variables for monitoring progress | . import numpy as np # Function to calculate the accuracy of our predictions vs labels def flat_accuracy(preds, labels): pred_flat = np.argmax(preds, axis=1).flatten() labels_flat = labels.flatten() return np.sum(pred_flat == labels_flat) / len(labels_flat) . import time import datetime def format_time(elapsed): &#39;&#39;&#39; Takes a time in seconds and returns a string hh:mm:ss &#39;&#39;&#39; # Round to the nearest second. elapsed_rounded = int(round((elapsed))) # Format as hh:mm:ss print(&quot; &quot;, elapsed, elapsed_rounded, str(datetime.timedelta(seconds=elapsed_rounded))) return str(datetime.timedelta(seconds=elapsed_rounded)) . import random import numpy as np # This training code is based on the `run_glue.py` script here: # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128 # Set the seed value all over the place to make this reproducible. seed_val = 42 random.seed(seed_val) np.random.seed(seed_val) torch.manual_seed(seed_val) torch.cuda.manual_seed_all(seed_val) # We&#39;ll store a number of quantities such as training and validation loss, # validation accuracy, and timings. training_stats = [] # Measure the total training time for the whole run. total_t0 = time.time() # For each epoch, for epoch_i in range(0, epochs): # ======================================== # Training # ======================================== # Perform one full pass over the training set. print(&quot;&quot;) print(&#39;======== Epoch {:} / {:} ========&#39;.format(epoch_i+1, epochs)) print(&#39;Training...&#39;) # Measure how long the training epoch takes. t0 = time.time() # Reset the total loss for this epoch. total_train_loss = 0 # Put the model into training mode. Don&#39;t be mislead--the call to # `train` just changes the *mode*, it doesn&#39;t *perform* the training. # `dropout` and `batchnorm` layers behave differently during training vs test # (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch) model.train() # For each batch of training data, for step, batch in enumerate(train_dataloader): # # of step : 241 # len of batch : 32 # size of batch[0] : 64 # Progress update every 40 batches. if step % 40 == 0 and not step == 0: # Calcuate elapsed time in minutes. elapsed = format_time(time.time() - t0) # Report progress. print(&#39; Batch {:&gt;5,} of {:&gt;5,}. Elapsed: {:}&#39;.format(step, len(train_dataloader), elapsed)) # Unpack this training batch from our dataloader # # As we unpack the batch, we&#39;ll also copy each tensor to the GPU using `to` method. # # `batch` contains three pytorch tensors: # [0]: input ids # [1] : attention masks # [2] : labels b_input_ids = batch[0].to(device) b_input_mask = batch[1].to(device) b_labels = batch[2].to(device) print(type(batch[0])) print(batch[0]) print(type(b_input_ids)) print(b_input_ids) # Always clear any previously calculated gradients before performing backward pass. # PyTorch doesn&#39;t do this automatically because accumulating the gradients is # &quot;convenient while training RNNs&quot;. (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch) model.zero_grad() # Perform a forwad pass (evaluate the model on this training batch). # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification # It returns different numbers of parameters depending on what arguments # are given and what flags are set. For our usage here, it returns the loss (because we provided labels) # and the &quot;logits&quot;--the model outputs prior to activation. loss, logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end. # `loss` is a Tensor containing a single value; the `.item()` function just returns the Python value from the tensor. total_train_loss += loss.item() # Perform a backward pass to calculate the gradients. loss.backward() # Clip the norm of the gradients to 1.0. # This is to help prevent the &quot;exploding gradients&quot; problem. torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Update parameters and take a step using the computed gradient. # The optimizer dictates the &quot;update rule&quot;--how the parameters are modified # based on their gradients, the learning rate, etc. optimizer.step() # Update the learning rate. scheduler.step() # Calculate the average loss over all of the batches. avg_train_loss = total_train_loss / len(train_dataloader) # Measure how long this epoch took. training_time = format_time(time.time() - t0) print(&quot;&quot;) print(&quot; Average training loss: {0:.2f}&quot;.format(avg_train_loss)) print(&quot; Training epoch took: {:}&quot;.format(training_time)) # ======================================== # Validation # ======================================== # After the completion of each training epoch, measure our performance on our validation set. print(&quot;&quot;) print(&quot;Running Validation...&quot;) t0 = time.time() # Put the model in evaluation mode--the dropout layers behave differently during evaluation. model.eval() # Tracking variables total_eval_accuracy = 0 total_eval_loss = 0 nb_eval_steps = 0 # Evaluate data for one epoch for batch in validation_dataloader: # Unpack this training batch from our dataloader. # # As we unpack the batch, we&#39;ll also copy each tensor to the GPU using the `to` method. # # `batch` contains three pytorch tensors: # [0]: input ids # [1]: attention masks # [2]: labels b_input_ids = batch[0].to(device) b_input_mask = batch[1].to(device) b_labels = batch[2].to(device) # Tell pytorch not to bother with constructing the compute graph during the forward pass, # since this is only needed for backprop (training). with torch.no_grad(): # Forward pass, calculate logit predictions. # token_type_ids is the same as the &quot;segment ids&quot;, which differentiates sentence 1 and 2 in 2-sentence tasks. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification # Get the &quot;logits&quot; output byt the model. The &quot;logits&quot; are the output # values prior to applying an activation function like the softmax. (loss, logits) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) # Accumulate the validation loss. total_eval_loss += loss.item() # Move logits and labels to CPU logits = logits.detach().cpu().numpy() label_ids = b_labels.to(&#39;cpu&#39;).numpy() # Calculate the accuracy for this batch of test sentence, # and accumulate it over all batches. total_eval_accuracy += flat_accuracy(logits, label_ids) # Report the final accuracy for this validation run. avg_val_accuracy = total_eval_accuracy / len(validation_dataloader) print(&quot; Accuracy: {0:.2f}&quot;.format(avg_val_accuracy)) # Calculate the average loss over all of the batches. avg_val_loss = total_eval_loss / len(validation_dataloader) # Measure how long the validation run took. validation_time = format_time(time.time() - t0) print(&quot; Validation Loss: {0:.2f}&quot;.format(avg_val_loss)) print(&quot; Validation took: {:}&quot;.format(validation_time)) # Record all statistics from this epoch. training_stats.append( { &#39;epoch&#39;: epoch_i + 1, &#39;Training Loss&#39;: avg_train_loss, &#39;Valid. Loss&#39;: avg_val_loss, &#39;Valid. Accur.&#39;: avg_val_accuracy, &#39;Training Time&#39;: training_time, &#39;Validation Time&#39;: validation_time } ) print(&quot;&quot;) print(&quot;Training complete!&quot;) print(&quot;Total training took {:} (h:mm:ss)&quot;.format(format_time(time.time()-total_t0))) . ======== Epoch 1 / 4 ======== Training... &lt;class &#39;torch.Tensor&#39;&gt; tensor([[ 101, 11312, 10320, ..., 0, 0, 0], [ 101, 13667, 10564, ..., 0, 0, 0], [ 101, 10103, 23222, ..., 0, 0, 0], ..., [ 101, 21506, 10935, ..., 0, 0, 0], [ 101, 69042, 10127, ..., 0, 0, 0], [ 101, 12941, 50134, ..., 0, 0, 0]]) &lt;class &#39;torch.Tensor&#39;&gt; tensor([[ 101, 11312, 10320, ..., 0, 0, 0], [ 101, 13667, 10564, ..., 0, 0, 0], [ 101, 10103, 23222, ..., 0, 0, 0], ..., [ 101, 21506, 10935, ..., 0, 0, 0], [ 101, 69042, 10127, ..., 0, 0, 0], [ 101, 12941, 50134, ..., 0, 0, 0]], device=&#39;cuda:0&#39;) . RuntimeError Traceback (most recent call last) &lt;ipython-input-26-443d4943574d&gt; in &lt;module&gt;() 90 token_type_ids=None, 91 attention_mask=b_input_mask, &gt; 92 labels=b_labels) 93 94 # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end. /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 487 result = self._slow_forward(*input, **kwargs) 488 else: --&gt; 489 result = self.forward(*input, **kwargs) 490 for hook in self._forward_hooks.values(): 491 hook_result = hook(self, input, result) /usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels) 1174 position_ids=position_ids, 1175 head_mask=head_mask, -&gt; 1176 inputs_embeds=inputs_embeds, 1177 ) 1178 /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 487 result = self._slow_forward(*input, **kwargs) 488 else: --&gt; 489 result = self.forward(*input, **kwargs) 490 for hook in self._forward_hooks.values(): 491 hook_result = hook(self, input, result) /usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask) 781 782 embedding_output = self.embeddings( --&gt; 783 input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds 784 ) 785 encoder_outputs = self.encoder( /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 487 result = self._slow_forward(*input, **kwargs) 488 else: --&gt; 489 result = self.forward(*input, **kwargs) 490 for hook in self._forward_hooks.values(): 491 hook_result = hook(self, input, result) /usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py in forward(self, input_ids, token_type_ids, position_ids, inputs_embeds) 177 embeddings = inputs_embeds + position_embeddings + token_type_embeddings 178 embeddings = self.LayerNorm(embeddings) --&gt; 179 embeddings = self.dropout(embeddings) 180 return embeddings 181 /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 487 result = self._slow_forward(*input, **kwargs) 488 else: --&gt; 489 result = self.forward(*input, **kwargs) 490 for hook in self._forward_hooks.values(): 491 hook_result = hook(self, input, result) /usr/local/lib/python3.6/dist-packages/torch/nn/modules/dropout.py in forward(self, input) 56 @weak_script_method 57 def forward(self, input): &gt; 58 return F.dropout(input, self.p, self.training, self.inplace) 59 60 /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in dropout(input, p, training, inplace) 747 return (_VF.dropout_(input, p, training) 748 if inplace --&gt; 749 else _VF.dropout(input, p, training)) 750 751 RuntimeError: Creating MTGP constants failed. at /pytorch/aten/src/THC/THCTensorRandom.cu:35 .",
            "url": "https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning-Sentence-Classification.html",
            "relUrl": "/bert/jupyter/2020/04/19/BERT_Fine-Tuning-Sentence-Classification.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Inspect Bert Vocabulary",
            "content": "Load the model . Install the huggingface implementation. . !pip install pytorch-pretrained-bert -q . import torch from pytorch_pretrained_bert import BertTokenizer # Load pre-trained model tokenizer (vocabulary) tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-multilingual-uncased&#39;) # bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, # bert-base-multilingual-uncased, bert-base-multilingual-cased, # bert-base-chinese . Inspect BERT Vocabulary . Vocab Dump . Retrieve the entire list of &quot;tokens&quot; and write these out to text files so we can peruse them. . with open(&quot;vocabulary.txt&quot;, &#39;w&#39;, encoding=&quot;utf-8&quot;) as f: for token in tokenizer.vocab.keys(): f.write(token+&#39; n&#39;) . Single Characters . one_chars = [] one_chars_hashes = [] # For each token in the vocabulary, for token in tokenizer.vocab.keys(): # Record any single-character tokens. if len(token) == 1: one_chars.append(token) # Record single-character tokens preceded by the two hashes. elif len(token) == 3 and token[0:2] == &#39;##&#39;: one_chars_hashes.append(token) . print(&#39;Number of single character tokens:&#39;, len(one_chars), &#39; n&#39;) # Print all of the single characters, 40 per row. # For every batch of 40 tokens, for i in range(0, len(one_chars), 40): # Limit the end index so we don&#39;t go past the end of the list. end = min(i+40, len(one_chars)+1) # Print out the tokens, seperated by a space. print(&#39; &#39;.join(one_chars[i:end])) . Number of single character tokens: 9995 ! &#34; # $ % &amp; &#39; ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; &lt; = &gt; ? @ [ ] ^ _ a b c d e f g h i j k l m n o p q r s t u v w x y z { | } ~ ¡ ¢ £ ¤ ¥ ¦ § ¨ © ª « ¬ ® ¯ ° ± ² ³ µ ¶ · ¹ º » ¼ ½ ¾ ¿ × ß æ ð ÷ ø þ đ ħ ı ĳ ł ŉ ŋ œ ſ ƒ ǁ ɐ ɑ ɒ ɓ ɔ ɕ ə ɛ ɟ ɡ ɣ ɤ ɦ ɨ ɪ ɫ ɬ ɯ ɲ ɴ ɸ ɹ ɾ ʀ ʁ ʂ ʃ ʉ ʊ ʋ ʌ ʎ ʐ ʑ ʒ ʔ ʕ ʙ ʝ ʤ ʦ ʧ ʰ ʲ ʳ ʷ ʸ ʻ ʼ ʾ ʿ ˁ ˈ ˋ ˌ ː ˙ ˚ ˡ ˢ ˣ ΄ α β γ δ ε ζ η θ ι κ λ μ ν ξ ο π ρ ς σ τ υ φ χ ψ ω ϕ а б в г д е ж з и к л м н о п р с т у ф х ц ч ш щ ъ ы ь э ю я ђ є ѕ і ј љ њ ћ џ ѣ ѧ ѫ ѳ ґ ғ җ ҙ қ ҡ ң ҫ ү ұ ҳ ҷ ҹ һ ӊ ӏ ә ө ՚ ՛ ՜ ՝ ՞ ա բ գ դ ե զ է ը թ ժ ի լ խ ծ կ հ ձ ղ ճ մ յ ն շ ո չ պ ջ ռ ս վ տ ր ց ւ փ ք օ ֆ և ։ ֊ ־ ׃ א ב ג ד ה ו ז ח ט י ך כ ל ם מ ן נ ס ע ף פ ץ צ ק ר ש ת ׳ ״ ، ؍ ؛ ؟ ء ا ب ة ت ث ج ح خ د ذ ر ز س ش ص ض ط ظ ع غ ـ ف ق ك ل م ن ه و ى ي ٠ ١ ٢ ٣ ٤ ٥ ٦ ٧ ٨ ٩ ٪ ٫ ٬ ٭ ٴ ٹ پ چ ڈ ڑ ژ ڠ ڤ ک ڭ گ ں ھ ہ ۃ ۆ ۇ ۋ ی ێ ے ۔ ە ۰ ۱ ۲ ۳ ۴ ۵ ۶ ۷ ۸ ۹ ݨ ः अ आ इ ई उ ऊ ऋ ऍ ऎ ए ऐ ऑ ऒ ओ औ क ख ग घ ङ च छ ज झ ञ ट ठ ड ढ ण त थ द ध न प फ ब भ म य र ल ळ व श ष स ह ऽ ा ि ी ॉ ॊ ो ौ ॐ ॠ । ॥ ० १ २ ३ ४ ५ ६ ७ ८ ९ ॰ ॲ ং ঃ অ আ ই ঈ উ ঊ ঋ এ ঐ ও ঔ ক খ গ ঘ ঙ চ ছ জ ঝ ঞ ট ঠ ড ঢ ণ ত থ দ ধ ন প ফ ব ভ ম য র ল শ ষ স হ া ি ী ে ৈ ৎ ৗ ০ ১ ২ ৩ ৪ ৫ ৬ ৭ ৮ ৯ ৰ ৱ ৷ ਅ ਆ ਇ ਈ ਉ ਊ ਏ ਐ ਓ ਔ ਕ ਖ ਗ ਘ ਚ ਛ ਜ ਝ ਟ ਠ ਡ ਢ ਣ ਤ ਥ ਦ ਧ ਨ ਪ ਫ ਬ ਭ ਮ ਯ ਰ ਲ ਵ ਸ ਹ ਾ ਿ ੀ ੜ ੦ ੧ ੨ ੩ ੪ ੫ ੬ ੭ ੮ ੯ ੲ ੳ ઃ અ આ ઇ ઈ ઉ ઊ ઋ ઍ એ ઐ ઑ ઓ ઔ ક ખ ગ ઘ ચ છ જ ઝ ઞ ટ ઠ ડ ઢ ણ ત થ દ ધ ન પ ફ બ ભ મ ય ર લ ળ વ શ ષ સ હ ા િ ી ૉ ો ૌ ૦ ૧ ૨ ૩ ૪ ૫ ૬ ૭ ૮ ૯ ஃ அ ஆ இ ஈ உ ஊ எ ஏ ஐ ஒ ஓ க ங ச ஜ ஞ ட ண த ந ன ப ம ய ர ற ல ள ழ வ ஷ ஸ ஹ ா ி ு ூ ெ ே ை ௗ ఁ ం ః అ ఆ ఇ ఈ ఉ ఊ ఋ ఎ ఏ ఐ ఒ ఓ ఔ క ఖ గ ఘ ఙ చ ఛ జ ఝ ఞ ట ఠ డ ఢ ణ త థ ద ధ న ప ఫ బ భ మ య ర ఱ ల ళ వ శ ష స హ ు ూ ృ ಂ ಃ ಅ ಆ ಇ ಈ ಉ ಊ ಋ ಎ ಏ ಐ ಒ ಓ ಔ ಕ ಖ ಗ ಘ ಚ ಛ ಜ ಝ ಞ ಟ ಠ ಡ ಢ ಣ ತ ಥ ದ ಧ ನ ಪ ಫ ಬ ಭ ಮ ಯ ರ ಲ ಳ ವ ಶ ಷ ಸ ಹ ಾ ು ೂ ೃ ೕ ೖ ೦ ೧ ೨ ೩ ೪ ೫ ೬ ೭ ೮ ೯ ം ഃ അ ആ ഇ ഈ ഉ ഊ ഋ എ ഏ ഐ ഒ ഓ ഔ ക ഖ ഗ ഘ ങ ച ഛ ജ ഝ ഞ ട ഠ ഡ ഢ ണ ത ഥ ദ ധ ന പ ഫ ബ ഭ മ യ ര റ ല ള ഴ വ ശ ഷ സ ഹ ാ ി ീ െ േ ൈ ൗ ൧ ൨ ൺ ൻ ർ ൽ ൾ ൿ ක ද ප ම ය ර ල ශ ා ෙ ก ข ค ง จ ด ต ท น บ ป ภ ม ย ร ล ว ส ห อ า ำ เ แ ไ ་ ། ཀ ཁ ག ང ཆ ད ན པ ཕ བ མ ཚ ཟ འ ར ལ ཤ ས က ခ ဂ ဃ င စ ဆ ဇ ဈ ဉ ည ဋ ဌ ဍ ဏ တ ထ ဒ ဓ န ပ ဖ ဗ ဘ မ ယ ရ လ ဝ သ ဟ ဠ အ ဤ ဥ ဧ ဩ ါ ာ ေ း ျ ြ ဿ ၀ ၁ ၂ ၃ ၄ ၅ ၆ ၇ ၈ ၉ ၊ ။ ၌ ၍ ၎ ၏ ა ბ გ დ ე ვ ზ თ ი კ ლ მ ნ ო პ ჟ რ ს ტ უ ფ ქ ღ ყ შ ჩ ც ძ წ ჭ ხ ჯ ჰ ჲ ᄀ ᄁ ᄂ ᄃ ᄄ ᄅ ᄆ ᄇ ᄈ ᄉ ᄊ ᄋ ᄌ ᄍ ᄎ ᄏ ᄐ ᄑ ᄒ ᅡ ᅢ ᅣ ᅤ ᅥ ᅦ ᅧ ᅨ ᅩ ᅪ ᅫ ᅬ ᅭ ᅮ ᅯ ᅰ ᅱ ᅲ ᅳ ᅴ ᅵ ᆨ ᆩ ᆪ ᆫ ᆬ ᆭ ᆮ ᆯ ᆰ ᆱ ᆲ ᆵ ᆶ ᆷ ᆸ ᆹ ᆺ ᆻ ᆼ ᆽ ᆾ ᆿ ᇀ ᇁ ᇂ ᛫ ᛬ ᠠ ᠨ ᠬ ᠭ ᠰ ᡳ ᴬ ᴮ ᴰ ᴵ ᴶ ᴷ ᴺ ᴼ ᴾ ᴿ ᵀ ᵂ ᵃ ᵇ ᵈ ᵉ ᵍ ᵏ ᵐ ᵒ ᵖ ᵗ ᵘ ᵛ ᵢ ᵣ ᵤ ᵥ ᶜ ᶠ ᶻ ᾽ ‖ ‚ ‛ „ ‟ † ‡ • ․ ‥ ‧ ‰ ′ ″ ‹ › ※ ‼ ‿ ⁄ ⁰ ⁱ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ⁺ ⁻ ⁾ ⁿ ₀ ₁ ₂ ₃ ₄ ₅ ₆ ₇ ₈ ₉ ₊ ₍ ₎ ₐ ₑ ₒ ₓ ₕ ₖ ₗ ₘ ₙ ₚ ₛ ₜ ₤ ₩ ₪ € ₱ ₴ ₹ ℂ ℃ ℓ ℕ № ℚ ℝ ™ ℤ ⅓ ⅔ ⅛ ⅰ ⅱ ⅲ ⅳ ⅴ ⅵ ⅶ ← ↑ → ↓ ↔ ↗ ↦ ↺ ⇄ ⇌ ⇒ ⇔ ⇧ ∀ ∂ ∃ ∅ ∆ ∇ ∈ ∑ − ∕ ∗ ∘ ∙ √ ∞ ∥ ∧ ∨ ∩ ∪ ∫ ∴ ∼ ≅ ≈ ≒ ≡ ≤ ≥ ≪ ≫ ⊂ ⊃ ⊆ ⊕ ⊗ ⊙ ⊞ ⊥ ⋅ ⋯ ① ② ③ ④ ⑤ ⑥ ⑦ ⑧ ⓐ ─ ━ │ ┃ ┏ ┓ └ ┗ ┛ ├ ┣ ┫ ┬ ┳ ║ ╱ █ ■ □ ▪ ▬ ▭ ▲ △ ▶ ► ▼ ▽ ◆ ◇ ◊ ○ ◌ ◎ ● ◡ ◦ ◯ ★ ☆ ☉ ♀ ♂ ♕ ♖ ♗ ♘ ♙ ♠ ♡ ♣ ♥ ♦ ♪ ♭ ♯ ⚭ ✝ ⟨ ⟩ ⟪ ⟫ ⟶ ⱼ ⵏ ⺩ ⽥ 、 。 〃 々 〆 〇 〈 〉 《 》 「 」 『 』 【 】 〒 〔 〕 〖 〗 〜 〝 〞 〟 ぁ あ ぃ い ぅ う ぇ え ぉ お か き く け こ さ し す せ そ た ち っ つ て と な に ぬ ね の は ひ ふ へ ほ ま み む め も ゃ や ゅ ゆ ょ よ ら り る れ ろ わ ゐ ゑ を ん ゝ ァ ア ィ イ ゥ ウ ェ エ ォ オ カ キ ク ケ コ サ シ ス セ ソ タ チ ッ ツ テ ト ナ ニ ヌ ネ ノ ハ ヒ フ ヘ ホ マ ミ ム メ モ ャ ヤ ュ ユ ョ ヨ ラ リ ル レ ロ ワ ヰ ヱ ヲ ン ヵ ヶ ・ ー ㄚ ㄧ ㄨ ㄱ ㄴ ㄷ ㄹ ㅁ ㅂ ㅅ ㅇ ㅈ ㅡ ㅣ ㆍ ㈱ ㎒ ㎞ ㎡ 㭴 㹴 䜣 䲁 䲗 䲢 一 丁 七 万 丈 三 上 下 不 与 丐 丑 专 且 丕 世 丘 丙 业 丛 东 丝 丞 丟 両 丢 两 严 並 丧 个 丫 中 丰 串 临 丶 丸 丹 为 主 丼 丽 举 乂 乃 久 么 义 之 乌 乍 乎 乏 乐 乒 乓 乔 乖 乗 乘 乙 乜 九 乞 也 习 乡 书 乩 乭 买 乱 乳 乾 亀 亂 了 予 争 事 二 于 亏 云 互 五 井 亘 亙 亚 些 亜 亞 亟 亡 亢 交 亥 亦 产 亨 亩 享 京 亭 亮 亰 亲 亳 亵 亶 人 亻 亿 什 仁 仃 仄 仅 仆 仇 今 介 仍 从 仏 仑 仓 仔 仕 他 仗 付 仙 仝 仞 仟 仡 代 令 以 仪 们 仮 仰 仲 仵 件 价 任 份 仿 企 伉 伊 伋 伍 伎 伏 伐 休 伕 众 优 伙 会 伝 伞 伟 传 伤 伦 伪 伫 伯 估 伴 伶 伷 伸 伺 似 伽 佃 但 佇 佈 佉 位 低 住 佐 佑 体 佔 何 佗 佘 余 佚 佛 作 佞 佟 你 佢 佣 佤 佥 佩 佬 佯 佰 佳 併 佶 佺 佻 佼 佾 使 侂 侃 侄 來 侈 例 侍 侏 侑 侖 侗 侘 供 依 侠 価 侣 侥 侦 侧 侨 侪 侬 侭 侮 侯 侵 侶 侷 便 俁 係 促 俄 俅 俊 俎 俏 俐 俑 俗 俘 俚 保 俞 俟 俠 信 俣 俥 俦 俨 俩 俪 俬 俭 修 俯 俱 俳 俵 俶 俸 俺 俾 倂 倆 倉 個 倍 們 倒 倓 倔 倖 倘 候 倚 倜 借 倡 倣 値 倦 倧 倩 倪 倫 倬 倭 倮 倶 倹 债 倻 值 倾 偁 偃 假 偈 偉 偏 偕 做 停 健 偰 偲 側 偵 偶 偷 偽 偿 傀 傅 傈 傉 傍 傑 傕 傘 備 傚 傢 傣 储 催 傭 傲 傳 債 傷 傻 傾 僅 僉 僊 働 像 僑 僕 僖 僚 僞 僥 僧 僭 僮 僱 僳 僵 價 僻 儀 儁 儂 億 儆 儉 儋 儒 儔 儕 儘 儚 償 儡 優 儲 儷 儸 儺 儼 儿 兀 允 元 兄 充 兆 兇 先 光 克 兌 免 兎 児 兑 兒 兔 兖 兗 党 兜 兢 入 內 全 兩 兪 八 公 六 兮 兰 共 关 兴 兵 其 具 典 兹 养 兼 兽 冀 内 円 冇 冈 冉 冊 册 再 冏 冑 冒 冕 冗 写 军 农 冠 冢 冤 冥 冨 冪 冬 冯 冰 冲 决 冴 况 冶 冷 冻 冼 冽 净 凄 准 凈 凉 凊 凋 凌 凍 减 凑 凖 凛 凜 凝 几 凡 凤 処 凧 凪 凫 凭 凯 凰 凱 凳 凶 凸 凹 出 击 凼 函 凿 刀 刁 刃 分 切 刈 刊 刍 刎 刑 划 列 刘 则 刚 创 初 删 判 別 刨 利 刪 别 刮 到 刳 制 刷 券 刹 刺 刻 刽 剁 剂 剃 則 削 剋 剌 前 剎 剑 剔 剖 剛 剝 剡 剣 剤 剥 剧 剩 剪 副 剰 剱 割 創 剷 剽 剿 劃 劇 劈 劉 劊 劍 劑 劔 力 劝 办 功 加 务 劢 劣 动 助 努 劫 劭 励 劲 劳 労 劵 効 劻 劾 势 勁 勃 勅 勇 勉 勋 勐 勒 勔 動 勖 勗 勘 務 勛 勝 勞 募 勢 勣 勤 勧 勰 勱 勲 勳 勵 勸 勺 勻 勾 勿 匀 匁 匂 包 匆 匈 匍 匏 匐 匕 化 北 匙 匝 匠 匡 匣 匪 匮 匯 匱 匹 区 医 匾 匿 區 十 千 卅 升 午 卉 半 卍 华 协 卐 卑 卒 卓 協 单 卖 南 単 博 卜 卞 卟 占 卡 卢 卤 卦 卧 卫 卬 卯 印 危 即 却 卵 卷 卸 卹 卻 卽 卿 厂 厄 厅 历 厉 压 厌 厓 厔 厕 厘 厚 厝 原 厢 厥 厦 厨 厩 厭 厮 厲 厳 去 县 叁 参 參 叄 又 叉 及 友 双 反 収 发 叔 取 受 变 叙 叛 叟 叠 叡 叢 口 古 句 另 叨 叩 只 叫 召 叭 叮 可 台 叱 史 右 叶 号 司 叹 叻 叼 吁 吃 各 合 吉 吊 吋 同 名 后 吏 吐 向 吒 吓 吕 吗 君 吝 吞 吟 吠 吡 否 吧 吨 吩 含 听 吮 启 吱 吲 吳 吴 吵 吶 吸 吹 吻 吼 吽 吾 呀 呂 呃 呆 呈 呉 告 呋 呎 呐 呑 呕 呗 员 呛 呜 呟 呢 呤 呦 周 呪 呱 味 呵 呷 呻 呼 命 咀 咁 咄 咆 咋 和 咎 咏 咐 咒 咔 咕 咖 咗 咙 咚 咤 咥 咧 咨 咩 咪 咫 咬 咭 咯 咱 咲 咳 咸 咽 咿 哀 品 哂 哄 哆 哇 哈 哉 哋 哌 响 哎 哏 哑 哒 哔 哗 哙 哚 哟 員 哥 哦 哨 哩 哪 哭 哮 哲 哺 哼 哽 唁 唄 唆 唇 唎 唐 唑 唔 唖 唛 唢 唤 唧 唬 唭 售 唯 唱 唵 唷 唸 唾 啃 啄 商 啉 啊 問 啓 啖 啜 啞 啟 啡 啤 啥 啦 啪 啬 啮 啰 啲 啶 啸 啼 啾 喀 喂 喃 善 喆 喇 喉 喊 喋 喔 喘 喙 喚 喜 喝 喧 喩 喪 喫 喬 單 喰 喱 喲 喵 営 喷 喹 喺 喻 嗅 嗆 嗇 嗎 嗓 嗔 嗚 嗜 嗝 嗟 嗡 嗣 嗤 嗨 嗩 嗪 嗯 嗲 嗶 嗽 嘅 嘆 嘈 嘉 嘌 嘎 嘏 嘔 嘗 嘘 嘛 嘜 嘟 嘢 嘧 嘩 嘯 嘱 嘲 嘴 嘶 嘹 嘻 嘿 噁 噂 噌 噏 噓 噗 噛 噜 噠 噢 噤 器 噩 噪 噫 噬 噱 噲 噴 噶 噸 噹 噺 噻 嚆 嚇 嚈 嚎 嚏 嚐 嚕 嚙 嚟 嚢 嚣 嚥 嚨 嚮 嚴 嚷 嚼 囁 囂 囃 囉 囊 囍 囑 囓 囗 囚 四 囝 回 因 囡 团 団 囤 囧 囪 园 囮 困 囱 囲 図 围 固 国 图 囿 圀 圃 圆 圈 圉 國 圍 圏 園 圓 圖 團 圜 土 圣 圧 在 圩 圪 圭 圮 圯 地 圳 圹 场 圻 圾 址 坂 均 坊 坌 坍 坎 坏 坐 坑 块 坚 坛 坜 坝 坞 坟 坠 坡 坤 坦 坨 坩 坪 坬 坭 坯 坳 坵 坷 坻 垂 垃 垄 垅 垈 型 垌 垒 垓 垕 垚 垛 垟 垠 垡 垢 垣 垦 垩 垫 垭 垮 垴 垵 垸 埂 埃 埈 埋 城 埏 埒 埔 埕 埗 埙 埜 埝 域 埠 埡 埤 埭 埴 埵 執 埸 培 基 埼 堀 堂 堃 堅 堆 堇 堉 堊 堑 堕 堝 堞 堡 堤 堪 堯 堰 報 場 堵 堺 塀 塁 塊 塋 塌 塑 塔 塗 塘 塙 塚 塞 塡 塢 塩 填 塬 塭 塱 塵 塹 塽 塾 墀 境 墅 墉 墊 墓 墕 増 墘 墙 墜 增 墟 墨 墩 墬 墮 墳 墺 墻 墾 壁 壅 壆 壇 壊 壌 壑 壓 壕 壘 壙 壞 壟 壠 壢 壤 壩 士 壬 壮 壯 声 壱 売 壳 壶 壷 壹 壺 壽 处 备 変 复 夏 夔 夕 外 夙 多 夜 够 夠 夢 夤 夥 大 天 太 夫 夭 央 夯 失 头 夷 夸 夹 夺 夼 夾 奂 奄 奇 奈 奉 奋 奎 奏 奐 契 奔 奕 奖 套 奘 奚 奠 奢 奥 奧 奨 奪 奬 奭 奮 女 奴 奶 奸 她 好 如 妃 妄 妆 妇 妈 妊 妍 妒 妓 妖 妙 妝 妞 妣 妤 妥 妨 妫 妬 妮 妲 妳 妹 妻 妾 姆 姉 姊 始 姍 姐 姑 姒 姓 委 姗 姚 姜 姝 姣 姥 姦 姨 姪 姫 姬 姵 姶 姻 姿 威 娃 娄 娅 娆 娇 娉 娑 娓 娘 娛 娜 娟 娠 娣 娥 娩 娯 娱 娲 娴 娶 娼 婁 婆 婉 婕 婚 婢 婦 婧 婪 婬 婭 婴 婵 婶 婷 婺 婿 媒 媚 媛 媞 媧 媯 媲 媳 媽 媾 嫁 嫂 嫄 嫉 嫌 嫔 嫖 嫗 嫚 嫡 嫣 嫦 嫩 嫻 嬅 嬈 嬉 嬋 嬌 嬖 嬗 嬛 嬢 嬤 嬪 嬬 嬰 嬲 嬴 嬷 嬸 嬿 孀 孃 子 孑 孔 孕 孖 字 存 孙 孚 孛 孜 孝 孟 孢 季 孤 学 孩 孪 孫 孰 孱 孳 孵 學 孺 孽 孿 宀 宁 它 宅 宇 守 安 宋 完 宍 宏 宓 宕 宗 官 宙 定 宛 宜 宝 实 実 宠 审 客 宣 室 宥 宦 宪 宫 宮 宰 害 宴 宵 家 宸 容 宽 宾 宿 寂 寄 寅 密 寇 富 寐 寒 寓 寔 寘 寛 寝 寞 察 寡 寢 寥 實 寧 寨 審 寫 寬 寮 寯 寰 寳 寵 寶 寸 对 寺 寻 导 対 寿 封 専 射 将 將 專 尉 尊 尋 對 導 小 少 尔 尕 尖 尘 尙 尚 尝 尤 尧 尪 尬 尭 就 尴 尷 尸 尹 尺 尻 尼 尽 尾 尿 局 屁 层 居 屆 屈 届 屋 屌 屍 屎 屏 屐 屑 屓 展 属 屠 屡 屢 層 履 屬 屯 山 屹 屿 岀 岁 岂 岌 岐 岑 岔 岖 岗 岘 岙 岚 岛 岡 岢 岩 岫 岬 岭 岱 岳 岷 岸 峁 峄 峇 峋 峒 峙 峠 峡 峤 峥 峦 峨 峩 峪 峭 峯 峰 峴 島 峻 峽 崁 崂 崆 崇 崋 崍 崎 崐 崑 崔 崖 崗 崙 崚 崛 崞 崢 崤 崧 崩 崭 崮 崱 崴 崽 嵇 嵊 嵋 嵌 嵐 嵘 嵙 嵜 嵩 嵬 嵯 嵴 嶂 嶄 嶇 嶋 嶌 嶗 嶙 嶝 嶠 嶧 嶴 嶷 嶸 嶺 嶼 嶽 巂 巅 巌 巍 巒 巔 巖 川 州 巡 巢 巣 工 左 巧 巨 巩 巫 差 己 已 巳 巴 巷 巻 巽 巾 巿 币 市 布 帅 帆 师 希 帐 帑 帕 帖 帘 帙 帚 帛 帜 帝 帥 带 帧 師 席 帮 帯 帰 帳 帶 帷 常 帼 帽 幀 幂 幄 幅 幇 幌 幔 幕 幗 幟 幡 幢 幣 幪 幫 干 平 年 并 幷 幸 幹 幺 幻 幼 幽 幾 广 庁 広 庄 庆 庇 床 序 庐 庑 库 应 底 庖 店 庙 庚 府 庞 废 庠 庥 度 座 庫 庭 庵 庶 康 庸 庹 庾 廁 廂 廃 廄 廆 廈 廉 廊 廍 廓 廖 廙 廚 廝 廞 廟 廠 廡 廢 廣 廨 廩 廪 廬 廳 延 廷 廸 建 廻 廼 廿 开 弁 异 弃 弄 弇 弈 弉 弊 弋 弌 式 弐 弑 弒 弓 弔 引 弖 弗 弘 弛 弟 张 弢 弥 弦 弧 弩 弭 弯 弱 張 強 弹 强 弼 弾 彀 彅 彈 彊 彌 彎 归 当 录 彗 彘 彙 彝 彡 形 彤 彥 彦 彧 彩 彪 彫 彬 彭 彰 影 彳 彷 役 彻 彼 彿 往 征 徂 径 待 徇 很 徊 律 後 徐 徑 徒 従 徕 得 徘 徙 從 徠 御 徨 復 循 徬 徭 微 徳 徴 徵 德 徹 徽 心 忄 必 忆 忉 忌 忍 忏 忒 志 忘 忙 応 忞 忠 忡 忤 忧 快 忱 念 忻 忽 忿 怀 态 怂 怎 怒 怕 怖 怙 怛 怜 思 怠 怡 急 怦 性 怨 怪 怯 总 怿 恂 恃 恆 恋 恍 恐 恒 恕 恙 恚 恢 恣 恤 恥 恨 恩 恪 恫 恬 恭 息 恰 恳 恵 恶 恸 恺 恼 恽 恿 悄 悅 悉 悌 悍 悔 悖 悚 悛 悝 悟 悠 患 悦 悧 您 悩 悪 悫 悬 悯 悰 悲 悳 悴 悶 悸 悼 悽 情 惇 惊 惋 惑 惕 惘 惚 惛 惜 惟 惠 惡 惣 惧 惨 惩 惫 惭 惮 惯 惰 惱 惲 想 惶 惹 惺 愁 愃 愆 愈 愉 愍 愎 意 愔 愕 愚 愛 感 愤 愧 愨 愫 愴 愷 愼 愾 愿 慄 慇 慈 態 慌 慎 慑 慕 慘 慚 慜 慟 慢 慣 慤 慧 慨 慫 慮 慰 慳 慵 慶 慷 慾 憂 憊 憍 憎 憐 憑 憔 憙 憚 憤 憧 憨 憩 憫 憬 憲 憶 憺 憾 懂 懃 懇 懈 應 懊 懋 懌 懐 懒 懦 懲 懵 懶 懷 懸 懺 懼 懾 懿 戀 戈 戊 戌 戍 戎 戏 成 我 戒 戔 戕 或 战 戚 戛 戟 戡 戢 戦 戩 截 戮 戯 戰 戱 戲 戳 戴 戶 户 戸 戻 戾 房 所 扁 扆 扇 扈 扉 手 扌 才 扎 扑 扒 打 扔 払 托 扛 扣 扦 执 扩 扫 扬 扭 扮 扯 扰 扱 扳 扶 批 扼 找 承 技 抄 抉 把 抑 抒 抓 投 抖 抗 折 抚 抛 抜 択 抟 抡 抢 护 报 抨 披 抬 抱 抵 抹 押 抽 拂 担 拆 拇 拈 拉 拋 拌 拍 拏 拐 拒 拓 拔 拖 拗 拘 拙 拚 招 拜 拝 拟 拠 拡 拢 拣 拥 拦 拨 择 括 拭 拮 拯 拱 拳 拴 拵 拶 拷 拼 拽 拾 拿 持 挂 指 挈 按 挑 挖 挙 挚 挛 挝 挞 挟 挠 挡 挣 挤 挥 挨 挪 挫 振 挹 挺 挽 挾 挿 捅 捆 捉 捌 捍 捎 捏 捐 捒 捕 捗 捜 捞 损 捡 换 捣 捧 捨 捩 据 捱 捲 捶 捷 捺 捻 掀 掃 授 掉 掌 掏 掐 排 掖 掘 掙 掛 掟 掠 採 探 掣 接 控 推 掩 措 掬 掰 掲 掳 掴 掷 掸 掺 掻 掾 揀 揃 揄 揆 揉 揍 描 提 插 揖 揚 換 握 揣 揪 揭 揮 援 揶 揺 揽 揾 搁 搅 損 搏 搐 搓 搔 搖 搗 搜 搞 搦 搪 搬 搭 搵 搶 携 搾 摂 摄 摆 摇 摊 摑 摒 摔 摘 摠 摧 摩 摭 摯 摶 摸 摹 摺 摻 撃 撇 撈 撐 撑 撒 撓 撕 撚 撞 撣 撤 撥 撩 撫 撬 播 撮 撰 撲 撷 撹 撻 撼 撿 擁 擂 擄 擅 擇 擊 擋 操 擎 擒 擔 擘 據 擠 擢 擦 擧 擬 擱 擲 擴 擷 擺 擾 攀 攏 攒 攔 攘 攜 攝 攢 攣 攤 攪 攫 攬 支 收 攷 攸 改 攻 放 政 故 效 敌 敍 敎 敏 救 敕 敖 敗 敘 教 敛 敝 敞 敢 散 敦 敬 数 敲 整 敵 敷 數 斂 斃 文 斉 斋 斌 斎 斐 斑 斗 料 斛 斜 斟 斡 斤 斥 斧 斩 斫 斬 断 斯 新 斷 方 於 施 旁 旃 旅 旋 旌 族 旒 旗 旛 无 既 旣 日 旦 旧 旨 早 旬 旭 旱 时 旷 旸 旺 旻 旼 昀 昂 昆 昇 昉 昊 昌 明 昏 昐 易 昔 昕 昙 昝 昞 星 映 春 昧 昨 昪 昭 是 昰 昱 昴 昵 昶 昺 昼 显 晁 時 晃 晄 晉 晋 晏 晒 晓 晔 晕 晖 晗 晙 晚 晝 晞 晟 晤 晦 晧 晨 晩 普 景 晰 晳 晴 晶 晷 晸 智 晾 暁 暂 暄 暇 暈 暉 暌 暎 暐 暑 暖 暗 暘 暝 暠 暢 暦 暧 暨 暫 暮 暱 暲 暴 暹 暻 暾 曄 曆 曇 曉 曖 曙 曜 曝 曠 曦 曩 曬 曰 曲 曳 更 曷 書 曹 曺 曼 曽 曾 替 最 會 月 有 朋 服 朐 朓 朔 朕 朗 望 朝 期 朦 朧 木 未 末 本 札 朮 术 朱 朴 朵 朶 机 朽 杀 杂 权 杆 杉 杌 李 杏 材 村 杓 杖 杙 杜 杞 束 杠 条 杢 杣 来 杨 杭 杮 杯 杰 東 杲 杳 杵 杷 杼 松 板 极 构 枇 枉 枋 析 枕 林 枚 果 枝 枞 枠 枡 枢 枣 枪 枫 枭 枯 枳 架 枷 枸 枹 柁 柃 柄 柊 柏 某 柑 柒 染 柔 柘 柚 柜 柝 柞 柠 柢 查 柩 柬 柯 柱 柳 柴 柵 査 柽 柾 柿 栂 栃 栄 栅 标 栈 栉 栋 栎 栏 树 栒 栓 栖 栗 栞 校 栢 栩 株 栱 栲 栴 样 核 根 栻 格 栽 栾 桀 桁 桂 桃 桅 框 案 桉 桌 桎 桐 桑 桓 桔 桜 桝 桟 桡 桢 档 桥 桦 桧 桨 桩 桫 桶 桷 桿 梁 梃 梅 梆 梓 梔 梗 條 梟 梠 梢 梦 梧 梨 梭 梯 械 梱 梳 梵 梶 检 棂 棄 棉 棋 棍 棒 棕 棗 棘 棚 棟 棠 棣 棧 棨 棫 森 棱 棲 棵 棹 棺 棻 椀 椁 椅 椋 植 椎 椏 椒 椙 椚 椛 検 椭 椰 椴 椹 椽 椿 楂 楊 楓 楔 楕 楙 楚 楝 楞 楠 楡 楢 楣 楨 楫 業 楮 楯 楳 極 楷 楸 楹 楼 楽 概 榄 榆 榈 榉 榊 榎 榑 榔 榕 榖 榘 榛 榜 榧 榨 榫 榭 榮 榴 榷 榻 槁 槃 槇 槊 構 槌 槍 槎 槐 槓 様 槙 槛 槟 槤 槨 槪 槭 槲 槳 槺 槻 槽 槿 樁 樂 樅 樊 樋 樑 樓 樗 標 樞 樟 模 樣 樨 権 横 樫 樱 樵 樸 樹 樺 樽 樾 橄 橇 橈 橋 橐 橘 橙 機 橡 橢 橫 橱 橹 橿 檀 檄 檉 檎 檐 檔 檗 檜 檢 檣 檬 檯 檳 檸 檻 櫂 櫃 櫓 櫚 櫛 櫟 櫥 櫨 櫸 櫻 欄 欅 欉 權 欒 欖 欞 欠 次 欢 欣 欤 欧 欲 欺 欽 款 歆 歇 歉 歌 歎 歐 歓 歙 歟 歡 止 正 此 步 武 歧 歩 歪 歯 歲 歳 歴 歷 歸 歹 死 歼 歿 殁 殃 殆 殇 殉 殊 残 殒 殓 殖 殘 殞 殡 殤 殭 殮 殯 殲 殴 段 殷 殺 殻 殼 殿 毀 毁 毂 毅 毆 毋 毌 母 毎 每 毒 毓 比 毕 毖 毗 毘 毙 毛 毡 毫 毬 毯 毽 氂 氈 氏 氐 民 氓 气 氖 気 氘 氙 氚 氛 氟 氡 氢 氣 氦 氧 氨 氩 氪 氫 氬 氮 氯 氰 水 氵 氷 永 氹 氾 汀 汁 求 汇 汉 汊 汎 汐 汕 汗 汙 汚 汛 汜 汝 汞 江 池 污 汤 汧 汨 汪 汭 汰 汲 汴 汶 汹 決 汽 汾 沁 沂 沃 沄 沅 沆 沈 沉 沌 沐 沒 沓 沔 沖 沙 沛 沟 没 沢 沣 沥 沦 沧 沪 沫 沭 沮 沱 河 沸 油 治 沼 沽 沾 沿 況 泄 泉 泊 泌 泓 法 泗 泚 泛 泞 泠 泡 波 泣 泥 注 泪 泫 泮 泯 泰 泱 泳 泵 泷 泸 泻 泼 泽 泾 洁 洄 洋 洎 洒 洗 洙 洛 洞 津 洧 洩 洪 洮 洱 洲 洵 洶 洸 洹 洺 活 洼 洽 派 流 浄 浅 浆 浇 浊 测 济 浏 浑 浒 浓 浔 浙 浚 浜 浞 浠 浣 浤 浦 浩 浪 浬 浮 浯 浴 海 浸 涂 涅 涇 消 涉 涌 涎 涓 涔 涕 涙 涛 涜 涝 涞 涟 涡 涣 涤 润 涧 涨 涩 涪 涮 涯 液 涵 涸 涼 涿 淀 淄 淅 淆 淇 淋 淌 淑 淒 淖 淘 淙 淚 淝 淞 淡 淤 淦 淨 淩 淪 淫 淬 淮 淯 深 淳 淵 淶 混 淸 淹 淺 添 淼 清 渇 済 渉 渊 渋 渍 渎 渐 渓 渔 渕 渗 渙 渚 減 渝 渟 渠 渡 渣 渤 渥 渦 温 渫 測 渭 港 渲 渴 游 渺 渾 湃 湄 湊 湍 湖 湘 湛 湜 湟 湣 湧 湫 湮 湯 湳 湾 湿 満 溃 溅 溆 溉 溏 源 準 溜 溝 溟 溢 溥 溧 溪 溫 溯 溱 溲 溴 溶 溺 溼 滁 滂 滄 滅 滇 滉 滋 滌 滎 滏 滑 滓 滔 滕 滘 滙 滚 滝 滞 满 滢 滤 滥 滦 滨 滩 滬 滯 滲 滴 滷 滸 滹 滾 滿 漁 漂 漆 漉 漏 漑 漓 演 漕 漠 漢 漣 漩 漪 漫 漬 漯 漱 漲 漳 漵 漸 漾 漿 潁 潇 潍 潑 潔 潘 潛 潜 潞 潟 潢 潤 潦 潭 潮 潯 潰 潴 潺 潼 潾 澀 澁 澂 澄 澆 澇 澈 澍 澎 澗 澜 澠 澡 澤 澧 澪 澮 澱 澳 澶 澹 激 濁 濂 濃 濉 濊 濑 濒 濕 濘 濛 濞 濟 濠 濡 濤 濫 濬 濮 濯 濰 濱 濵 濺 濾 瀅 瀆 瀉 瀋 瀏 瀑 瀕 瀘 瀚 瀛 瀝 瀞 瀟 瀧 瀨 瀬 瀰 瀾 灃 灌 灏 灑 灕 灘 灝 灞 灣 灤 火 灭 灯 灰 灵 灶 灸 灼 災 灾 灿 炀 炁 炅 炆 炉 炊 炎 炒 炔 炕 炖 炘 炙 炜 炤 炫 炬 炭 炮 炯 炱 炳 炸 点 為 炼 炽 烁 烂 烃 烈 烏 烘 烙 烛 烜 烝 烟 烤 烦 烧 烨 烩 烫 烬 热 烯 烴 烷 烹 烺 烽 焉 焊 焓 焔 焕 焗 焘 焙 焚 焜 無 焦 焯 焰 焱 然 焼 煇 煉 煊 煌 煎 煒 煕 煖 煙 煚 煜 煞 煤 煥 煦 照 煨 煩 煬 煮 煲 煽 熄 熈 熊 熏 熒 熔 熙 熟 熠 熨 熬 熱 熲 熵 熹 熾 燁 燃 燄 燈 燉 燊 燎 燐 燒 燔 燕 燗 燙 營 燥 燦 燧 燬 燭 燮 燴 燹 燻 燼 燾 燿 爀 爆 爍 爐 爛 爨 爪 爬 爭 爰 爱 爲 爵 父 爷 爸 爹 爺 爻 爽 爾 牂 牆 片 版 牌 牍 牒 牘 牙 牛 牝 牟 牠 牡 牢 牧 物 牯 牲 牴 牵 特 牺 牻 牽 犀 犁 犂 犄 犊 犍 犒 犛 犠 犢 犧 犬 犯 犰 状 犷 犸 犹 犽 狀 狂 狄 狈 狍 狎 狐 狒 狗 狙 狛 狠 狡 狨 狩 独 狭 狮 狱 狳 狷 狸 狹 狼 狽 猁 猊 猎 猕 猖 猗 猛 猜 猝 猞 猟 猥 猩 猪 猫 猬 献 猴 猶 猷 猾 猿 獁 獄 獅 獎 獏 獐 獒 獗 獠 獣 獨 獬 獭 獰 獲 獴 獵 獷 獸 獺 獻 獼 獾 玄 玆 率 玉 王 玑 玕 玖 玘 玛 玟 玠 玢 玥 玩 玫 玮 环 现 玲 玳 玷 玹 玺 玻 珀 珂 珅 珈 珉 珊 珍 珏 珐 珑 珙 珝 珞 珠 珣 珥 珦 珩 珪 班 珮 珲 珺 珽 現 球 琅 理 琇 琉 琊 琍 琏 琐 琚 琛 琢 琤 琥 琦 琨 琪 琬 琮 琯 琰 琲 琳 琴 琵 琶 琺 琼 琿 瑀 瑁 瑄 瑈 瑊 瑋 瑕 瑗 瑙 瑚 瑛 瑜 瑞 瑟 瑠 瑢 瑣 瑤 瑩 瑪 瑭 瑯 瑰 瑱 瑳 瑶 瑷 瑾 璀 璁 璃 璆 璇 璈 璉 璋 璐 璘 璜 璞 璟 璠 璣 璥 璦 璧 璨 璩 璪 環 璲 璹 璽 璿 瓊 瓌 瓏 瓒 瓔 瓘 瓚 瓛 瓜 瓠 瓢 瓣 瓦 瓩 瓮 瓯 瓶 瓷 甄 甌 甑 甕 甘 甚 甜 生 甡 產 産 甥 甦 用 甩 甫 甬 甯 田 由 甲 申 电 男 甸 町 画 甾 畀 畅 畈 畋 界 畏 畑 畔 留 畜 畝 畠 畢 畤 略 畦 番 畫 畬 畯 異 畲 畳 畴 畵 當 畷 畸 畹 畿 疃 疆 疇 疊 疋 疍 疎 疏 疑 疒 疗 疙 疚 疝 疟 疡 疣 疤 疥 疫 疮 疯 疱 疲 疵 疸 疹 疼 疽 疾 痂 病 症 痉 痊 痍 痒 痔 痕 痘 痙 痛 痞 痢 痣 痤 痩 痪 痫 痰 痲 痴 痹 痺 瘀 瘁 瘋 瘍 瘓 瘙 瘟 瘠 瘡 瘢 瘤 瘦 瘧 瘩 瘫 瘴 瘾 療 癇 癌 癒 癖 癡 癢 癣 癥 癩 癪 癫 癬 癮 癱 癲 癸 発 登 發 白 百 皂 的 皆 皇 皈 皋 皎 皐 皓 皖 皙 皝 皮 皰 皱 皺 皿 盂 盃 盅 盆 盈 益 盎 盏 盐 监 盒 盔 盖 盗 盘 盛 盜 盞 盟 盡 監 盤 盥 盧 盩 盪 目 盯 盱 盲 直 相 盼 盾 省 眈 眉 看 県 眙 眞 真 眠 眨 眩 眭 眶 眷 眸 眺 眼 眾 着 睁 睇 睐 睑 睛 睜 睞 睡 睢 督 睦 睨 睪 睫 睬 睹 睺 睽 睾 睿 瞄 瞋 瞌 瞎 瞑 瞒 瞞 瞥 瞧 瞩 瞪 瞬 瞭 瞰 瞳 瞻 瞼 瞽 瞿 矇 矍 矗 矚 矛 矜 矢 矣 知 矧 矩 矫 短 矮 矯 石 矶 矽 矾 矿 砀 码 砂 砌 砍 砒 研 砕 砖 砚 砜 砥 砦 砧 砬 砭 砰 砲 破 砵 砷 砸 砺 砾 砿 础 硃 硅 硏 硐 硒 硕 硖 硝 硤 硫 硬 确 硯 硼 碁 碇 碉 碌 碍 碎 碑 碓 碕 碗 碘 碚 碛 碟 碣 碧 碩 碭 碰 碱 碲 碳 碴 碶 碸 確 碼 碾 磁 磅 磊 磋 磐 磔 磕 磘 磚 磡 磧 磨 磬 磯 磴 磷 磺 磻 磾 礁 礎 礒 礙 礦 礪 礫 礬 礴 示 礼 礽 社 祀 祁 祂 祆 祇 祈 祉 祎 祏 祐 祓 祔 祕 祖 祗 祚 祛 祜 祝 神 祟 祠 祢 祥 票 祭 祯 祷 祸 祺 祿 禀 禁 禄 禅 禊 禍 禎 福 禑 禔 禕 禛 禦 禧 禩 禪 禮 禰 禱 禹 禺 离 禽 禾 禿 秀 私 秃 秆 秉 秋 种 科 秒 秘 租 秣 秤 秦 秧 秩 秭 积 称 秸 移 秽 稀 稃 稅 稈 程 稍 税 稔 稗 稙 稚 稜 稟 稠 稣 種 稱 稲 稳 稷 稹 稻 稼 稽 稿 穀 穂 穆 穌 積 穎 穏 穐 穗 穢 穣 穩 穫 穰 穴 究 穷 穹 空 穿 突 窃 窄 窈 窍 窑 窒 窓 窕 窖 窗 窘 窜 窝 窟 窠 窣 窥 窦 窩 窪 窮 窯 窺 窿 竄 竅 竇 竈 竊 立 竑 竖 站 竜 竝 竞 竟 章 竣 童 竦 竪 竭 端 競 竹 竺 竿 笃 笄 笆 笈 笋 笏 笑 笔 笕 笙 笛 笞 笠 笥 符 笨 笪 第 笮 笳 笹 笺 笼 筅 筆 筈 等 筊 筋 筌 筍 筏 筐 筑 筒 答 策 筛 筝 筠 筥 筧 筮 筰 筱 筲 筵 筷 筹 筺 签 简 箇 箋 箍 箏 箐 箒 箓 箔 箕 算 箚 箝 管 箪 箫 箬 箭 箱 箴 箸 節 篁 範 篆 篇 築 篋 篙 篝 篠 篡 篤 篦 篩 篪 篭 篮 篱 篷 篾 簀 簃 簇 簋 簑 簒 簕 簗 簡 簧 簪 簫 簷 簸 簽 簾 簿 籀 籁 籃 籌 籍 籐 籓 籔 籙 籟 籠 籤 籬 籲 米 类 籽 籾 粁 粂 粄 粉 粋 粍 粑 粒 粕 粗 粘 粛 粟 粤 粥 粧 粪 粮 粱 粲 粳 粵 粹 粽 精 粿 糀 糅 糊 糍 糎 糕 糖 糗 糙 糜 糞 糟 糠 糧 糬 糯 糰 糸 糹 糺 系 糾 紀 紂 約 紅 紆 紇 紉 紊 紋 納 紐 紓 純 紗 紘 紙 級 紛 紜 素 紡 索 紧 紫 紬 紮 累 細 紳 紹 紺 終 絃 組 絅 絆 経 結 絕 絜 絞 絡 絢 絣 給 絨 絮 統 絲 絳 絵 絶 絹 綁 綃 綉 綏 綑 經 継 続 綜 綝 綠 綢 綦 綫 綬 維 綰 綱 網 綴 綵 綸 綺 綻 綽 綾 綿 緊 緋 総 緑 緒 緖 緘 線 緝 緞 締 緡 緣 編 緩 緬 緯 緲 練 緹 緻 縁 縄 縈 縉 縊 縛 縝 縞 縣 縦 縫 縮 縯 縱 縷 縹 縻 總 績 繁 繃 繆 繇 繊 繋 繍 繒 織 繕 繙 繚 繞 繡 繩 繪 繫 繭 繰 繳 繹 繼 繽 纂 纈 續 纍 纏 纓 纖 纘 纛 纜 纠 红 纣 纤 纥 约 级 纪 纫 纬 纭 纮 纯 纱 纲 纳 纵 纶 纷 纸 纹 纺 纽 纾 线 绀 绂 练 组 绅 细 织 终 绊 绍 绎 经 绑 绒 结 绕 绘 给 绚 绛 络 绝 绞 统 绡 绢 绣 绥 继 绩 绪 绫 续 绮 绯 绰 绳 维 绵 绶 绷 绸 综 绽 绾 绿 缀 缄 缅 缆 缇 缉 缎 缓 缔 缕 编 缘 缙 缚 缜 缝 缟 缠 缢 缤 缨 缩 缪 缬 缭 缮 缯 缴 缵 缶 缸 缺 缽 罂 罄 罅 罈 罌 罐 网 罔 罕 罗 罘 罚 罟 罠 罡 罢 罩 罪 置 罰 署 罵 罷 罹 罽 羁 羅 羆 羈 羊 羋 羌 美 羔 羚 羞 羟 羡 羣 群 羥 羧 羨 義 羯 羰 羲 羸 羹 羽 羿 翀 翁 翅 翊 翌 翎 習 翔 翕 翘 翟 翠 翡 翥 翦 翩 翫 翮 翰 翱 翳 翹 翻 翼 耀 老 考 者 耆 而 耍 耐 耒 耕 耗 耘 耙 耜 耦 耨 耳 耶 耸 耻 耽 耿 聂 聃 聆 聊 聋 职 联 聖 聘 聚 聞 聡 聪 聯 聰 聲 聳 聴 聶 職 聽 聾 聿 肃 肄 肅 肆 肇 肉 肋 肌 肓 肖 肘 肚 肛 肜 肝 肟 肠 股 肢 肤 肥 肩 肪 肮 肯 肱 育 肴 肺 肼 肽 肾 肿 胀 胁 胃 胄 胆 背 胍 胎 胖 胚 胛 胜 胝 胞 胡 胤 胥 胧 胪 胫 胭 胯 胰 胱 胳 胴 胶 胸 胺 胼 能 脂 脅 脆 脇 脈 脉 脊 脍 脏 脐 脑 脓 脖 脚 脛 脣 脩 脫 脯 脱 脲 脳 脷 脸 脹 脾 腆 腈 腊 腋 腌 腎 腐 腑 腓 腔 腕 腥 腦 腩 腫 腭 腮 腰 腱 腳 腴 腸 腹 腺 腻 腾 腿 膀 膂 膈 膊 膏 膑 膚 膛 膜 膝 膠 膣 膦 膨 膩 膳 膵 膺 膽 膾 膿 臀 臂 臆 臈 臉 臍 臏 臓 臘 臚 臟 臣 臥 臧 臨 自 臬 臭 至 致 臺 臻 臼 臾 舁 舂 舅 舆 與 興 舉 舊 舌 舍 舎 舐 舒 舔 舖 舗 舘 舛 舜 舞 舟 舢 舩 航 舫 般 舯 舰 舱 舳 舵 舶 舷 舸 船 舺 舾 艀 艇 艉 艋 艏 艘 艙 艛 艤 艦 艮 良 艰 艱 色 艳 艶 艷 艸 艹 艺 艾 节 芃 芈 芊 芋 芍 芎 芒 芗 芙 芜 芝 芡 芥 芦 芨 芩 芪 芫 芬 芭 芮 芯 花 芳 芷 芸 芹 芻 芽 芾 苄 苅 苇 苋 苌 苍 苎 苏 苑 苓 苔 苕 苗 苛 苜 苞 苟 苡 苣 若 苦 苧 苫 苯 英 苳 苴 苷 苹 苺 苻 苾 茁 茂 范 茄 茅 茉 茌 茎 茔 茗 茛 茜 茧 茨 茫 茯 茱 茲 茴 茵 茶 茸 茹 荀 荃 荆 草 荊 荏 荐 荒 荔 荖 荘 荚 荞 荟 荠 荡 荣 荤 荥 荧 荨 荩 荪 荫 药 荳 荷 荸 荻 荼 荽 莅 莆 莉 莊 莎 莒 莓 莖 莘 莞 莠 莢 莧 莨 莪 莫 莱 莲 莳 莴 获 莹 莺 莽 莿 菀 菁 菅 菇 菈 菊 菌 菏 菓 菖 菘 菜 菝 菟 菠 菡 菩 菫 華 菰 菱 菲 菴 菸 菽 萁 萃 萄 萇 萊 萌 萍 萎 萘 萜 萝 萠 萣 萤 营 萦 萧 萨 萩 萬 萱 萵 萸 萼 落 葆 葉 葎 著 葛 葜 葡 董 葦 葩 葫 葬 葭 葯 葱 葳 葵 葶 葷 葺 蒂 蒋 蒐 蒔 蒙 蒜 蒞 蒟 蒡 蒨 蒯 蒲 蒴 蒸 蒺 蒻 蒼 蒽 蒾 蒿 蓀 蓁 蓄 蓆 蓉 蓋 蓍 蓑 蓓 蓖 蓝 蓟 蓣 蓬 蓮 蓼 蓿 蔀 蔑 蔓 蔔 蔗 蔘 蔚 蔡 蔣 蔥 蔦 蔬 蔭 蔴 蔵 蔷 蔺 蔻 蔼 蔽 蕁 蕃 蕈 蕉 蕊 蕎 蕗 蕙 蕤 蕨 蕩 蕪 蕭 蕲 蕴 蕷 蕾 薀 薄 薇 薈 薊 薌 薏 薑 薔 薗 薙 薛 薜 薦 薨 薩 薪 薫 薬 薮 薯 薰 薷 薹 薺 藁 藉 藍 藎 藏 藐 藓 藔 藕 藜 藝 藤 藥 藨 藩 藪 藷 藹 藺 藻 藿 蘂 蘄 蘅 蘆 蘇 蘊 蘋 蘑 蘚 蘭 蘸 蘿 虎 虏 虐 虑 虓 虔 處 虚 虛 虜 虞 號 虢 虧 虫 虬 虯 虱 虹 虻 虽 虾 蚀 蚁 蚂 蚊 蚋 蚌 蚓 蚕 蚜 蚝 蚣 蚤 蚩 蚪 蚬 蚯 蚱 蚵 蚶 蚺 蛀 蛄 蛆 蛇 蛉 蛊 蛋 蛍 蛎 蛏 蛔 蛙 蛛 蛞 蛟 蛤 蛭 蛮 蛯 蛰 蛱 蛳 蛸 蛹 蛺 蛻 蛾 蜀 蜂 蜃 蜆 蜈 蜉 蜊 蜍 蜑 蜒 蜓 蜕 蜗 蜘 蜚 蜜 蜡 蜢 蜥 蜱 蜴 蜷 蜻 蜿 蝇 蝉 蝋 蝌 蝎 蝓 蝕 蝗 蝙 蝟 蝠 蝣 蝦 蝨 蝮 蝰 蝴 蝶 蝸 蝽 蝾 蝿 螂 螃 螄 螈 融 螞 螟 螢 螨 螫 螭 螯 螳 螺 螽 蟀 蟄 蟆 蟇 蟋 蟎 蟑 蟒 蟜 蟠 蟬 蟲 蟷 蟹 蟻 蟾 蠅 蠊 蠍 蠑 蠔 蠕 蠟 蠡 蠢 蠣 蠱 蠲 蠵 蠶 蠹 蠻 血 衅 衆 衊 行 衍 衒 術 衔 衕 街 衙 衚 衛 衝 衞 衡 衢 衣 补 表 衫 衬 衮 衰 衷 衹 衽 衾 衿 袁 袂 袄 袈 袋 袍 袒 袖 袛 袜 袞 袢 袤 被 袭 袱 袴 裁 裂 装 裏 裒 裔 裕 裘 裙 補 裝 裟 裡 裤 裨 裱 裳 裴 裵 裸 裹 製 裾 褂 複 褌 褐 褒 褓 褔 褚 褥 褪 褫 褲 褶 褻 襁 襄 襖 襞 襟 襦 襪 襯 襲 襷 西 要 覃 覆 覇 見 規 覓 視 覗 覚 覦 覧 親 覬 覲 観 覺 覽 觀 见 观 规 觅 视 览 觉 觊 觎 觐 角 觚 觞 解 触 觴 觸 言 訁 訂 訃 訇 計 訊 訌 討 訓 訕 訖 託 記 訛 訝 訟 訢 訣 訥 訪 設 許 訳 訴 訶 診 註 証 訾 詁 詆 詈 詐 詒 詔 評 詛 詞 詠 詡 詢 詣 試 詧 詩 詫 詬 詭 詮 詰 話 該 詳 詵 詹 詼 誅 誇 誉 誊 誌 認 誑 誓 誕 誘 語 誠 誡 誣 誤 誥 誦 誨 說 説 読 誰 課 誹 誼 誾 調 諂 諄 談 請 諌 諍 諏 諒 論 諛 諜 諡 諤 諦 諧 諫 諭 諮 諱 諲 諳 諴 諶 諷 諸 諺 諾 謀 謁 謂 謄 謇 謊 謎 謐 謔 謖 謗 謙 謚 講 謝 謠 謡 謨 謫 謬 謳 謹 謾 譁 證 譏 識 譙 譚 譜 警 譬 譯 議 譲 譴 護 譽 讀 讃 變 讎 讐 讒 讓 讖 讚 讞 计 订 讣 认 讥 讦 讧 讨 让 讪 讫 训 议 讯 记 讲 讳 讴 讶 讷 许 讹 论 讼 讽 设 访 诀 证 诂 诃 评 诅 识 诈 诉 诊 诋 词 诏 译 诒 试 诗 诘 诙 诚 诛 诜 话 诞 诟 诠 诡 询 诣 诤 该 详 诧 诩 诫 诬 语 误 诰 诱 诲 说 诵 请 诸 诹 诺 读 诽 课 谀 谁 调 谄 谅 谈 谊 谋 谌 谍 谎 谏 谐 谑 谒 谓 谔 谕 谗 谘 谙 谚 谛 谜 谟 谢 谣 谤 谥 谦 谧 谨 谩 谪 谬 谭 谯 谱 谳 谴 谶 谷 谿 豁 豆 豈 豉 豊 豌 豎 豐 豔 豕 豚 象 豢 豨 豪 豫 豬 豳 豸 豹 豺 貂 貉 貊 貌 貓 貘 貝 貞 負 財 貢 貧 貨 販 貪 貫 責 貯 貰 貳 貴 貶 買 貸 費 貼 貽 貿 賀 賁 賂 賃 賄 資 賈 賊 賎 賑 賓 賚 賛 賜 賞 賠 賡 賢 賣 賤 賦 質 賬 賭 賴 賺 購 賽 賾 贄 贅 贇 贈 贊 贋 贍 贏 贓 贔 贖 贛 贝 贞 负 贡 财 责 贤 败 账 货 质 贩 贪 贫 贬 购 贮 贯 贰 贱 贲 贴 贵 贷 贸 费 贺 贻 贼 贽 贾 贿 赀 赁 赂 赃 资 赈 赉 赋 赌 赍 赎 赏 赐 赓 赔 赖 赘 赚 赛 赝 赞 赟 赠 赡 赢 赣 赤 赦 赧 赫 赭 走 赳 赴 赵 赶 起 趁 超 越 趋 趕 趙 趟 趣 趨 足 趴 趸 趺 趾 跃 跆 跋 跌 跏 跑 跖 跗 跛 距 跟 跡 跤 跨 跪 路 跳 践 跷 跸 跻 踊 踏 踐 踝 踞 踢 踩 踪 踰 踴 踵 踹 蹂 蹄 蹇 蹈 蹉 蹊 蹋 蹕 蹙 蹟 蹠 蹤 蹦 蹬 蹲 蹴 蹶 蹺 蹼 躁 躄 躅 躇 躉 躊 躋 躍 躏 躑 躓 躙 躪 身 躬 躯 躰 躲 躺 躾 軀 車 軋 軌 軍 軒 軔 軛 軟 転 軫 軸 軻 軼 軽 軾 較 輅 載 輋 輌 輒 輓 輔 輕 輛 輜 輝 輟 輥 輦 輩 輪 輯 輳 輸 輻 輾 輿 轂 轄 轅 轆 轉 轍 轎 轟 轡 轢 轤 车 轧 轨 轩 轫 转 轭 轮 软 轰 轲 轴 轶 轸 轻 轼 载 轿 辂 较 辄 辅 辆 辇 辈 辉 辊 辍 辎 辐 辑 输 辕 辖 辗 辘 辙 辛 辜 辞 辟 辣 辦 辨 辩 辫 辭 辮 辯 辰 辱 農 辶 边 辺 辻 込 辽 达 辿 迁 迂 迄 迅 过 迈 迎 运 近 返 还 这 进 远 违 连 迟 迢 迤 迥 迦 迨 迩 迪 迫 迭 述 迳 迴 迷 迸 迹 迺 追 退 送 适 逃 逄 逅 逆 选 逊 逋 逍 透 逐 逑 递 逓 途 逕 逖 逗 這 通 逛 逝 逞 速 造 逡 逢 連 逮 逯 週 進 逵 逸 逹 逻 逼 逾 遁 遂 遅 遇 遊 運 遍 過 遏 遐 遑 遒 道 達 違 遗 遘 遙 遜 遞 遠 遡 遣 遥 遨 適 遭 遮 遲 遴 遵 遶 遷 選 遹 遺 遼 遽 避 邀 邁 邂 邃 還 邇 邈 邉 邊 邏 邑 邓 邕 邗 邙 邛 邝 邠 邡 邢 那 邦 邨 邪 邬 邮 邯 邰 邱 邳 邴 邵 邸 邹 邺 邻 邽 邾 郁 郃 郅 郇 郊 郎 郑 郓 郕 郗 郛 郜 郝 郞 郡 郢 郤 郦 郧 部 郫 郭 郯 郴 郵 郷 郸 都 郾 郿 鄂 鄄 鄆 鄉 鄒 鄔 鄕 鄖 鄙 鄚 鄜 鄞 鄠 鄢 鄣 鄧 鄭 鄯 鄰 鄱 鄲 鄴 鄺 酆 酈 酉 酊 酋 酌 配 酎 酐 酒 酔 酗 酚 酝 酞 酢 酣 酥 酩 酪 酬 酮 酯 酰 酱 酵 酶 酷 酸 酿 醃 醇 醉 醋 醌 醍 醐 醒 醗 醚 醛 醜 醞 醣 醤 醪 醫 醬 醮 醯 醴 醸 醺 釀 釁 采 釈 釉 释 釋 里 重 野 量 釐 金 釔 釗 釘 釜 針 釣 釤 釧 釩 釵 釷 鈀 鈉 鈍 鈎 鈐 鈑 鈔 鈕 鈞 鈣 鈦 鈮 鈰 鈴 鈷 鈸 鈹 鈺 鈽 鈾 鈿 鉀 鉄 鉅 鉈 鉉 鉋 鉍 鉏 鉑 鉗 鉚 鉛 鉞 鉢 鉤 鉦 鉬 鉭 鉱 鉴 鉸 鉻 鉾 銀 銃 銅 銑 銓 銕 銖 銘 銚 銛 銜 銣 銥 銦 銨 銫 銬 銭 銮 銳 銷 銹 銻 銼 鋁 鋅 鋆 鋇 鋌 鋏 鋐 鋒 鋤 鋪 鋭 鋯 鋰 鋲 鋳 鋸 鋹 鋼 錄 錆 錐 錒 錕 錘 錚 錠 錡 錢 錦 錨 錫 錬 錮 錯 録 錳 錶 錸 鍇 鍊 鍋 鍍 鍔 鍛 鍬 鍮 鍰 鍱 鍵 鍶 鍺 鍼 鍾 鎂 鎊 鎌 鎏 鎔 鎖 鎗 鎘 鎚 鎝 鎢 鎧 鎬 鎭 鎮 鎰 鎳 鎵 鏃 鏈 鏊 鏐 鏑 鏖 鏗 鏘 鏜 鏞 鏟 鏡 鏢 鏤 鏵 鏽 鐐 鐘 鐙 鐡 鐫 鐮 鐲 鐳 鐵 鐸 鐺 鑄 鑅 鑑 鑒 鑓 鑛 鑠 鑣 鑫 鑭 鑰 鑲 鑼 鑽 鑾 鑿 钇 针 钉 钊 钍 钏 钒 钓 钗 钙 钚 钛 钜 钝 钞 钟 钠 钡 钢 钤 钥 钦 钧 钨 钩 钫 钮 钯 钰 钱 钳 钴 钵 钹 钺 钻 钼 钽 钾 钿 铀 铁 铂 铃 铅 铆 铈 铉 铊 铋 铌 铍 铎 铐 铛 铜 铝 铟 铠 铢 铣 铤 铧 铨 铫 铬 铭 铮 铯 铰 铱 铲 铳 铵 银 铷 铸 铺 铼 链 铿 销 锁 锂 锄 锅 锆 锈 锉 锋 锌 锐 锑 锕 锗 错 锚 锜 锝 锟 锡 锢 锣 锤 锥 锦 锭 键 锯 锰 锴 锵 锶 锷 锹 锺 锻 锽 锾 镀 镁 镂 镇 镉 镌 镍 镐 镑 镒 镓 镕 镖 镗 镛 镜 镝 镠 镧 镫 镭 镯 镰 镳 镶 長 长 門 閂 閃 閆 閉 開 閎 閏 閑 閒 間 閔 閘 閡 関 閣 閤 閥 閨 閩 閬 閭 閱 閲 閹 閻 閼 閾 闆 闇 闈 闊 闋 闌 闍 闐 闓 闔 闕 闖 闘 關 闞 闡 闢 闥 门 闩 闪 闫 闭 问 闯 闰 闱 闲 闳 间 闵 闷 闸 闹 闺 闻 闼 闽 闾 闿 阀 阁 阂 阅 阆 阇 阈 阉 阎 阏 阐 阑 阔 阕 阖 阗 阙 阚 阜 阝 队 阡 阪 阮 阯 阱 防 阳 阴 阵 阶 阻 阿 陀 陂 附 际 陆 陇 陈 陉 陋 陌 降 限 陔 陕 陘 陛 陜 陝 陞 陟 陡 院 陣 除 陥 陨 险 陪 陰 陲 陳 陵 陶 陷 陸 険 陽 隅 隆 隈 隊 隋 隍 階 随 隐 隔 隕 隗 隘 隙 際 障 隠 隣 隧 隨 險 隰 隱 隴 隶 隷 隸 隹 隻 隼 隽 难 雀 雁 雄 雅 集 雇 雉 雋 雌 雍 雎 雏 雑 雒 雕 雖 雙 雛 雜 雞 雠 離 難 雨 雩 雪 雫 雯 雰 雲 雳 零 雷 雹 電 雾 需 霁 霄 霆 震 霈 霉 霊 霍 霎 霏 霑 霓 霖 霜 霞 霧 霭 霰 露 霸 霹 霽 霾 靂 靄 靈 靑 青 靓 靖 静 靚 靛 靜 非 靠 靡 面 革 靭 靱 靳 靴 靶 靺 靼 鞄 鞅 鞆 鞋 鞍 鞏 鞑 鞘 鞞 鞠 鞣 鞦 鞨 鞫 鞬 鞭 鞮 鞴 韃 韆 韋 韌 韓 韜 韞 韦 韧 韩 韫 韬 韭 韮 音 韵 韶 韻 響 頁 頂 頃 項 順 須 頊 頌 預 頑 頒 頓 頗 領 頚 頜 頠 頡 頤 頦 頫 頬 頭 頰 頴 頵 頷 頸 頹 頻 頼 頽 顆 題 額 顎 顏 顒 顓 顔 顕 顗 願 顛 類 顥 顧 顫 顯 顰 顱 顳 页 顶 顷 项 顺 须 顼 顽 顾 顿 颁 颂 预 颅 领 颇 颈 颉 颊 颌 颍 颏 颐 频 颓 颔 颖 颗 题 颙 颚 颛 颜 额 颞 颠 颢 颤 風 颯 颱 颳 颶 飄 飆 飈 风 飒 飓 飘 飙 飚 飛 飜 飞 食 飡 飢 飨 飩 飪 飫 飭 飮 飯 飲 飴 飼 飽 飾 餃 餅 餉 養 餌 餐 餓 餘 餚 餞 餡 館 餮 餵 餾 饅 饉 饋 饌 饑 饒 饕 饗 饥 饪 饬 饭 饮 饯 饰 饱 饲 饵 饶 饷 饺 饼 饿 馀 馅 馆 馈 馏 馑 馒 馔 首 馗 馘 香 馥 馨 馬 馭 馮 馱 馳 馴 駁 駄 駅 駆 駈 駐 駒 駕 駙 駛 駝 駟 駢 駭 駱 駸 駿 騁 騎 騏 騒 験 騙 騨 騫 騭 騮 騰 騶 騷 騾 驃 驅 驊 驍 驒 驕 驗 驚 驛 驟 驢 驤 驥 驩 驪 马 驭 驮 驯 驰 驱 驳 驴 驶 驷 驸 驹 驺 驻 驼 驾 驿 骁 骂 骄 骅 骆 骇 骈 骊 骋 验 骏 骐 骑 骗 骘 骚 骝 骞 骠 骡 骤 骥 骧 骨 骯 骰 骶 骷 骸 骼 髀 髂 髄 髅 髋 髏 髑 髒 髓 體 髖 高 髙 髡 髣 髦 髪 髭 髮 髯 髴 髷 髹 髻 鬃 鬆 鬍 鬓 鬘 鬚 鬟 鬢 鬣 鬥 鬧 鬨 鬪 鬬 鬯 鬱 鬲 鬻 鬼 魁 魂 魃 魄 魅 魇 魉 魋 魍 魎 魏 魑 魔 魘 魚 魟 魣 魨 魮 魯 魴 魷 鮃 鮄 鮈 鮋 鮎 鮑 鮒 鮗 鮟 鮠 鮡 鮨 鮪 鮫 鮭 鮮 鯉 鯊 鯒 鯓 鯔 鯖 鯙 鯛 鯡 鯤 鯧 鯨 鯪 鯰 鯱 鯵 鯷 鯽 鰂 鰈 鰍 鰐 鰓 鰕 鰧 鰨 鰩 鰭 鰯 鰲 鰹 鰺 鰻 鰾 鱂 鱇 鱈 鱉 鱒 鱔 鱗 鱘 鱚 鱟 鱥 鱧 鱨 鱲 鱵 鱷 鱸 鱺 鱼 鱿 鲀 鲁 鲂 鲆 鲇 鲈 鲉 鲍 鲎 鲑 鲔 鲗 鲛 鲜 鲟 鲡 鲣 鲤 鲨 鲫 鲬 鲭 鲮 鲱 鲲 鲳 鲶 鲷 鲸 鲹 鲻 鲽 鲿 鳀 鳃 鳄 鳅 鳉 鳌 鳍 鳎 鳐 鳔 鳕 鳖 鳗 鳚 鳝 鳞 鳟 鳢 鳥 鳧 鳩 鳳 鳴 鳶 鴇 鴈 鴉 鴎 鴒 鴕 鴛 鴝 鴞 鴟 鴣 鴦 鴨 鴫 鴻 鴿 鵄 鵑 鵙 鵜 鵝 鵞 鵟 鵠 鵡 鵬 鵯 鵰 鵲 鵺 鶇 鶉 鶏 鶘 鶚 鶥 鶯 鶲 鶴 鶺 鶻 鶿 鷂 鷄 鷓 鷗 鷯 鷲 鷸 鷹 鷺 鸕 鸚 鸛 鸞 鸟 鸠 鸡 鸢 鸣 鸥 鸦 鸨 鸪 鸫 鸬 鸭 鸮 鸯 鸰 鸲 鸳 鸵 鸽 鸾 鸿 鹀 鹃 鹄 鹅 鹈 鹉 鹊 鹌 鹎 鹏 鹑 鹕 鹗 鹘 鹚 鹛 鹞 鹟 鹡 鹤 鹦 鹧 鹩 鹫 鹬 鹭 鹮 鹰 鹳 鹵 鹸 鹹 鹼 鹽 鹿 麁 麂 麋 麒 麓 麗 麝 麟 麤 麥 麦 麩 麴 麵 麸 麹 麺 麻 麼 麽 麾 麿 黃 黄 黌 黍 黎 黏 黐 黑 黒 黔 默 黙 黛 黜 黝 點 黟 黠 黥 黨 黯 黴 黻 黼 鼈 鼎 鼐 鼓 鼠 鼩 鼬 鼯 鼱 鼴 鼹 鼻 齊 齋 齎 齐 齒 齟 齡 齢 齣 齦 齧 齬 齶 齿 龄 龈 龍 龐 龑 龔 龕 龙 龚 龛 龜 龟 龢 龸 ꧉ 﨑 ﭖ ﭘ ﮏ ﮐ ﮟ ﮧ ﮨ ﮭ ﮯ ﯽ ﯾ ﯿ ﴾ ﴿ ﷲ ﷺ ︰ ﹐ ﹑ ﹒ ﹔ ﹕ ﹙ ﹚ ﹝ ﹞ ﹣ ﺍ ﺎ ﺑ ﺗ ﺟ ﺭ ﺮ ﺱ ﺳ ﻣ ﻦ ﻧ ﻭ ﻮ ！ ＂ ＃ ％ ＆ （ ） ＊ ＋ ， － ． ／ ０ １ ２ ３ ４ ５ ６ ７ ８ ９ ： ； ＜ ＝ ＞ ？ ＠ ［ ＼ ］ ＿ ａ ｂ ｃ ｄ ｅ ｆ ｇ ｈ ｉ ｊ ｋ ｌ ｍ ｎ ｏ ｐ ｒ ｓ ｔ ｖ ｗ ｘ ｚ ｛ ｜ ｝ ～ ｡ ｢ ｣ ､ ･ ｰ ￥ ￼ 𩽾 𩾌 𪨶 𫖮 𫚉 𫚒 𬀩 𬘭 𬞟 𬶋 . print(&#39;Number of single character tokens with hashes:&#39;, len(one_chars_hashes), &#39; n&#39;) # Print all of the single characters, 40 per row. # Strip the hash marks, since they just clutter the display. tokens = [token.replace(&#39;##&#39;, &#39;&#39;) for token in one_chars_hashes] # For every batch of 40 tokens... for i in range(0, len(tokens), 40): # Limit the end index so we don&#39;t go past the end of the list. end = min(i+40, len(tokens)+1) # Print out the tokens, seperated by a space. print(&#39; &#39;.join(tokens[i:end])) . Number of single character tokens with hashes: 9995 s e a n i t r o u m y l d k и а е у h g м c н z ی ы і о p ا т х ة я ه b j v f ה к ي ו ت ς л с р י ת в ю w x ن の م د ı ر ी 2 か 1 д ा 3 ل ь 0 ם ల п न ը г 4 ł ب س ן ो و র ר ն 5 ш 6 त ի 7 を ल ہ ে に ל ს న 9 8 र q は ə ز з य ა と ش ದ ス ন ు ᆫ ι ع ک ف て ನ ம ے क υ б য स ق ס க ى ° ד ב া ц ರ ч క ப ᆯ ン ש ј ק ð є ك ం త ು ף ర ಯ ح ન म ט గ ಕ ᆨ α ء ি ג ν ன ф ח ج ड ਾ ա ল ж ә ル ट ᆼ ர ि ை ல ᵉ ক ত ം ト య ο ੀ ਰ ਨ ց ע ట ী ᆷ ത ग ი ں ી も த ң ਲ ா ط द ા ಲ ി ತ က ൽ ಗ ಾ қ స व ಸ ज თ ण ਕ א ー し ম ி ો ク ך স ർ ム ү వ い ր च ז կ డ ய ય మ ద ص ք ض ս ই ર ß ث ო η ൻ ਤ ラ ச ਦ り տ ವ º യ گ ਸ प ٹ フ დ લ す ह خ ई ট き る ე ץ দ ಡ ಮ ব غ ક ત ਣ イ λ ら ब ಟ コ श ਆ τ ლ ਜ ೂ ਮ ਟ ᆸ ೕ ക く ட ۰ ρ చ ナ ª み タ സ સ ০ ० ာ ア た ए پ カ ओ մ ნ ん ಳ κ મ ᆺ ո တ မ ণ ு ড थ း ۲ ও щ ஸ э ε მ ణ գ ટ ₂ ന গ ۳ ڈ դ జ ே ۵ ۹ ৫ צ ध শ ள ৭ ಣ ৬ ۱ န ۴ ৩ ロ ۶ ² რ ৮ ۷ ћ ਗ ং फ ҡ ө ե ५ ৪ や ણ լ ख ડ ਹ چ ळ リ ৯ ১ ২ ۸ թ ూ ప ८ ø ६ ც ३ २ વ な െ マ μ হ စ ရ င छ ര ω १ ष জ け ४ פ ਡ ေ প ७ つ う こ থ ९ ష ウ દ ગ ാ ठ ਵ ಪ お ذ ಷ જ ઓ ھ シ ᅡ േ ਈ သ ᅢ π വ ظ ണ ま ಂ キ չ வ ծ ഡ શ ண ノ လ վ ტ ः オ բ れ ਪ ъ ڑ ಜ レ ₁ ᅩ ၀ ਬ ち և պ כ ൺ ढ ツ ₃ ハ せ ઈ ջ チ ੜ ষ ਚ ᅬ է ળ サ బ ҙ भ ᅵ િ ၅ զ უ ပ ယ ვ ³ ဝ ઇ ᅥ પ ચ इ φ ೦ ধ മ ಚ מ ヒ ၂ ધ ਖ ည კ え ദ ΄ ട ၈ ಥ ၁ め ᅲ ല ற љ ਧ ၄ ၆ ろ ధ ミ ₄ એ ళ ಶ চ ᅱ ಹ ၃ × ζ બ ਥ ঠ റ ၇ ਫ ᅦ հ æ ழ ヤ ഷ ½ խ હ ീ ၉ ೖ థ ခ খ ಧ హ ಬ ျ テ ौ へ נ ネ გ ホ ঃ さ ૦ ҳ భ ഥ ৎ घ ఫ ᅭ ケ ⁺ ᅧ ᅮ よ ೫ շ ռ ൾ ժ ছ ィ ニ ါ ਿ ʻ ᅳ エ ژ ഫ ங થ ワ ਏ ফ ひ ヌ δ ბ శ モ ബ ξ ヶ セ θ յ χ ₀ わ ভ ಫ ജ ഴ ூ ൈ ખ β ೨ პ ղ ထ പ ェ ങ ᅨ փ ね ほ ソ ಠ ճ ւ ೧ σ ಃ ള ശ ೮ ձ ₙ ೬ ೭ ─ झ ऊ メ ೪ ზ ャ ફ ფ そ ೯ ઠ ఖ ᆾ ァ ೩ ङ ხ ᵢ ঙ ஜ ષ ʳ ふ ֆ џ ਠ む ૧ ૭ ュ ˢ ഗ đ ⁿ ഖ ૫ ဒ ஷ ᅫ ქ ြ ᵐ ᅣ ʰ ઝ ಭ ૪ ۃ ぬ њ ಖ উ ૨ ૩ ঘ ! &#34; # $ % &amp; &#39; ( ) * + , - . / : ; &lt; = &gt; ? @ [ ] ^ _ { | } ~ ¡ ¢ £ ¤ ¥ ¦ § ¨ © « ¬ ® ¯ ± µ ¶ · ¹ » ¼ ¾ ¿ ÷ þ ħ ĳ ŉ ŋ œ ſ ƒ ǁ ɐ ɑ ɒ ɓ ɔ ɕ ɛ ɟ ɡ ɣ ɤ ɦ ɨ ɪ ɫ ɬ ɯ ɲ ɴ ɸ ɹ ɾ ʀ ʁ ʂ ʃ ʉ ʊ ʋ ʌ ʎ ʐ ʑ ʒ ʔ ʕ ʙ ʝ ʤ ʦ ʧ ʲ ʷ ʸ ʼ ʾ ʿ ˁ ˈ ˋ ˌ ː ˙ ˚ ˡ ˣ γ ψ ϕ ђ ѕ ѣ ѧ ѫ ѳ ґ ғ җ ҫ ұ ҷ ҹ һ ӊ ӏ ՚ ՛ ՜ ՝ ՞ օ ։ ֊ ־ ׃ ׳ ״ ، ؍ ؛ ؟ ـ ٠ ١ ٢ ٣ ٤ ٥ ٦ ٧ ٨ ٩ ٪ ٫ ٬ ٭ ٴ ڠ ڤ ڭ ۆ ۇ ۋ ێ ۔ ە ݨ अ आ उ ऋ ऍ ऎ ऐ ऑ ऒ औ ञ ऽ ॉ ॊ ॐ ॠ । ॥ ॰ ॲ অ আ ঈ ঊ ঋ এ ঐ ঔ ঝ ঞ ঢ ৈ ৗ ৰ ৱ ৷ ਅ ਇ ਉ ਊ ਐ ਓ ਔ ਘ ਛ ਝ ਢ ਭ ਯ ੦ ੧ ੨ ੩ ੪ ੫ ੬ ੭ ੮ ੯ ੲ ੳ ઃ અ આ ઉ ઊ ઋ ઍ ઐ ઑ ઔ ઘ છ ઞ ઢ ભ ૉ ૌ ૬ ૮ ૯ ஃ அ ஆ இ ஈ உ ஊ எ ஏ ஐ ஒ ஓ ஞ ந ஹ ெ ௗ ఁ ః అ ఆ ఇ ఈ ఉ ఊ ఋ ఎ ఏ ఐ ఒ ఓ ఔ ఘ ఙ ఛ ఝ ఞ ఠ ఢ ఱ ృ ಅ ಆ ಇ ಈ ಉ ಊ ಋ ಎ ಏ ಐ ಒ ಓ ಔ ಘ ಛ ಝ ಞ ಢ ೃ ഃ അ ആ ഇ ഈ ഉ ഊ ഋ എ ഏ ഐ ഒ ഓ ഔ ഘ ച ഛ ഝ ഞ ഠ ഢ ധ ഭ ഹ ൗ ൧ ൨ ൿ ක ද ප ම ය ර ල ශ ා ෙ ก ข ค ง จ ด ต ท น บ ป ภ ม ย ร ล ว ส ห อ า ำ เ แ ไ ་ ། ཀ ཁ ག ང ཆ ད ན པ ཕ བ མ ཚ ཟ འ ར ལ ཤ ས ဂ ဃ ဆ ဇ ဈ ဉ ဋ ဌ ဍ ဏ ဓ ဖ ဗ ဘ ဟ ဠ အ ဤ ဥ ဧ ဩ ဿ ၊ ။ ၌ ၍ ၎ ၏ ჟ ღ ყ შ ჩ ძ წ ჭ ჯ ჰ ჲ ᄀ ᄁ ᄂ ᄃ ᄄ ᄅ ᄆ ᄇ ᄈ ᄉ ᄊ ᄋ ᄌ ᄍ ᄎ ᄏ ᄐ ᄑ ᄒ ᅤ ᅪ ᅯ ᅰ ᅴ ᆩ ᆪ ᆬ ᆭ ᆮ ᆰ ᆱ ᆲ ᆵ ᆶ ᆹ ᆻ ᆽ ᆿ ᇀ ᇁ ᇂ ᛫ ᛬ ᠠ ᠨ ᠬ ᠭ ᠰ ᡳ ᴬ ᴮ ᴰ ᴵ ᴶ ᴷ ᴺ ᴼ ᴾ ᴿ ᵀ ᵂ ᵃ ᵇ ᵈ ᵍ ᵏ ᵒ ᵖ ᵗ ᵘ ᵛ ᵣ ᵤ ᵥ ᶜ ᶠ ᶻ ᾽ ‖ ‚ ‛ „ ‟ † ‡ • ․ ‥ ‧ ‰ ′ ″ ‹ › ※ ‼ ‿ ⁄ ⁰ ⁱ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ⁻ ⁾ ₅ ₆ ₇ ₈ ₉ ₊ ₍ ₎ ₐ ₑ ₒ ₓ ₕ ₖ ₗ ₘ ₚ ₛ ₜ ₤ ₩ ₪ € ₱ ₴ ₹ ℂ ℃ ℓ ℕ № ℚ ℝ ™ ℤ ⅓ ⅔ ⅛ ⅰ ⅱ ⅲ ⅳ ⅴ ⅵ ⅶ ← ↑ → ↓ ↔ ↗ ↦ ↺ ⇄ ⇌ ⇒ ⇔ ⇧ ∀ ∂ ∃ ∅ ∆ ∇ ∈ ∑ − ∕ ∗ ∘ ∙ √ ∞ ∥ ∧ ∨ ∩ ∪ ∫ ∴ ∼ ≅ ≈ ≒ ≡ ≤ ≥ ≪ ≫ ⊂ ⊃ ⊆ ⊕ ⊗ ⊙ ⊞ ⊥ ⋅ ⋯ ① ② ③ ④ ⑤ ⑥ ⑦ ⑧ ⓐ ━ │ ┃ ┏ ┓ └ ┗ ┛ ├ ┣ ┫ ┬ ┳ ║ ╱ █ ■ □ ▪ ▬ ▭ ▲ △ ▶ ► ▼ ▽ ◆ ◇ ◊ ○ ◌ ◎ ● ◡ ◦ ◯ ★ ☆ ☉ ♀ ♂ ♕ ♖ ♗ ♘ ♙ ♠ ♡ ♣ ♥ ♦ ♪ ♭ ♯ ⚭ ✝ ⟨ ⟩ ⟪ ⟫ ⟶ ⱼ ⵏ ⺩ ⽥ 、 。 〃 々 〆 〇 〈 〉 《 》 「 」 『 』 【 】 〒 〔 〕 〖 〗 〜 〝 〞 〟 ぁ あ ぃ ぅ ぇ ぉ っ ゃ ゅ ゆ ょ ゐ ゑ ゝ ゥ ォ ッ ヘ ユ ョ ヨ ヰ ヱ ヲ ヵ ・ ㄚ ㄧ ㄨ ㄱ ㄴ ㄷ ㄹ ㅁ ㅂ ㅅ ㅇ ㅈ ㅡ ㅣ ㆍ ㈱ ㎒ ㎞ ㎡ 㭴 㹴 䜣 䲁 䲗 䲢 一 丁 七 万 丈 三 上 下 不 与 丐 丑 专 且 丕 世 丘 丙 业 丛 东 丝 丞 丟 両 丢 两 严 並 丧 个 丫 中 丰 串 临 丶 丸 丹 为 主 丼 丽 举 乂 乃 久 么 义 之 乌 乍 乎 乏 乐 乒 乓 乔 乖 乗 乘 乙 乜 九 乞 也 习 乡 书 乩 乭 买 乱 乳 乾 亀 亂 了 予 争 事 二 于 亏 云 互 五 井 亘 亙 亚 些 亜 亞 亟 亡 亢 交 亥 亦 产 亨 亩 享 京 亭 亮 亰 亲 亳 亵 亶 人 亻 亿 什 仁 仃 仄 仅 仆 仇 今 介 仍 从 仏 仑 仓 仔 仕 他 仗 付 仙 仝 仞 仟 仡 代 令 以 仪 们 仮 仰 仲 仵 件 价 任 份 仿 企 伉 伊 伋 伍 伎 伏 伐 休 伕 众 优 伙 会 伝 伞 伟 传 伤 伦 伪 伫 伯 估 伴 伶 伷 伸 伺 似 伽 佃 但 佇 佈 佉 位 低 住 佐 佑 体 佔 何 佗 佘 余 佚 佛 作 佞 佟 你 佢 佣 佤 佥 佩 佬 佯 佰 佳 併 佶 佺 佻 佼 佾 使 侂 侃 侄 來 侈 例 侍 侏 侑 侖 侗 侘 供 依 侠 価 侣 侥 侦 侧 侨 侪 侬 侭 侮 侯 侵 侶 侷 便 俁 係 促 俄 俅 俊 俎 俏 俐 俑 俗 俘 俚 保 俞 俟 俠 信 俣 俥 俦 俨 俩 俪 俬 俭 修 俯 俱 俳 俵 俶 俸 俺 俾 倂 倆 倉 個 倍 們 倒 倓 倔 倖 倘 候 倚 倜 借 倡 倣 値 倦 倧 倩 倪 倫 倬 倭 倮 倶 倹 债 倻 值 倾 偁 偃 假 偈 偉 偏 偕 做 停 健 偰 偲 側 偵 偶 偷 偽 偿 傀 傅 傈 傉 傍 傑 傕 傘 備 傚 傢 傣 储 催 傭 傲 傳 債 傷 傻 傾 僅 僉 僊 働 像 僑 僕 僖 僚 僞 僥 僧 僭 僮 僱 僳 僵 價 僻 儀 儁 儂 億 儆 儉 儋 儒 儔 儕 儘 儚 償 儡 優 儲 儷 儸 儺 儼 儿 兀 允 元 兄 充 兆 兇 先 光 克 兌 免 兎 児 兑 兒 兔 兖 兗 党 兜 兢 入 內 全 兩 兪 八 公 六 兮 兰 共 关 兴 兵 其 具 典 兹 养 兼 兽 冀 内 円 冇 冈 冉 冊 册 再 冏 冑 冒 冕 冗 写 军 农 冠 冢 冤 冥 冨 冪 冬 冯 冰 冲 决 冴 况 冶 冷 冻 冼 冽 净 凄 准 凈 凉 凊 凋 凌 凍 减 凑 凖 凛 凜 凝 几 凡 凤 処 凧 凪 凫 凭 凯 凰 凱 凳 凶 凸 凹 出 击 凼 函 凿 刀 刁 刃 分 切 刈 刊 刍 刎 刑 划 列 刘 则 刚 创 初 删 判 別 刨 利 刪 别 刮 到 刳 制 刷 券 刹 刺 刻 刽 剁 剂 剃 則 削 剋 剌 前 剎 剑 剔 剖 剛 剝 剡 剣 剤 剥 剧 剩 剪 副 剰 剱 割 創 剷 剽 剿 劃 劇 劈 劉 劊 劍 劑 劔 力 劝 办 功 加 务 劢 劣 动 助 努 劫 劭 励 劲 劳 労 劵 効 劻 劾 势 勁 勃 勅 勇 勉 勋 勐 勒 勔 動 勖 勗 勘 務 勛 勝 勞 募 勢 勣 勤 勧 勰 勱 勲 勳 勵 勸 勺 勻 勾 勿 匀 匁 匂 包 匆 匈 匍 匏 匐 匕 化 北 匙 匝 匠 匡 匣 匪 匮 匯 匱 匹 区 医 匾 匿 區 十 千 卅 升 午 卉 半 卍 华 协 卐 卑 卒 卓 協 单 卖 南 単 博 卜 卞 卟 占 卡 卢 卤 卦 卧 卫 卬 卯 印 危 即 却 卵 卷 卸 卹 卻 卽 卿 厂 厄 厅 历 厉 压 厌 厓 厔 厕 厘 厚 厝 原 厢 厥 厦 厨 厩 厭 厮 厲 厳 去 县 叁 参 參 叄 又 叉 及 友 双 反 収 发 叔 取 受 变 叙 叛 叟 叠 叡 叢 口 古 句 另 叨 叩 只 叫 召 叭 叮 可 台 叱 史 右 叶 号 司 叹 叻 叼 吁 吃 各 合 吉 吊 吋 同 名 后 吏 吐 向 吒 吓 吕 吗 君 吝 吞 吟 吠 吡 否 吧 吨 吩 含 听 吮 启 吱 吲 吳 吴 吵 吶 吸 吹 吻 吼 吽 吾 呀 呂 呃 呆 呈 呉 告 呋 呎 呐 呑 呕 呗 员 呛 呜 呟 呢 呤 呦 周 呪 呱 味 呵 呷 呻 呼 命 咀 咁 咄 咆 咋 和 咎 咏 咐 咒 咔 咕 咖 咗 咙 咚 咤 咥 咧 咨 咩 咪 咫 咬 咭 咯 咱 咲 咳 咸 咽 咿 哀 品 哂 哄 哆 哇 哈 哉 哋 哌 响 哎 哏 哑 哒 哔 哗 哙 哚 哟 員 哥 哦 哨 哩 哪 哭 哮 哲 哺 哼 哽 唁 唄 唆 唇 唎 唐 唑 唔 唖 唛 唢 唤 唧 唬 唭 售 唯 唱 唵 唷 唸 唾 啃 啄 商 啉 啊 問 啓 啖 啜 啞 啟 啡 啤 啥 啦 啪 啬 啮 啰 啲 啶 啸 啼 啾 喀 喂 喃 善 喆 喇 喉 喊 喋 喔 喘 喙 喚 喜 喝 喧 喩 喪 喫 喬 單 喰 喱 喲 喵 営 喷 喹 喺 喻 嗅 嗆 嗇 嗎 嗓 嗔 嗚 嗜 嗝 嗟 嗡 嗣 嗤 嗨 嗩 嗪 嗯 嗲 嗶 嗽 嘅 嘆 嘈 嘉 嘌 嘎 嘏 嘔 嘗 嘘 嘛 嘜 嘟 嘢 嘧 嘩 嘯 嘱 嘲 嘴 嘶 嘹 嘻 嘿 噁 噂 噌 噏 噓 噗 噛 噜 噠 噢 噤 器 噩 噪 噫 噬 噱 噲 噴 噶 噸 噹 噺 噻 嚆 嚇 嚈 嚎 嚏 嚐 嚕 嚙 嚟 嚢 嚣 嚥 嚨 嚮 嚴 嚷 嚼 囁 囂 囃 囉 囊 囍 囑 囓 囗 囚 四 囝 回 因 囡 团 団 囤 囧 囪 园 囮 困 囱 囲 図 围 固 国 图 囿 圀 圃 圆 圈 圉 國 圍 圏 園 圓 圖 團 圜 土 圣 圧 在 圩 圪 圭 圮 圯 地 圳 圹 场 圻 圾 址 坂 均 坊 坌 坍 坎 坏 坐 坑 块 坚 坛 坜 坝 坞 坟 坠 坡 坤 坦 坨 坩 坪 坬 坭 坯 坳 坵 坷 坻 垂 垃 垄 垅 垈 型 垌 垒 垓 垕 垚 垛 垟 垠 垡 垢 垣 垦 垩 垫 垭 垮 垴 垵 垸 埂 埃 埈 埋 城 埏 埒 埔 埕 埗 埙 埜 埝 域 埠 埡 埤 埭 埴 埵 執 埸 培 基 埼 堀 堂 堃 堅 堆 堇 堉 堊 堑 堕 堝 堞 堡 堤 堪 堯 堰 報 場 堵 堺 塀 塁 塊 塋 塌 塑 塔 塗 塘 塙 塚 塞 塡 塢 塩 填 塬 塭 塱 塵 塹 塽 塾 墀 境 墅 墉 墊 墓 墕 増 墘 墙 墜 增 墟 墨 墩 墬 墮 墳 墺 墻 墾 壁 壅 壆 壇 壊 壌 壑 壓 壕 壘 壙 壞 壟 壠 壢 壤 壩 士 壬 壮 壯 声 壱 売 壳 壶 壷 壹 壺 壽 处 备 変 复 夏 夔 夕 外 夙 多 夜 够 夠 夢 夤 夥 大 天 太 夫 夭 央 夯 失 头 夷 夸 夹 夺 夼 夾 奂 奄 奇 奈 奉 奋 奎 奏 奐 契 奔 奕 奖 套 奘 奚 奠 奢 奥 奧 奨 奪 奬 奭 奮 女 奴 奶 奸 她 好 如 妃 妄 妆 妇 妈 妊 妍 妒 妓 妖 妙 妝 妞 妣 妤 妥 妨 妫 妬 妮 妲 妳 妹 妻 妾 姆 姉 姊 始 姍 姐 姑 姒 姓 委 姗 姚 姜 姝 姣 姥 姦 姨 姪 姫 姬 姵 姶 姻 姿 威 娃 娄 娅 娆 娇 娉 娑 娓 娘 娛 娜 娟 娠 娣 娥 娩 娯 娱 娲 娴 娶 娼 婁 婆 婉 婕 婚 婢 婦 婧 婪 婬 婭 婴 婵 婶 婷 婺 婿 媒 媚 媛 媞 媧 媯 媲 媳 媽 媾 嫁 嫂 嫄 嫉 嫌 嫔 嫖 嫗 嫚 嫡 嫣 嫦 嫩 嫻 嬅 嬈 嬉 嬋 嬌 嬖 嬗 嬛 嬢 嬤 嬪 嬬 嬰 嬲 嬴 嬷 嬸 嬿 孀 孃 子 孑 孔 孕 孖 字 存 孙 孚 孛 孜 孝 孟 孢 季 孤 学 孩 孪 孫 孰 孱 孳 孵 學 孺 孽 孿 宀 宁 它 宅 宇 守 安 宋 完 宍 宏 宓 宕 宗 官 宙 定 宛 宜 宝 实 実 宠 审 客 宣 室 宥 宦 宪 宫 宮 宰 害 宴 宵 家 宸 容 宽 宾 宿 寂 寄 寅 密 寇 富 寐 寒 寓 寔 寘 寛 寝 寞 察 寡 寢 寥 實 寧 寨 審 寫 寬 寮 寯 寰 寳 寵 寶 寸 对 寺 寻 导 対 寿 封 専 射 将 將 專 尉 尊 尋 對 導 小 少 尔 尕 尖 尘 尙 尚 尝 尤 尧 尪 尬 尭 就 尴 尷 尸 尹 尺 尻 尼 尽 尾 尿 局 屁 层 居 屆 屈 届 屋 屌 屍 屎 屏 屐 屑 屓 展 属 屠 屡 屢 層 履 屬 屯 山 屹 屿 岀 岁 岂 岌 岐 岑 岔 岖 岗 岘 岙 岚 岛 岡 岢 岩 岫 岬 岭 岱 岳 岷 岸 峁 峄 峇 峋 峒 峙 峠 峡 峤 峥 峦 峨 峩 峪 峭 峯 峰 峴 島 峻 峽 崁 崂 崆 崇 崋 崍 崎 崐 崑 崔 崖 崗 崙 崚 崛 崞 崢 崤 崧 崩 崭 崮 崱 崴 崽 嵇 嵊 嵋 嵌 嵐 嵘 嵙 嵜 嵩 嵬 嵯 嵴 嶂 嶄 嶇 嶋 嶌 嶗 嶙 嶝 嶠 嶧 嶴 嶷 嶸 嶺 嶼 嶽 巂 巅 巌 巍 巒 巔 巖 川 州 巡 巢 巣 工 左 巧 巨 巩 巫 差 己 已 巳 巴 巷 巻 巽 巾 巿 币 市 布 帅 帆 师 希 帐 帑 帕 帖 帘 帙 帚 帛 帜 帝 帥 带 帧 師 席 帮 帯 帰 帳 帶 帷 常 帼 帽 幀 幂 幄 幅 幇 幌 幔 幕 幗 幟 幡 幢 幣 幪 幫 干 平 年 并 幷 幸 幹 幺 幻 幼 幽 幾 广 庁 広 庄 庆 庇 床 序 庐 庑 库 应 底 庖 店 庙 庚 府 庞 废 庠 庥 度 座 庫 庭 庵 庶 康 庸 庹 庾 廁 廂 廃 廄 廆 廈 廉 廊 廍 廓 廖 廙 廚 廝 廞 廟 廠 廡 廢 廣 廨 廩 廪 廬 廳 延 廷 廸 建 廻 廼 廿 开 弁 异 弃 弄 弇 弈 弉 弊 弋 弌 式 弐 弑 弒 弓 弔 引 弖 弗 弘 弛 弟 张 弢 弥 弦 弧 弩 弭 弯 弱 張 強 弹 强 弼 弾 彀 彅 彈 彊 彌 彎 归 当 录 彗 彘 彙 彝 彡 形 彤 彥 彦 彧 彩 彪 彫 彬 彭 彰 影 彳 彷 役 彻 彼 彿 往 征 徂 径 待 徇 很 徊 律 後 徐 徑 徒 従 徕 得 徘 徙 從 徠 御 徨 復 循 徬 徭 微 徳 徴 徵 德 徹 徽 心 忄 必 忆 忉 忌 忍 忏 忒 志 忘 忙 応 忞 忠 忡 忤 忧 快 忱 念 忻 忽 忿 怀 态 怂 怎 怒 怕 怖 怙 怛 怜 思 怠 怡 急 怦 性 怨 怪 怯 总 怿 恂 恃 恆 恋 恍 恐 恒 恕 恙 恚 恢 恣 恤 恥 恨 恩 恪 恫 恬 恭 息 恰 恳 恵 恶 恸 恺 恼 恽 恿 悄 悅 悉 悌 悍 悔 悖 悚 悛 悝 悟 悠 患 悦 悧 您 悩 悪 悫 悬 悯 悰 悲 悳 悴 悶 悸 悼 悽 情 惇 惊 惋 惑 惕 惘 惚 惛 惜 惟 惠 惡 惣 惧 惨 惩 惫 惭 惮 惯 惰 惱 惲 想 惶 惹 惺 愁 愃 愆 愈 愉 愍 愎 意 愔 愕 愚 愛 感 愤 愧 愨 愫 愴 愷 愼 愾 愿 慄 慇 慈 態 慌 慎 慑 慕 慘 慚 慜 慟 慢 慣 慤 慧 慨 慫 慮 慰 慳 慵 慶 慷 慾 憂 憊 憍 憎 憐 憑 憔 憙 憚 憤 憧 憨 憩 憫 憬 憲 憶 憺 憾 懂 懃 懇 懈 應 懊 懋 懌 懐 懒 懦 懲 懵 懶 懷 懸 懺 懼 懾 懿 戀 戈 戊 戌 戍 戎 戏 成 我 戒 戔 戕 或 战 戚 戛 戟 戡 戢 戦 戩 截 戮 戯 戰 戱 戲 戳 戴 戶 户 戸 戻 戾 房 所 扁 扆 扇 扈 扉 手 扌 才 扎 扑 扒 打 扔 払 托 扛 扣 扦 执 扩 扫 扬 扭 扮 扯 扰 扱 扳 扶 批 扼 找 承 技 抄 抉 把 抑 抒 抓 投 抖 抗 折 抚 抛 抜 択 抟 抡 抢 护 报 抨 披 抬 抱 抵 抹 押 抽 拂 担 拆 拇 拈 拉 拋 拌 拍 拏 拐 拒 拓 拔 拖 拗 拘 拙 拚 招 拜 拝 拟 拠 拡 拢 拣 拥 拦 拨 择 括 拭 拮 拯 拱 拳 拴 拵 拶 拷 拼 拽 拾 拿 持 挂 指 挈 按 挑 挖 挙 挚 挛 挝 挞 挟 挠 挡 挣 挤 挥 挨 挪 挫 振 挹 挺 挽 挾 挿 捅 捆 捉 捌 捍 捎 捏 捐 捒 捕 捗 捜 捞 损 捡 换 捣 捧 捨 捩 据 捱 捲 捶 捷 捺 捻 掀 掃 授 掉 掌 掏 掐 排 掖 掘 掙 掛 掟 掠 採 探 掣 接 控 推 掩 措 掬 掰 掲 掳 掴 掷 掸 掺 掻 掾 揀 揃 揄 揆 揉 揍 描 提 插 揖 揚 換 握 揣 揪 揭 揮 援 揶 揺 揽 揾 搁 搅 損 搏 搐 搓 搔 搖 搗 搜 搞 搦 搪 搬 搭 搵 搶 携 搾 摂 摄 摆 摇 摊 摑 摒 摔 摘 摠 摧 摩 摭 摯 摶 摸 摹 摺 摻 撃 撇 撈 撐 撑 撒 撓 撕 撚 撞 撣 撤 撥 撩 撫 撬 播 撮 撰 撲 撷 撹 撻 撼 撿 擁 擂 擄 擅 擇 擊 擋 操 擎 擒 擔 擘 據 擠 擢 擦 擧 擬 擱 擲 擴 擷 擺 擾 攀 攏 攒 攔 攘 攜 攝 攢 攣 攤 攪 攫 攬 支 收 攷 攸 改 攻 放 政 故 效 敌 敍 敎 敏 救 敕 敖 敗 敘 教 敛 敝 敞 敢 散 敦 敬 数 敲 整 敵 敷 數 斂 斃 文 斉 斋 斌 斎 斐 斑 斗 料 斛 斜 斟 斡 斤 斥 斧 斩 斫 斬 断 斯 新 斷 方 於 施 旁 旃 旅 旋 旌 族 旒 旗 旛 无 既 旣 日 旦 旧 旨 早 旬 旭 旱 时 旷 旸 旺 旻 旼 昀 昂 昆 昇 昉 昊 昌 明 昏 昐 易 昔 昕 昙 昝 昞 星 映 春 昧 昨 昪 昭 是 昰 昱 昴 昵 昶 昺 昼 显 晁 時 晃 晄 晉 晋 晏 晒 晓 晔 晕 晖 晗 晙 晚 晝 晞 晟 晤 晦 晧 晨 晩 普 景 晰 晳 晴 晶 晷 晸 智 晾 暁 暂 暄 暇 暈 暉 暌 暎 暐 暑 暖 暗 暘 暝 暠 暢 暦 暧 暨 暫 暮 暱 暲 暴 暹 暻 暾 曄 曆 曇 曉 曖 曙 曜 曝 曠 曦 曩 曬 曰 曲 曳 更 曷 書 曹 曺 曼 曽 曾 替 最 會 月 有 朋 服 朐 朓 朔 朕 朗 望 朝 期 朦 朧 木 未 末 本 札 朮 术 朱 朴 朵 朶 机 朽 杀 杂 权 杆 杉 杌 李 杏 材 村 杓 杖 杙 杜 杞 束 杠 条 杢 杣 来 杨 杭 杮 杯 杰 東 杲 杳 杵 杷 杼 松 板 极 构 枇 枉 枋 析 枕 林 枚 果 枝 枞 枠 枡 枢 枣 枪 枫 枭 枯 枳 架 枷 枸 枹 柁 柃 柄 柊 柏 某 柑 柒 染 柔 柘 柚 柜 柝 柞 柠 柢 查 柩 柬 柯 柱 柳 柴 柵 査 柽 柾 柿 栂 栃 栄 栅 标 栈 栉 栋 栎 栏 树 栒 栓 栖 栗 栞 校 栢 栩 株 栱 栲 栴 样 核 根 栻 格 栽 栾 桀 桁 桂 桃 桅 框 案 桉 桌 桎 桐 桑 桓 桔 桜 桝 桟 桡 桢 档 桥 桦 桧 桨 桩 桫 桶 桷 桿 梁 梃 梅 梆 梓 梔 梗 條 梟 梠 梢 梦 梧 梨 梭 梯 械 梱 梳 梵 梶 检 棂 棄 棉 棋 棍 棒 棕 棗 棘 棚 棟 棠 棣 棧 棨 棫 森 棱 棲 棵 棹 棺 棻 椀 椁 椅 椋 植 椎 椏 椒 椙 椚 椛 検 椭 椰 椴 椹 椽 椿 楂 楊 楓 楔 楕 楙 楚 楝 楞 楠 楡 楢 楣 楨 楫 業 楮 楯 楳 極 楷 楸 楹 楼 楽 概 榄 榆 榈 榉 榊 榎 榑 榔 榕 榖 榘 榛 榜 榧 榨 榫 榭 榮 榴 榷 榻 槁 槃 槇 槊 構 槌 槍 槎 槐 槓 様 槙 槛 槟 槤 槨 槪 槭 槲 槳 槺 槻 槽 槿 樁 樂 樅 樊 樋 樑 樓 樗 標 樞 樟 模 樣 樨 権 横 樫 樱 樵 樸 樹 樺 樽 樾 橄 橇 橈 橋 橐 橘 橙 機 橡 橢 橫 橱 橹 橿 檀 檄 檉 檎 檐 檔 檗 檜 檢 檣 檬 檯 檳 檸 檻 櫂 櫃 櫓 櫚 櫛 櫟 櫥 櫨 櫸 櫻 欄 欅 欉 權 欒 欖 欞 欠 次 欢 欣 欤 欧 欲 欺 欽 款 歆 歇 歉 歌 歎 歐 歓 歙 歟 歡 止 正 此 步 武 歧 歩 歪 歯 歲 歳 歴 歷 歸 歹 死 歼 歿 殁 殃 殆 殇 殉 殊 残 殒 殓 殖 殘 殞 殡 殤 殭 殮 殯 殲 殴 段 殷 殺 殻 殼 殿 毀 毁 毂 毅 毆 毋 毌 母 毎 每 毒 毓 比 毕 毖 毗 毘 毙 毛 毡 毫 毬 毯 毽 氂 氈 氏 氐 民 氓 气 氖 気 氘 氙 氚 氛 氟 氡 氢 氣 氦 氧 氨 氩 氪 氫 氬 氮 氯 氰 水 氵 氷 永 氹 氾 汀 汁 求 汇 汉 汊 汎 汐 汕 汗 汙 汚 汛 汜 汝 汞 江 池 污 汤 汧 汨 汪 汭 汰 汲 汴 汶 汹 決 汽 汾 沁 沂 沃 沄 沅 沆 沈 沉 沌 沐 沒 沓 沔 沖 沙 沛 沟 没 沢 沣 沥 沦 沧 沪 沫 沭 沮 沱 河 沸 油 治 沼 沽 沾 沿 況 泄 泉 泊 泌 泓 法 泗 泚 泛 泞 泠 泡 波 泣 泥 注 泪 泫 泮 泯 泰 泱 泳 泵 泷 泸 泻 泼 泽 泾 洁 洄 洋 洎 洒 洗 洙 洛 洞 津 洧 洩 洪 洮 洱 洲 洵 洶 洸 洹 洺 活 洼 洽 派 流 浄 浅 浆 浇 浊 测 济 浏 浑 浒 浓 浔 浙 浚 浜 浞 浠 浣 浤 浦 浩 浪 浬 浮 浯 浴 海 浸 涂 涅 涇 消 涉 涌 涎 涓 涔 涕 涙 涛 涜 涝 涞 涟 涡 涣 涤 润 涧 涨 涩 涪 涮 涯 液 涵 涸 涼 涿 淀 淄 淅 淆 淇 淋 淌 淑 淒 淖 淘 淙 淚 淝 淞 淡 淤 淦 淨 淩 淪 淫 淬 淮 淯 深 淳 淵 淶 混 淸 淹 淺 添 淼 清 渇 済 渉 渊 渋 渍 渎 渐 渓 渔 渕 渗 渙 渚 減 渝 渟 渠 渡 渣 渤 渥 渦 温 渫 測 渭 港 渲 渴 游 渺 渾 湃 湄 湊 湍 湖 湘 湛 湜 湟 湣 湧 湫 湮 湯 湳 湾 湿 満 溃 溅 溆 溉 溏 源 準 溜 溝 溟 溢 溥 溧 溪 溫 溯 溱 溲 溴 溶 溺 溼 滁 滂 滄 滅 滇 滉 滋 滌 滎 滏 滑 滓 滔 滕 滘 滙 滚 滝 滞 满 滢 滤 滥 滦 滨 滩 滬 滯 滲 滴 滷 滸 滹 滾 滿 漁 漂 漆 漉 漏 漑 漓 演 漕 漠 漢 漣 漩 漪 漫 漬 漯 漱 漲 漳 漵 漸 漾 漿 潁 潇 潍 潑 潔 潘 潛 潜 潞 潟 潢 潤 潦 潭 潮 潯 潰 潴 潺 潼 潾 澀 澁 澂 澄 澆 澇 澈 澍 澎 澗 澜 澠 澡 澤 澧 澪 澮 澱 澳 澶 澹 激 濁 濂 濃 濉 濊 濑 濒 濕 濘 濛 濞 濟 濠 濡 濤 濫 濬 濮 濯 濰 濱 濵 濺 濾 瀅 瀆 瀉 瀋 瀏 瀑 瀕 瀘 瀚 瀛 瀝 瀞 瀟 瀧 瀨 瀬 瀰 瀾 灃 灌 灏 灑 灕 灘 灝 灞 灣 灤 火 灭 灯 灰 灵 灶 灸 灼 災 灾 灿 炀 炁 炅 炆 炉 炊 炎 炒 炔 炕 炖 炘 炙 炜 炤 炫 炬 炭 炮 炯 炱 炳 炸 点 為 炼 炽 烁 烂 烃 烈 烏 烘 烙 烛 烜 烝 烟 烤 烦 烧 烨 烩 烫 烬 热 烯 烴 烷 烹 烺 烽 焉 焊 焓 焔 焕 焗 焘 焙 焚 焜 無 焦 焯 焰 焱 然 焼 煇 煉 煊 煌 煎 煒 煕 煖 煙 煚 煜 煞 煤 煥 煦 照 煨 煩 煬 煮 煲 煽 熄 熈 熊 熏 熒 熔 熙 熟 熠 熨 熬 熱 熲 熵 熹 熾 燁 燃 燄 燈 燉 燊 燎 燐 燒 燔 燕 燗 燙 營 燥 燦 燧 燬 燭 燮 燴 燹 燻 燼 燾 燿 爀 爆 爍 爐 爛 爨 爪 爬 爭 爰 爱 爲 爵 父 爷 爸 爹 爺 爻 爽 爾 牂 牆 片 版 牌 牍 牒 牘 牙 牛 牝 牟 牠 牡 牢 牧 物 牯 牲 牴 牵 特 牺 牻 牽 犀 犁 犂 犄 犊 犍 犒 犛 犠 犢 犧 犬 犯 犰 状 犷 犸 犹 犽 狀 狂 狄 狈 狍 狎 狐 狒 狗 狙 狛 狠 狡 狨 狩 独 狭 狮 狱 狳 狷 狸 狹 狼 狽 猁 猊 猎 猕 猖 猗 猛 猜 猝 猞 猟 猥 猩 猪 猫 猬 献 猴 猶 猷 猾 猿 獁 獄 獅 獎 獏 獐 獒 獗 獠 獣 獨 獬 獭 獰 獲 獴 獵 獷 獸 獺 獻 獼 獾 玄 玆 率 玉 王 玑 玕 玖 玘 玛 玟 玠 玢 玥 玩 玫 玮 环 现 玲 玳 玷 玹 玺 玻 珀 珂 珅 珈 珉 珊 珍 珏 珐 珑 珙 珝 珞 珠 珣 珥 珦 珩 珪 班 珮 珲 珺 珽 現 球 琅 理 琇 琉 琊 琍 琏 琐 琚 琛 琢 琤 琥 琦 琨 琪 琬 琮 琯 琰 琲 琳 琴 琵 琶 琺 琼 琿 瑀 瑁 瑄 瑈 瑊 瑋 瑕 瑗 瑙 瑚 瑛 瑜 瑞 瑟 瑠 瑢 瑣 瑤 瑩 瑪 瑭 瑯 瑰 瑱 瑳 瑶 瑷 瑾 璀 璁 璃 璆 璇 璈 璉 璋 璐 璘 璜 璞 璟 璠 璣 璥 璦 璧 璨 璩 璪 環 璲 璹 璽 璿 瓊 瓌 瓏 瓒 瓔 瓘 瓚 瓛 瓜 瓠 瓢 瓣 瓦 瓩 瓮 瓯 瓶 瓷 甄 甌 甑 甕 甘 甚 甜 生 甡 產 産 甥 甦 用 甩 甫 甬 甯 田 由 甲 申 电 男 甸 町 画 甾 畀 畅 畈 畋 界 畏 畑 畔 留 畜 畝 畠 畢 畤 略 畦 番 畫 畬 畯 異 畲 畳 畴 畵 當 畷 畸 畹 畿 疃 疆 疇 疊 疋 疍 疎 疏 疑 疒 疗 疙 疚 疝 疟 疡 疣 疤 疥 疫 疮 疯 疱 疲 疵 疸 疹 疼 疽 疾 痂 病 症 痉 痊 痍 痒 痔 痕 痘 痙 痛 痞 痢 痣 痤 痩 痪 痫 痰 痲 痴 痹 痺 瘀 瘁 瘋 瘍 瘓 瘙 瘟 瘠 瘡 瘢 瘤 瘦 瘧 瘩 瘫 瘴 瘾 療 癇 癌 癒 癖 癡 癢 癣 癥 癩 癪 癫 癬 癮 癱 癲 癸 発 登 發 白 百 皂 的 皆 皇 皈 皋 皎 皐 皓 皖 皙 皝 皮 皰 皱 皺 皿 盂 盃 盅 盆 盈 益 盎 盏 盐 监 盒 盔 盖 盗 盘 盛 盜 盞 盟 盡 監 盤 盥 盧 盩 盪 目 盯 盱 盲 直 相 盼 盾 省 眈 眉 看 県 眙 眞 真 眠 眨 眩 眭 眶 眷 眸 眺 眼 眾 着 睁 睇 睐 睑 睛 睜 睞 睡 睢 督 睦 睨 睪 睫 睬 睹 睺 睽 睾 睿 瞄 瞋 瞌 瞎 瞑 瞒 瞞 瞥 瞧 瞩 瞪 瞬 瞭 瞰 瞳 瞻 瞼 瞽 瞿 矇 矍 矗 矚 矛 矜 矢 矣 知 矧 矩 矫 短 矮 矯 石 矶 矽 矾 矿 砀 码 砂 砌 砍 砒 研 砕 砖 砚 砜 砥 砦 砧 砬 砭 砰 砲 破 砵 砷 砸 砺 砾 砿 础 硃 硅 硏 硐 硒 硕 硖 硝 硤 硫 硬 确 硯 硼 碁 碇 碉 碌 碍 碎 碑 碓 碕 碗 碘 碚 碛 碟 碣 碧 碩 碭 碰 碱 碲 碳 碴 碶 碸 確 碼 碾 磁 磅 磊 磋 磐 磔 磕 磘 磚 磡 磧 磨 磬 磯 磴 磷 磺 磻 磾 礁 礎 礒 礙 礦 礪 礫 礬 礴 示 礼 礽 社 祀 祁 祂 祆 祇 祈 祉 祎 祏 祐 祓 祔 祕 祖 祗 祚 祛 祜 祝 神 祟 祠 祢 祥 票 祭 祯 祷 祸 祺 祿 禀 禁 禄 禅 禊 禍 禎 福 禑 禔 禕 禛 禦 禧 禩 禪 禮 禰 禱 禹 禺 离 禽 禾 禿 秀 私 秃 秆 秉 秋 种 科 秒 秘 租 秣 秤 秦 秧 秩 秭 积 称 秸 移 秽 稀 稃 稅 稈 程 稍 税 稔 稗 稙 稚 稜 稟 稠 稣 種 稱 稲 稳 稷 稹 稻 稼 稽 稿 穀 穂 穆 穌 積 穎 穏 穐 穗 穢 穣 穩 穫 穰 穴 究 穷 穹 空 穿 突 窃 窄 窈 窍 窑 窒 窓 窕 窖 窗 窘 窜 窝 窟 窠 窣 窥 窦 窩 窪 窮 窯 窺 窿 竄 竅 竇 竈 竊 立 竑 竖 站 竜 竝 竞 竟 章 竣 童 竦 竪 竭 端 競 竹 竺 竿 笃 笄 笆 笈 笋 笏 笑 笔 笕 笙 笛 笞 笠 笥 符 笨 笪 第 笮 笳 笹 笺 笼 筅 筆 筈 等 筊 筋 筌 筍 筏 筐 筑 筒 答 策 筛 筝 筠 筥 筧 筮 筰 筱 筲 筵 筷 筹 筺 签 简 箇 箋 箍 箏 箐 箒 箓 箔 箕 算 箚 箝 管 箪 箫 箬 箭 箱 箴 箸 節 篁 範 篆 篇 築 篋 篙 篝 篠 篡 篤 篦 篩 篪 篭 篮 篱 篷 篾 簀 簃 簇 簋 簑 簒 簕 簗 簡 簧 簪 簫 簷 簸 簽 簾 簿 籀 籁 籃 籌 籍 籐 籓 籔 籙 籟 籠 籤 籬 籲 米 类 籽 籾 粁 粂 粄 粉 粋 粍 粑 粒 粕 粗 粘 粛 粟 粤 粥 粧 粪 粮 粱 粲 粳 粵 粹 粽 精 粿 糀 糅 糊 糍 糎 糕 糖 糗 糙 糜 糞 糟 糠 糧 糬 糯 糰 糸 糹 糺 系 糾 紀 紂 約 紅 紆 紇 紉 紊 紋 納 紐 紓 純 紗 紘 紙 級 紛 紜 素 紡 索 紧 紫 紬 紮 累 細 紳 紹 紺 終 絃 組 絅 絆 経 結 絕 絜 絞 絡 絢 絣 給 絨 絮 統 絲 絳 絵 絶 絹 綁 綃 綉 綏 綑 經 継 続 綜 綝 綠 綢 綦 綫 綬 維 綰 綱 網 綴 綵 綸 綺 綻 綽 綾 綿 緊 緋 総 緑 緒 緖 緘 線 緝 緞 締 緡 緣 編 緩 緬 緯 緲 練 緹 緻 縁 縄 縈 縉 縊 縛 縝 縞 縣 縦 縫 縮 縯 縱 縷 縹 縻 總 績 繁 繃 繆 繇 繊 繋 繍 繒 織 繕 繙 繚 繞 繡 繩 繪 繫 繭 繰 繳 繹 繼 繽 纂 纈 續 纍 纏 纓 纖 纘 纛 纜 纠 红 纣 纤 纥 约 级 纪 纫 纬 纭 纮 纯 纱 纲 纳 纵 纶 纷 纸 纹 纺 纽 纾 线 绀 绂 练 组 绅 细 织 终 绊 绍 绎 经 绑 绒 结 绕 绘 给 绚 绛 络 绝 绞 统 绡 绢 绣 绥 继 绩 绪 绫 续 绮 绯 绰 绳 维 绵 绶 绷 绸 综 绽 绾 绿 缀 缄 缅 缆 缇 缉 缎 缓 缔 缕 编 缘 缙 缚 缜 缝 缟 缠 缢 缤 缨 缩 缪 缬 缭 缮 缯 缴 缵 缶 缸 缺 缽 罂 罄 罅 罈 罌 罐 网 罔 罕 罗 罘 罚 罟 罠 罡 罢 罩 罪 置 罰 署 罵 罷 罹 罽 羁 羅 羆 羈 羊 羋 羌 美 羔 羚 羞 羟 羡 羣 群 羥 羧 羨 義 羯 羰 羲 羸 羹 羽 羿 翀 翁 翅 翊 翌 翎 習 翔 翕 翘 翟 翠 翡 翥 翦 翩 翫 翮 翰 翱 翳 翹 翻 翼 耀 老 考 者 耆 而 耍 耐 耒 耕 耗 耘 耙 耜 耦 耨 耳 耶 耸 耻 耽 耿 聂 聃 聆 聊 聋 职 联 聖 聘 聚 聞 聡 聪 聯 聰 聲 聳 聴 聶 職 聽 聾 聿 肃 肄 肅 肆 肇 肉 肋 肌 肓 肖 肘 肚 肛 肜 肝 肟 肠 股 肢 肤 肥 肩 肪 肮 肯 肱 育 肴 肺 肼 肽 肾 肿 胀 胁 胃 胄 胆 背 胍 胎 胖 胚 胛 胜 胝 胞 胡 胤 胥 胧 胪 胫 胭 胯 胰 胱 胳 胴 胶 胸 胺 胼 能 脂 脅 脆 脇 脈 脉 脊 脍 脏 脐 脑 脓 脖 脚 脛 脣 脩 脫 脯 脱 脲 脳 脷 脸 脹 脾 腆 腈 腊 腋 腌 腎 腐 腑 腓 腔 腕 腥 腦 腩 腫 腭 腮 腰 腱 腳 腴 腸 腹 腺 腻 腾 腿 膀 膂 膈 膊 膏 膑 膚 膛 膜 膝 膠 膣 膦 膨 膩 膳 膵 膺 膽 膾 膿 臀 臂 臆 臈 臉 臍 臏 臓 臘 臚 臟 臣 臥 臧 臨 自 臬 臭 至 致 臺 臻 臼 臾 舁 舂 舅 舆 與 興 舉 舊 舌 舍 舎 舐 舒 舔 舖 舗 舘 舛 舜 舞 舟 舢 舩 航 舫 般 舯 舰 舱 舳 舵 舶 舷 舸 船 舺 舾 艀 艇 艉 艋 艏 艘 艙 艛 艤 艦 艮 良 艰 艱 色 艳 艶 艷 艸 艹 艺 艾 节 芃 芈 芊 芋 芍 芎 芒 芗 芙 芜 芝 芡 芥 芦 芨 芩 芪 芫 芬 芭 芮 芯 花 芳 芷 芸 芹 芻 芽 芾 苄 苅 苇 苋 苌 苍 苎 苏 苑 苓 苔 苕 苗 苛 苜 苞 苟 苡 苣 若 苦 苧 苫 苯 英 苳 苴 苷 苹 苺 苻 苾 茁 茂 范 茄 茅 茉 茌 茎 茔 茗 茛 茜 茧 茨 茫 茯 茱 茲 茴 茵 茶 茸 茹 荀 荃 荆 草 荊 荏 荐 荒 荔 荖 荘 荚 荞 荟 荠 荡 荣 荤 荥 荧 荨 荩 荪 荫 药 荳 荷 荸 荻 荼 荽 莅 莆 莉 莊 莎 莒 莓 莖 莘 莞 莠 莢 莧 莨 莪 莫 莱 莲 莳 莴 获 莹 莺 莽 莿 菀 菁 菅 菇 菈 菊 菌 菏 菓 菖 菘 菜 菝 菟 菠 菡 菩 菫 華 菰 菱 菲 菴 菸 菽 萁 萃 萄 萇 萊 萌 萍 萎 萘 萜 萝 萠 萣 萤 营 萦 萧 萨 萩 萬 萱 萵 萸 萼 落 葆 葉 葎 著 葛 葜 葡 董 葦 葩 葫 葬 葭 葯 葱 葳 葵 葶 葷 葺 蒂 蒋 蒐 蒔 蒙 蒜 蒞 蒟 蒡 蒨 蒯 蒲 蒴 蒸 蒺 蒻 蒼 蒽 蒾 蒿 蓀 蓁 蓄 蓆 蓉 蓋 蓍 蓑 蓓 蓖 蓝 蓟 蓣 蓬 蓮 蓼 蓿 蔀 蔑 蔓 蔔 蔗 蔘 蔚 蔡 蔣 蔥 蔦 蔬 蔭 蔴 蔵 蔷 蔺 蔻 蔼 蔽 蕁 蕃 蕈 蕉 蕊 蕎 蕗 蕙 蕤 蕨 蕩 蕪 蕭 蕲 蕴 蕷 蕾 薀 薄 薇 薈 薊 薌 薏 薑 薔 薗 薙 薛 薜 薦 薨 薩 薪 薫 薬 薮 薯 薰 薷 薹 薺 藁 藉 藍 藎 藏 藐 藓 藔 藕 藜 藝 藤 藥 藨 藩 藪 藷 藹 藺 藻 藿 蘂 蘄 蘅 蘆 蘇 蘊 蘋 蘑 蘚 蘭 蘸 蘿 虎 虏 虐 虑 虓 虔 處 虚 虛 虜 虞 號 虢 虧 虫 虬 虯 虱 虹 虻 虽 虾 蚀 蚁 蚂 蚊 蚋 蚌 蚓 蚕 蚜 蚝 蚣 蚤 蚩 蚪 蚬 蚯 蚱 蚵 蚶 蚺 蛀 蛄 蛆 蛇 蛉 蛊 蛋 蛍 蛎 蛏 蛔 蛙 蛛 蛞 蛟 蛤 蛭 蛮 蛯 蛰 蛱 蛳 蛸 蛹 蛺 蛻 蛾 蜀 蜂 蜃 蜆 蜈 蜉 蜊 蜍 蜑 蜒 蜓 蜕 蜗 蜘 蜚 蜜 蜡 蜢 蜥 蜱 蜴 蜷 蜻 蜿 蝇 蝉 蝋 蝌 蝎 蝓 蝕 蝗 蝙 蝟 蝠 蝣 蝦 蝨 蝮 蝰 蝴 蝶 蝸 蝽 蝾 蝿 螂 螃 螄 螈 融 螞 螟 螢 螨 螫 螭 螯 螳 螺 螽 蟀 蟄 蟆 蟇 蟋 蟎 蟑 蟒 蟜 蟠 蟬 蟲 蟷 蟹 蟻 蟾 蠅 蠊 蠍 蠑 蠔 蠕 蠟 蠡 蠢 蠣 蠱 蠲 蠵 蠶 蠹 蠻 血 衅 衆 衊 行 衍 衒 術 衔 衕 街 衙 衚 衛 衝 衞 衡 衢 衣 补 表 衫 衬 衮 衰 衷 衹 衽 衾 衿 袁 袂 袄 袈 袋 袍 袒 袖 袛 袜 袞 袢 袤 被 袭 袱 袴 裁 裂 装 裏 裒 裔 裕 裘 裙 補 裝 裟 裡 裤 裨 裱 裳 裴 裵 裸 裹 製 裾 褂 複 褌 褐 褒 褓 褔 褚 褥 褪 褫 褲 褶 褻 襁 襄 襖 襞 襟 襦 襪 襯 襲 襷 西 要 覃 覆 覇 見 規 覓 視 覗 覚 覦 覧 親 覬 覲 観 覺 覽 觀 见 观 规 觅 视 览 觉 觊 觎 觐 角 觚 觞 解 触 觴 觸 言 訁 訂 訃 訇 計 訊 訌 討 訓 訕 訖 託 記 訛 訝 訟 訢 訣 訥 訪 設 許 訳 訴 訶 診 註 証 訾 詁 詆 詈 詐 詒 詔 評 詛 詞 詠 詡 詢 詣 試 詧 詩 詫 詬 詭 詮 詰 話 該 詳 詵 詹 詼 誅 誇 誉 誊 誌 認 誑 誓 誕 誘 語 誠 誡 誣 誤 誥 誦 誨 說 説 読 誰 課 誹 誼 誾 調 諂 諄 談 請 諌 諍 諏 諒 論 諛 諜 諡 諤 諦 諧 諫 諭 諮 諱 諲 諳 諴 諶 諷 諸 諺 諾 謀 謁 謂 謄 謇 謊 謎 謐 謔 謖 謗 謙 謚 講 謝 謠 謡 謨 謫 謬 謳 謹 謾 譁 證 譏 識 譙 譚 譜 警 譬 譯 議 譲 譴 護 譽 讀 讃 變 讎 讐 讒 讓 讖 讚 讞 计 订 讣 认 讥 讦 讧 讨 让 讪 讫 训 议 讯 记 讲 讳 讴 讶 讷 许 讹 论 讼 讽 设 访 诀 证 诂 诃 评 诅 识 诈 诉 诊 诋 词 诏 译 诒 试 诗 诘 诙 诚 诛 诜 话 诞 诟 诠 诡 询 诣 诤 该 详 诧 诩 诫 诬 语 误 诰 诱 诲 说 诵 请 诸 诹 诺 读 诽 课 谀 谁 调 谄 谅 谈 谊 谋 谌 谍 谎 谏 谐 谑 谒 谓 谔 谕 谗 谘 谙 谚 谛 谜 谟 谢 谣 谤 谥 谦 谧 谨 谩 谪 谬 谭 谯 谱 谳 谴 谶 谷 谿 豁 豆 豈 豉 豊 豌 豎 豐 豔 豕 豚 象 豢 豨 豪 豫 豬 豳 豸 豹 豺 貂 貉 貊 貌 貓 貘 貝 貞 負 財 貢 貧 貨 販 貪 貫 責 貯 貰 貳 貴 貶 買 貸 費 貼 貽 貿 賀 賁 賂 賃 賄 資 賈 賊 賎 賑 賓 賚 賛 賜 賞 賠 賡 賢 賣 賤 賦 質 賬 賭 賴 賺 購 賽 賾 贄 贅 贇 贈 贊 贋 贍 贏 贓 贔 贖 贛 贝 贞 负 贡 财 责 贤 败 账 货 质 贩 贪 贫 贬 购 贮 贯 贰 贱 贲 贴 贵 贷 贸 费 贺 贻 贼 贽 贾 贿 赀 赁 赂 赃 资 赈 赉 赋 赌 赍 赎 赏 赐 赓 赔 赖 赘 赚 赛 赝 赞 赟 赠 赡 赢 赣 赤 赦 赧 赫 赭 走 赳 赴 赵 赶 起 趁 超 越 趋 趕 趙 趟 趣 趨 足 趴 趸 趺 趾 跃 跆 跋 跌 跏 跑 跖 跗 跛 距 跟 跡 跤 跨 跪 路 跳 践 跷 跸 跻 踊 踏 踐 踝 踞 踢 踩 踪 踰 踴 踵 踹 蹂 蹄 蹇 蹈 蹉 蹊 蹋 蹕 蹙 蹟 蹠 蹤 蹦 蹬 蹲 蹴 蹶 蹺 蹼 躁 躄 躅 躇 躉 躊 躋 躍 躏 躑 躓 躙 躪 身 躬 躯 躰 躲 躺 躾 軀 車 軋 軌 軍 軒 軔 軛 軟 転 軫 軸 軻 軼 軽 軾 較 輅 載 輋 輌 輒 輓 輔 輕 輛 輜 輝 輟 輥 輦 輩 輪 輯 輳 輸 輻 輾 輿 轂 轄 轅 轆 轉 轍 轎 轟 轡 轢 轤 车 轧 轨 轩 轫 转 轭 轮 软 轰 轲 轴 轶 轸 轻 轼 载 轿 辂 较 辄 辅 辆 辇 辈 辉 辊 辍 辎 辐 辑 输 辕 辖 辗 辘 辙 辛 辜 辞 辟 辣 辦 辨 辩 辫 辭 辮 辯 辰 辱 農 辶 边 辺 辻 込 辽 达 辿 迁 迂 迄 迅 过 迈 迎 运 近 返 还 这 进 远 违 连 迟 迢 迤 迥 迦 迨 迩 迪 迫 迭 述 迳 迴 迷 迸 迹 迺 追 退 送 适 逃 逄 逅 逆 选 逊 逋 逍 透 逐 逑 递 逓 途 逕 逖 逗 這 通 逛 逝 逞 速 造 逡 逢 連 逮 逯 週 進 逵 逸 逹 逻 逼 逾 遁 遂 遅 遇 遊 運 遍 過 遏 遐 遑 遒 道 達 違 遗 遘 遙 遜 遞 遠 遡 遣 遥 遨 適 遭 遮 遲 遴 遵 遶 遷 選 遹 遺 遼 遽 避 邀 邁 邂 邃 還 邇 邈 邉 邊 邏 邑 邓 邕 邗 邙 邛 邝 邠 邡 邢 那 邦 邨 邪 邬 邮 邯 邰 邱 邳 邴 邵 邸 邹 邺 邻 邽 邾 郁 郃 郅 郇 郊 郎 郑 郓 郕 郗 郛 郜 郝 郞 郡 郢 郤 郦 郧 部 郫 郭 郯 郴 郵 郷 郸 都 郾 郿 鄂 鄄 鄆 鄉 鄒 鄔 鄕 鄖 鄙 鄚 鄜 鄞 鄠 鄢 鄣 鄧 鄭 鄯 鄰 鄱 鄲 鄴 鄺 酆 酈 酉 酊 酋 酌 配 酎 酐 酒 酔 酗 酚 酝 酞 酢 酣 酥 酩 酪 酬 酮 酯 酰 酱 酵 酶 酷 酸 酿 醃 醇 醉 醋 醌 醍 醐 醒 醗 醚 醛 醜 醞 醣 醤 醪 醫 醬 醮 醯 醴 醸 醺 釀 釁 采 釈 釉 释 釋 里 重 野 量 釐 金 釔 釗 釘 釜 針 釣 釤 釧 釩 釵 釷 鈀 鈉 鈍 鈎 鈐 鈑 鈔 鈕 鈞 鈣 鈦 鈮 鈰 鈴 鈷 鈸 鈹 鈺 鈽 鈾 鈿 鉀 鉄 鉅 鉈 鉉 鉋 鉍 鉏 鉑 鉗 鉚 鉛 鉞 鉢 鉤 鉦 鉬 鉭 鉱 鉴 鉸 鉻 鉾 銀 銃 銅 銑 銓 銕 銖 銘 銚 銛 銜 銣 銥 銦 銨 銫 銬 銭 銮 銳 銷 銹 銻 銼 鋁 鋅 鋆 鋇 鋌 鋏 鋐 鋒 鋤 鋪 鋭 鋯 鋰 鋲 鋳 鋸 鋹 鋼 錄 錆 錐 錒 錕 錘 錚 錠 錡 錢 錦 錨 錫 錬 錮 錯 録 錳 錶 錸 鍇 鍊 鍋 鍍 鍔 鍛 鍬 鍮 鍰 鍱 鍵 鍶 鍺 鍼 鍾 鎂 鎊 鎌 鎏 鎔 鎖 鎗 鎘 鎚 鎝 鎢 鎧 鎬 鎭 鎮 鎰 鎳 鎵 鏃 鏈 鏊 鏐 鏑 鏖 鏗 鏘 鏜 鏞 鏟 鏡 鏢 鏤 鏵 鏽 鐐 鐘 鐙 鐡 鐫 鐮 鐲 鐳 鐵 鐸 鐺 鑄 鑅 鑑 鑒 鑓 鑛 鑠 鑣 鑫 鑭 鑰 鑲 鑼 鑽 鑾 鑿 钇 针 钉 钊 钍 钏 钒 钓 钗 钙 钚 钛 钜 钝 钞 钟 钠 钡 钢 钤 钥 钦 钧 钨 钩 钫 钮 钯 钰 钱 钳 钴 钵 钹 钺 钻 钼 钽 钾 钿 铀 铁 铂 铃 铅 铆 铈 铉 铊 铋 铌 铍 铎 铐 铛 铜 铝 铟 铠 铢 铣 铤 铧 铨 铫 铬 铭 铮 铯 铰 铱 铲 铳 铵 银 铷 铸 铺 铼 链 铿 销 锁 锂 锄 锅 锆 锈 锉 锋 锌 锐 锑 锕 锗 错 锚 锜 锝 锟 锡 锢 锣 锤 锥 锦 锭 键 锯 锰 锴 锵 锶 锷 锹 锺 锻 锽 锾 镀 镁 镂 镇 镉 镌 镍 镐 镑 镒 镓 镕 镖 镗 镛 镜 镝 镠 镧 镫 镭 镯 镰 镳 镶 長 长 門 閂 閃 閆 閉 開 閎 閏 閑 閒 間 閔 閘 閡 関 閣 閤 閥 閨 閩 閬 閭 閱 閲 閹 閻 閼 閾 闆 闇 闈 闊 闋 闌 闍 闐 闓 闔 闕 闖 闘 關 闞 闡 闢 闥 门 闩 闪 闫 闭 问 闯 闰 闱 闲 闳 间 闵 闷 闸 闹 闺 闻 闼 闽 闾 闿 阀 阁 阂 阅 阆 阇 阈 阉 阎 阏 阐 阑 阔 阕 阖 阗 阙 阚 阜 阝 队 阡 阪 阮 阯 阱 防 阳 阴 阵 阶 阻 阿 陀 陂 附 际 陆 陇 陈 陉 陋 陌 降 限 陔 陕 陘 陛 陜 陝 陞 陟 陡 院 陣 除 陥 陨 险 陪 陰 陲 陳 陵 陶 陷 陸 険 陽 隅 隆 隈 隊 隋 隍 階 随 隐 隔 隕 隗 隘 隙 際 障 隠 隣 隧 隨 險 隰 隱 隴 隶 隷 隸 隹 隻 隼 隽 难 雀 雁 雄 雅 集 雇 雉 雋 雌 雍 雎 雏 雑 雒 雕 雖 雙 雛 雜 雞 雠 離 難 雨 雩 雪 雫 雯 雰 雲 雳 零 雷 雹 電 雾 需 霁 霄 霆 震 霈 霉 霊 霍 霎 霏 霑 霓 霖 霜 霞 霧 霭 霰 露 霸 霹 霽 霾 靂 靄 靈 靑 青 靓 靖 静 靚 靛 靜 非 靠 靡 面 革 靭 靱 靳 靴 靶 靺 靼 鞄 鞅 鞆 鞋 鞍 鞏 鞑 鞘 鞞 鞠 鞣 鞦 鞨 鞫 鞬 鞭 鞮 鞴 韃 韆 韋 韌 韓 韜 韞 韦 韧 韩 韫 韬 韭 韮 音 韵 韶 韻 響 頁 頂 頃 項 順 須 頊 頌 預 頑 頒 頓 頗 領 頚 頜 頠 頡 頤 頦 頫 頬 頭 頰 頴 頵 頷 頸 頹 頻 頼 頽 顆 題 額 顎 顏 顒 顓 顔 顕 顗 願 顛 類 顥 顧 顫 顯 顰 顱 顳 页 顶 顷 项 顺 须 顼 顽 顾 顿 颁 颂 预 颅 领 颇 颈 颉 颊 颌 颍 颏 颐 频 颓 颔 颖 颗 题 颙 颚 颛 颜 额 颞 颠 颢 颤 風 颯 颱 颳 颶 飄 飆 飈 风 飒 飓 飘 飙 飚 飛 飜 飞 食 飡 飢 飨 飩 飪 飫 飭 飮 飯 飲 飴 飼 飽 飾 餃 餅 餉 養 餌 餐 餓 餘 餚 餞 餡 館 餮 餵 餾 饅 饉 饋 饌 饑 饒 饕 饗 饥 饪 饬 饭 饮 饯 饰 饱 饲 饵 饶 饷 饺 饼 饿 馀 馅 馆 馈 馏 馑 馒 馔 首 馗 馘 香 馥 馨 馬 馭 馮 馱 馳 馴 駁 駄 駅 駆 駈 駐 駒 駕 駙 駛 駝 駟 駢 駭 駱 駸 駿 騁 騎 騏 騒 験 騙 騨 騫 騭 騮 騰 騶 騷 騾 驃 驅 驊 驍 驒 驕 驗 驚 驛 驟 驢 驤 驥 驩 驪 马 驭 驮 驯 驰 驱 驳 驴 驶 驷 驸 驹 驺 驻 驼 驾 驿 骁 骂 骄 骅 骆 骇 骈 骊 骋 验 骏 骐 骑 骗 骘 骚 骝 骞 骠 骡 骤 骥 骧 骨 骯 骰 骶 骷 骸 骼 髀 髂 髄 髅 髋 髏 髑 髒 髓 體 髖 高 髙 髡 髣 髦 髪 髭 髮 髯 髴 髷 髹 髻 鬃 鬆 鬍 鬓 鬘 鬚 鬟 鬢 鬣 鬥 鬧 鬨 鬪 鬬 鬯 鬱 鬲 鬻 鬼 魁 魂 魃 魄 魅 魇 魉 魋 魍 魎 魏 魑 魔 魘 魚 魟 魣 魨 魮 魯 魴 魷 鮃 鮄 鮈 鮋 鮎 鮑 鮒 鮗 鮟 鮠 鮡 鮨 鮪 鮫 鮭 鮮 鯉 鯊 鯒 鯓 鯔 鯖 鯙 鯛 鯡 鯤 鯧 鯨 鯪 鯰 鯱 鯵 鯷 鯽 鰂 鰈 鰍 鰐 鰓 鰕 鰧 鰨 鰩 鰭 鰯 鰲 鰹 鰺 鰻 鰾 鱂 鱇 鱈 鱉 鱒 鱔 鱗 鱘 鱚 鱟 鱥 鱧 鱨 鱲 鱵 鱷 鱸 鱺 鱼 鱿 鲀 鲁 鲂 鲆 鲇 鲈 鲉 鲍 鲎 鲑 鲔 鲗 鲛 鲜 鲟 鲡 鲣 鲤 鲨 鲫 鲬 鲭 鲮 鲱 鲲 鲳 鲶 鲷 鲸 鲹 鲻 鲽 鲿 鳀 鳃 鳄 鳅 鳉 鳌 鳍 鳎 鳐 鳔 鳕 鳖 鳗 鳚 鳝 鳞 鳟 鳢 鳥 鳧 鳩 鳳 鳴 鳶 鴇 鴈 鴉 鴎 鴒 鴕 鴛 鴝 鴞 鴟 鴣 鴦 鴨 鴫 鴻 鴿 鵄 鵑 鵙 鵜 鵝 鵞 鵟 鵠 鵡 鵬 鵯 鵰 鵲 鵺 鶇 鶉 鶏 鶘 鶚 鶥 鶯 鶲 鶴 鶺 鶻 鶿 鷂 鷄 鷓 鷗 鷯 鷲 鷸 鷹 鷺 鸕 鸚 鸛 鸞 鸟 鸠 鸡 鸢 鸣 鸥 鸦 鸨 鸪 鸫 鸬 鸭 鸮 鸯 鸰 鸲 鸳 鸵 鸽 鸾 鸿 鹀 鹃 鹄 鹅 鹈 鹉 鹊 鹌 鹎 鹏 鹑 鹕 鹗 鹘 鹚 鹛 鹞 鹟 鹡 鹤 鹦 鹧 鹩 鹫 鹬 鹭 鹮 鹰 鹳 鹵 鹸 鹹 鹼 鹽 鹿 麁 麂 麋 麒 麓 麗 麝 麟 麤 麥 麦 麩 麴 麵 麸 麹 麺 麻 麼 麽 麾 麿 黃 黄 黌 黍 黎 黏 黐 黑 黒 黔 默 黙 黛 黜 黝 點 黟 黠 黥 黨 黯 黴 黻 黼 鼈 鼎 鼐 鼓 鼠 鼩 鼬 鼯 鼱 鼴 鼹 鼻 齊 齋 齎 齐 齒 齟 齡 齢 齣 齦 齧 齬 齶 齿 龄 龈 龍 龐 龑 龔 龕 龙 龚 龛 龜 龟 龢 龸 ꧉ 﨑 ﭖ ﭘ ﮏ ﮐ ﮟ ﮧ ﮨ ﮭ ﮯ ﯽ ﯾ ﯿ ﴾ ﴿ ﷲ ﷺ ︰ ﹐ ﹑ ﹒ ﹔ ﹕ ﹙ ﹚ ﹝ ﹞ ﹣ ﺍ ﺎ ﺑ ﺗ ﺟ ﺭ ﺮ ﺱ ﺳ ﻣ ﻦ ﻧ ﻭ ﻮ ！ ＂ ＃ ％ ＆ （ ） ＊ ＋ ， － ． ／ ０ １ ２ ３ ４ ５ ６ ７ ８ ９ ： ； ＜ ＝ ＞ ？ ＠ ［ ＼ ］ ＿ ａ ｂ ｃ ｄ ｅ ｆ ｇ ｈ ｉ ｊ ｋ ｌ ｍ ｎ ｏ ｐ ｒ ｓ ｔ ｖ ｗ ｘ ｚ ｛ ｜ ｝ ～ ｡ ｢ ｣ ､ ･ ｰ ￥ ￼ 𩽾 𩾌 𪨶 𫖮 𫚉 𫚒 𬀩 𬘭 𬞟 𬶋 . print(&#39;Are the two sets identical?&#39;, set(one_chars) == set(tokens)) . Are the two sets identical? True . Subwords vs. Whole-words . Let&#39;s gather some statistics on the vocabulary. . ! pip install seaborn -q . import matplotlib.pyplot as plt import seaborn as sns import numpy as np sns.set(style=&#39;darkgrid&#39;) # Increase the plot size and font size. sns.set(font_scale=1.5) plt.rcParams[&quot;figure.figsize&quot;] = (10,5) # Measure the length of every token in the vocab. token_lengths = [len(token) for token in tokenizer.vocab.keys()] # Plot the number of tokens of each length. sns.countplot(token_lengths) plt.title(&#39;Vocab Token Lengths&#39;) plt.xlabel(&#39;Token Length&#39;) plt.ylabel(&#39;# of Tokens&#39;) print(&#39;Maximum token length:&#39;, max(token_lengths)) . Maximum token length: 22 . Let&#39;s look at just the tokens which begin with &#39;##&#39;. . num_subwords = 0 subword_lengths = [] # For each token in the vocabulary, for token in tokenizer.vocab.keys(): # If it&#39;s a subword, if len(token) &gt;=2 and token[0:2] == &#39;##&#39;: # Tally all wubwords num_subwords += 1 # Measure the subword length (without the hashes) length = len(token) - 2 # Record the lengths. subword_lengths.append(length) . How many &#39;##&#39; tokens are there vs. the full vocab? . vocab_size = len(tokenizer.vocab.keys()) print(&#39;Number of subwords: {:,} of {:,}&#39;.format(num_subwords, vocab_size)) # Calculate the percentage of words that are &#39;##&#39; subwords. prcnt = float(num_subwords) / vocab_size * 100.0 print(&#39;%.1f%%&#39; % prcnt) . Number of subwords: 36,436 of 105,879 34.4% . Plot the subword lengths (not including the two &#39;##&#39; characters). . sns.countplot(subword_lengths) plt.title(&#39;Subword Token Lengths (w/o &quot;##&quot;)&#39;) plt.xlabel(&#39;Subword Length&#39;) plt.ylabel(&#39;# of ## Subwords&#39;) . Text(0,0.5,&#39;# of ## Subwords&#39;) . Misspellings . &#39;misspelled&#39; in tokenizer.vocab # Right . False . &#39;mispelled&#39; in tokenizer.vocab # Wrong . False . &#39;government&#39; in tokenizer.vocab # Right . True . &#39;goverment&#39; in tokenizer.vocab # Wrong . False . &#39;beginning&#39; in tokenizer.vocab # Right . True . &#39;begining&#39; in tokenizer.vocab # Wrong . False . &#39;separate&#39; in tokenizer.vocab # Right . True . &#39;seperate&#39; in tokenizer.vocab # Wrong . False . What about contractions? . &quot;can&#39;t&quot; in tokenizer.vocab . False . &quot;cant&quot; in tokenizer.vocab . True . Start vs. Mid Subwords . For single characters, there are both the individual character and the &#39;##&#39; version for every character. Is the same true of subwords? . # For each token in the vocabulary, for token in tokenizer.vocab.keys(): # If it&#39;s a subword, if len(token) &gt;= 2 and token[0:2] == &#39;##&#39;: if not token[2:] in tokenizer.vocab: print(&#39;Did not find a token for&#39;, token[2:]) break . Did not find a token for ом . &#39;##ом&#39; in tokenizer.vocab . True . &#39;ом&#39; in tokenizer.vocab . False . Names . !pip install wget -q . import wget import random print(&#39;Beginning file download with wget module&#39;) url = &#39;http://www.gutenberg.org/files/3201/files/NAMES.TXT&#39; wget.download(url, &#39;first-names.txt&#39;) . Beginning file download with wget module . &#39;first-names (1).txt&#39; . # Read them in. with open(&#39;first-names.txt&#39;, &#39;rb&#39;) as f: names_encoded = f.readlines() names = [] # Decode the names, convert to lowercase, and strip newlines. for name in names_encoded: try: names.append(name.rstrip().lower().decode(&#39;utf-8&#39;)) except: continue print(&#39;Number of nmaes: {:,}&#39;.format(len(names))) print(&#39;Example:&#39;, random.choice(names)) . Number of nmaes: 21,985 Example: geis . num_names = 0 # For each name in our list, for name in names: # If it&#39;s in the vocab, if name in tokenizer.vocab: # Tally it. num_names += 1 print(&#39;{:,} names in the vocabulary&#39;.format(num_names)) . 4,493 names in the vocabulary . Numbers . # Count how many numbers are in the vocabulary. count = 0 # For each token in the vocabulary, for token in tokenizer.vocab.keys(): # Tally if it&#39;s a number. if token.isdigit(): count += 1 # Any numbers &gt;= 10,000? if len(token) &gt; 4: print(token) print(&#39;Vocab includes {:,} numbers.&#39;.format(count)) . 092917 511228 87237 0815752199 1403962588 1884964311 1579583377 0415285569 0821419161 0691008000 0136120512 0062700553 0195082095 0684805332 0691152071 0816071365 300000 933346 0521820486 1852337520 10000 ६३८५९६ 000291 १६८२६ 84433 9780521872386 9789004244870 00238 89800 12148 99947 Vocab includes 2,278 numbers. . # Count how many dates between 1600 and 2021 are included. count = 0 for i in range(1600, 2021): if str(i) in tokenizer.vocab: count += 1 print(&#39;Vocab includes {:,} of 421 dates from 1600 - 2021&#39;.format(count)) . Vocab includes 421 of 421 dates from 1600 - 2021 .",
            "url": "https://sparklingness.github.io/blog/bert/jupyter/2020/04/18/Inspect-Bert-Vocabulary.html",
            "relUrl": "/bert/jupyter/2020/04/18/Inspect-Bert-Vocabulary.html",
            "date": " • Apr 18, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Bert Notebook Blog Post(2)",
            "content": "ref: http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/ . 1. Loading Pre-Trained BERT . ! pip install pytorch-pretrained-bert . Collecting pytorch-pretrained-bert Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB) |████████████████████████████████| 133kB 2.8MB/s Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.2) Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.12.38) Requirement already satisfied: torch&gt;=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0) Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.38.0) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0) Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;pytorch-pretrained-bert) (0.9.5) Requirement already satisfied: botocore&lt;1.16.0,&gt;=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;pytorch-pretrained-bert) (1.15.38) Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;pytorch-pretrained-bert) (0.3.3) Requirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;pytorch-pretrained-bert) (1.24.3) Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;pytorch-pretrained-bert) (2.8) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;pytorch-pretrained-bert) (2020.4.5.1) Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;pytorch-pretrained-bert) (3.0.4) Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore&lt;1.16.0,&gt;=1.15.38-&gt;boto3-&gt;pytorch-pretrained-bert) (2.8.1) Requirement already satisfied: docutils&lt;0.16,&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore&lt;1.16.0,&gt;=1.15.38-&gt;boto3-&gt;pytorch-pretrained-bert) (0.15.2) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.16.0,&gt;=1.15.38-&gt;boto3-&gt;pytorch-pretrained-bert) (1.12.0) Installing collected packages: pytorch-pretrained-bert Successfully installed pytorch-pretrained-bert-0.6.2 . import torch from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM # OPTIONAL: if you want to have more information on what&#39;s happening, activate the logger as follows import logging # logging.basicConfig(level=logging.INFO) import matplotlib.pyplot as plt % matplotlib inline # Load pre-trained model tokenizer (vocabulary) tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;) . 100%|██████████| 231508/231508 [00:00&lt;00:00, 315280.35B/s] . 2. Input Formatting . 2.1. Special Tokens . Special tokens to mark the beginning ([CLS]) and seprataion/end of sentences ([SEP]) . BERT can take as input either one or two sentences, and expects special tokens to mark the beginning and end of each one: . 2 Sentence Input: . [CLS] The man wnent to the store. [SEP] He bought a gallon of milk. [SEP] . 1 Sentence Input: . [CLS] The man went to the store. [SEP] . 2.2. Tokenization . text = &quot;Here is the sentence I want embeddings for.&quot; marked_text = &quot;[CLS] &quot; + text + &quot; [SEP]&quot; # Tokenize our sentence with the BERT tokenizer. tokenized_text = tokenizer.tokenize(marked_text) # Print out the tokens. print(tokenized_text) . [&#39;[CLS]&#39;, &#39;here&#39;, &#39;is&#39;, &#39;the&#39;, &#39;sentence&#39;, &#39;i&#39;, &#39;want&#39;, &#39;em&#39;, &#39;##bed&#39;, &#39;##ding&#39;, &#39;##s&#39;, &#39;for&#39;, &#39;.&#39;, &#39;[SEP]&#39;] . # Define a new example sentence with multiple meanings of the word &quot;bank&quot; text = &quot;After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.&quot; # Add the special tokens. marked_text = &quot;[CLS] &quot; + text + &quot; [SEP]&quot; # Split the sentence into tokens. tokenized_text = tokenizer.tokenize(marked_text) # Map the token strings to their vocabulary indeces. indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) # Display the words with their indeces. for tup in zip(tokenized_text, indexed_tokens): print(&#39;{:&lt;12} {:&gt;6,}&#39;.format(tup[0], tup[1])) . [CLS] 101 after 2,044 stealing 11,065 money 2,769 from 2,013 the 1,996 bank 2,924 vault 11,632 , 1,010 the 1,996 bank 2,924 robber 27,307 was 2,001 seen 2,464 fishing 5,645 on 2,006 the 1,996 mississippi 5,900 river 2,314 bank 2,924 . 1,012 [SEP] 102 . 2.3. Segment ID . # Mark each of the 22 tokens as belonging to sentence &quot;1&quot;. segments_ids = [1] * len(tokenized_text) print(segments_ids) . [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] . 3. Extracting Embeddings . 3.1. Running BERT on our text . model.eval() evaluation mode turns off dropout regularization which is used in training . # Convert inputs to PyTorch tensors tokens_tensor = torch.tensor([indexed_tokens]) print(tokens_tensor) segments_tensors = torch.tensor([segments_ids]) # Load pre-trained model (weights) model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;) # Put the model in &quot;evaluation&quot; mode, meaning feed-forward operation. model.eval() . tensor([[ 101, 2044, 11065, 2769, 2013, 1996, 2924, 11632, 1010, 1996, 2924, 27307, 2001, 2464, 5645, 2006, 1996, 5900, 2314, 2924, 1012, 102]]) . 100%|██████████| 407873900/407873900 [00:35&lt;00:00, 11382845.79B/s] . BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): BertLayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) . torch.no_grad deactivates the gradient calculations, saves memory, and speeds up computation. (we don&#39;t need gradients or backpropagation since we&#39;re just running a forward pass). . # Predict hidden states features for each layer with torch.no_grad(): encoded_layers, _ = model(tokens_tensor, segments_tensors) . 3.2. Understanding the Output . print(&quot;Number of layers:&quot;, len(encoded_layers)) layer_i = 0 print(&quot;Number of batches:&quot;, len(encoded_layers[layer_i])) batch_i = 0 print(&quot;Number of tokens:&quot;, len(encoded_layers[layer_i][batch_i])) token_i = 0 print(&quot;Number of hidden units:&quot;, len(encoded_layers[layer_i][batch_i][token_i])) . Number of layers: 12 Number of batches: 1 Number of tokens: 22 Number of hidden units: 768 . # For the 5th token in our sentence, select its feature values from layer 5. token_i = 5 layer_i = 5 vec = encoded_layers[layer_i][batch_i][token_i] # Plot the values as a histogram to show their distribution. plt.figure(figsize=(10,10)) plt.hist(vec, bins=200) plt.show() . # `encoded_layers` is a Python list. print(&#39; Type of encoded_layers: &#39;, type(encoded_layers), len(encoded_layers)) # Each layer in the list is a torch tensor. print(&#39;Tensor shape for each layer: &#39;, encoded_layers[0].size()) . Type of encoded_layers: &lt;class &#39;list&#39;&gt; 12 Tensor shape for each layer: torch.Size([1, 22, 768]) . # Concatenate the tensors for all layers. We use `stack` here to # create a new dimension in the tensor. token_embeddings = torch.stack(encoded_layers, dim=0) token_embeddings.size() . torch.Size([12, 1, 22, 768]) . # Remove dimension 1, the &quot;batches&quot;. token_embeddings = torch.squeeze(token_embeddings, dim=1) token_embeddings.size() . torch.Size([12, 22, 768]) . # Swap dimensions 0 and 1. token_embeddings = token_embeddings.permute(1, 0, 2) token_embeddings.size() . torch.Size([22, 12, 768]) . 3.3. Creating word and sentence vectors from hidden states . Word Vectors . # Stores the token vectors, with shape [22 x 3,072] (4 x 768 = 3,072) token_vecs_cat = [] # `token_embeddings` is a [22 x 12 x 768] tensor. # For each token in the sentence... for token in token_embeddings: # `token` is a [12 x 768] tensor # Concatenate the vectors (that is, append them together) from the last four layers. # Each layer vector is 768 values, so `cat_vec` is length 3,072. cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0) # Use `cat_vec` to represent `token`. token_vecs_cat.append(cat_vec) print(&#39;Shape is: %d x %d&#39; % (len(token_vecs_cat), len(token_vecs_cat[0]))) . Shape is: 22 x 3072 . # Stores the token vectors, with shape [22 x 768] token_vecs_sum = [] # `token_embeddings` is a [22 x 12 x 768] tensor. # For each token in the sentence... for token in token_embeddings: # `token` is a [12 x 768] tensor # Sum the vectors from the last four layers. sum_vec = torch.sum(token[-4:], dim=0) # Use `sum_vec` to represent `token`. token_vecs_sum.append(sum_vec) print(&#39;Shape is: %d x %d&#39; % (len(token_vecs_sum), len(token_vecs_sum[0]))) . Shape is: 22 x 768 . Sentence Vectors . # `encoded_layers` has shape [12 x 1 x 22 x 768] # `token_vecs` is a tensor with shape [22 x 768] token_vecs = encoded_layers[11][0] # Calculate the average of all 22 token vectors. sentence_embedding = torch.mean(token_vecs, dim=0) . print(&quot;Our final sentence embedding vector of shape:&quot;, sentence_embedding.size()) . Our final sentence embedding vector of shape: torch.Size([768]) . 3.4. Confirming contextually dependent vectors . for i, token_str in enumerate(tokenized_text): print(i, token_str) . 0 [CLS] 1 after 2 stealing 3 money 4 from 5 the 6 bank 7 vault 8 , 9 the 10 bank 11 robber 12 was 13 seen 14 fishing 15 on 16 the 17 mississippi 18 river 19 bank 20 . 21 [SEP] . print(&#39;First 5 vector values for each instance of &quot;bank&quot;.&#39;) print(&#39;&#39;) print(&quot;bank vault &quot;, str(token_vecs_sum[6][:5])) print(&quot;bank robber &quot;, str(token_vecs_sum[10][:5])) print(&quot;river bank &quot;, str(token_vecs_sum[19][:5])) . First 5 vector values for each instance of &#34;bank&#34;. bank vault tensor([ 2.1319, -2.1413, -1.6260, 0.8638, 3.3173]) bank robber tensor([ 1.1868, -1.5298, -1.3770, 1.0648, 3.1446]) river bank tensor([ 1.1295, -1.4725, -0.7296, -0.0901, 2.4970]) . from scipy.spatial.distance import cosine # Calculate the cosine similarity between the word bank # in &quot;bank robber&quot; vs &quot;river bank&quot; (different meanings). diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19]) # Calculate the cosine similarity between the word bank # in &quot;bank robber&quot; vs &quot;bank vault&quot; (same meaning). same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6]) print(&#39;Vector similarity for *similar* meanings: %.2f&#39; % same_bank) print(&#39;Vector similarity for *different* meanings: %.2f&#39; % diff_bank) . Vector similarity for *similar* meanings: 0.95 Vector similarity for *different* meanings: 0.68 .",
            "url": "https://sparklingness.github.io/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html",
            "relUrl": "/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html",
            "date": " • Apr 11, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sparklingness.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}