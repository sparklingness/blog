<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>BERT Fine-Tuning Sentence Classification | wAIz NLP</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="BERT Fine-Tuning Sentence Classification" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="BERT Fine-Tuning Tutorial with PyTorch" />
<meta property="og:description" content="BERT Fine-Tuning Tutorial with PyTorch" />
<link rel="canonical" href="https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification.html" />
<meta property="og:url" content="https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification.html" />
<meta property="og:site_name" content="wAIz NLP" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-19T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"BERT Fine-Tuning Tutorial with PyTorch","@type":"BlogPosting","headline":"BERT Fine-Tuning Sentence Classification","dateModified":"2020-04-19T00:00:00-05:00","datePublished":"2020-04-19T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification.html"},"url":"https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://sparklingness.github.io/blog/feed.xml" title="wAIz NLP" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>BERT Fine-Tuning Sentence Classification | wAIz NLP</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="BERT Fine-Tuning Sentence Classification" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="BERT Fine-Tuning Tutorial with PyTorch" />
<meta property="og:description" content="BERT Fine-Tuning Tutorial with PyTorch" />
<link rel="canonical" href="https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification.html" />
<meta property="og:url" content="https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification.html" />
<meta property="og:site_name" content="wAIz NLP" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-19T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"BERT Fine-Tuning Tutorial with PyTorch","@type":"BlogPosting","headline":"BERT Fine-Tuning Sentence Classification","dateModified":"2020-04-19T00:00:00-05:00","datePublished":"2020-04-19T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification.html"},"url":"https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://sparklingness.github.io/blog/feed.xml" title="wAIz NLP" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">wAIz NLP</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">BERT Fine-Tuning Sentence Classification</h1><p class="page-description">BERT Fine-Tuning Tutorial with PyTorch</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-19T00:00:00-05:00" itemprop="datePublished">
        Apr 19, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#bert">bert</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/sparklingness/blog/tree/master/_notebooks/2020-04-19-BERT_Fine-Tuning_Sentence_Classification.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/sparklingness/blog/master?filepath=_notebooks%2F2020-04-19-BERT_Fine-Tuning_Sentence_Classification.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/sparklingness/blog/blob/master/_notebooks/2020-04-19-BERT_Fine-Tuning_Sentence_Classification.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#1.-Setup">1. Setup </a>
<ul>
<li class="toc-entry toc-h2"><a href="#1.1.-Using-Colab-GPU-for-Training">1.1. Using Colab GPU for Training </a></li>
<li class="toc-entry toc-h2"><a href="#1.2.-Installing-the-Hugging-Face-Library">1.2. Installing the Hugging Face Library </a></li>
<li class="toc-entry toc-h2"><a href="#2.2.-Parse">2.2. Parse </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#3.-Tokenization-&-Input-Formatting">3. Tokenization &amp; Input Formatting </a>
<ul>
<li class="toc-entry toc-h2"><a href="#3.1.-BERT-Tokenizer">3.1. BERT Tokenizer </a></li>
<li class="toc-entry toc-h2"><a href="#3.2.-Required-Formatting">3.2. Required Formatting </a></li>
<li class="toc-entry toc-h2"><a href="#3.3.-Tokenize-Dataset">3.3. Tokenize Dataset </a></li>
<li class="toc-entry toc-h2"><a href="#3.4.-Training-&-Validation-Split">3.4. Training &amp; Validation Split </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-19-BERT_Fine-Tuning_Sentence_Classification.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="1.-Setup">
<a class="anchor" href="#1.-Setup" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Setup<a class="anchor-link" href="#1.-Setup"> </a>
</h1>
<h2 id="1.1.-Using-Colab-GPU-for-Training">
<a class="anchor" href="#1.1.-Using-Colab-GPU-for-Training" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.1. Using Colab GPU for Training<a class="anchor-link" href="#1.1.-Using-Colab-GPU-for-Training"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Get the GPU device name.</span>
<span class="n">device_name</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">gpu_device_name</span><span class="p">()</span>

<span class="c1"># The device name should look like the following:</span>
<span class="k">if</span> <span class="n">device_name</span> <span class="o">==</span> <span class="s1">'/device:GPU:0'</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Found GPU at: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">device_name</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">SystemError</span><span class="p">(</span><span class="s1">'GPU device not found'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Found GPU at: /device:GPU:0
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># If there's a GPU available,</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>

    <span class="c1"># Tell PyTorch to user the GPU.</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'There are </span><span class="si">%d</span><span class="s1"> GPU(s) available.'</span> <span class="o">%</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">())</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'We will user the GPU:'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># If not,</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'No GPU available, using the CPU instead.'</span><span class="p">)</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>There are 1 GPU(s) available.
We will user the GPU: Tesla K80
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.2.-Installing-the-Hugging-Face-Library">
<a class="anchor" href="#1.2.-Installing-the-Hugging-Face-Library" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.2. Installing the Hugging Face Library<a class="anchor-link" href="#1.2.-Installing-the-Hugging-Face-Library"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span> pip install transformers -q
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span> pip install wget -q
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">wget</span> 
<span class="kn">import</span> <span class="nn">os</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Downloading dataset...'</span><span class="p">)</span>

<span class="c1"># The URL for the dataset zip file.</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'</span>

<span class="c1"># Download the file (if we haven't already)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">'./cola_public_1.1.zip'</span><span class="p">):</span>
    <span class="n">wget</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="s1">'./cola_public_1.1.zip'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Downloading dataset...
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Unzip the dataset (if we haven't already)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">'./cola_public/'</span><span class="p">):</span>
    <span class="o">!</span> unzip cola_public_1.1.zip
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span> head cola_public/*/*
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>==&gt; cola_public/raw/in_domain_dev.tsv &lt;==
gj04	1		The sailors rode the breeze clear of the rocks.
gj04	1		The weights made the rope stretch over the pulley.
gj04	1		The mechanical doll wriggled itself loose.
cj99	1		If you had eaten more, you would want less.
cj99	0	*	As you eat the most, you want the least.
cj99	0	*	The more you would want, the less you would eat.
cj99	0	*	I demand that the more John eat, the more he pays.
cj99	1		Mary listens to the Grateful Dead, she gets depressed.
cj99	1		The angrier Mary got, the more she looked at pictures.
cj99	1		The higher the stakes, the lower his expectations are.

==&gt; cola_public/raw/in_domain_train.tsv &lt;==
gj04	1		Our friends won't buy this analysis, let alone the next one we propose.
gj04	1		One more pseudo generalization and I'm giving up.
gj04	1		One more pseudo generalization or I'm giving up.
gj04	1		The more we study verbs, the crazier they get.
gj04	1		Day by day the facts are getting murkier.
gj04	1		I'll fix you a drink.
gj04	1		Fred watered the plants flat.
gj04	1		Bill coughed his way out of the restaurant.
gj04	1		We're dancing the night away.
gj04	1		Herman hammered the metal flat.

==&gt; cola_public/raw/out_of_domain_dev.tsv &lt;==
clc95	1		Somebody just left - guess who.
clc95	1		They claimed they had settled on something, but it wasn't clear what they had settled on.
clc95	1		If Sam was going, Sally would know where.
clc95	1		They're going to serve the guests something, but it's unclear what.
clc95	1		She's reading. I can't imagine what.
clc95	1		John said Joan saw someone from her graduating class.
clc95	0	*	John ate dinner but I don't know who.
clc95	0	*	She mailed John a letter, but I don't know to whom.
clc95	1		I served leek soup to my guests.
clc95	1		I served my guests.

==&gt; cola_public/tokenized/in_domain_dev.tsv &lt;==
gj04	1		the sailors rode the breeze clear of the rocks .
gj04	1		the weights made the rope stretch over the pulley .
gj04	1		the mechanical doll wriggled itself loose .
cj99	1		if you had eaten more , you would want less .
cj99	0	*	as you eat the most , you want the least .
cj99	0	*	the more you would want , the less you would eat .
cj99	0	*	i demand that the more john eat , the more he pays .
cj99	1		mary listens to the grateful dead , she gets depressed .
cj99	1		the angrier mary got , the more she looked at pictures .
cj99	1		the higher the stakes , the lower his expectations are .

==&gt; cola_public/tokenized/in_domain_train.tsv &lt;==
gj04	1		our friends wo n't buy this analysis , let alone the next one we propose .
gj04	1		one more pseudo generalization and i 'm giving up .
gj04	1		one more pseudo generalization or i 'm giving up .
gj04	1		the more we study verbs , the crazier they get .
gj04	1		day by day the facts are getting murkier .
gj04	1		i 'll fix you a drink .
gj04	1		fred watered the plants flat .
gj04	1		bill coughed his way out of the restaurant .
gj04	1		we 're dancing the night away .
gj04	1		herman hammered the metal flat .

==&gt; cola_public/tokenized/out_of_domain_dev.tsv &lt;==
clc95	1		somebody just left - guess who .
clc95	1		they claimed they had settled on something , but it was n't clear what they had settled on .
clc95	1		if sam was going , sally would know where .
clc95	1		they 're going to serve the guests something , but it 's unclear what .
clc95	1		she 's reading . i ca n't imagine what .
clc95	1		john said joan saw someone from her graduating class .
clc95	0	*	john ate dinner but i do n't know who .
clc95	0	*	she mailed john a letter , but i do n't know to whom .
clc95	1		i served leek soup to my guests .
clc95	1		i served my guests .
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.2.-Parse">
<a class="anchor" href="#2.2.-Parse" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2. Parse<a class="anchor-link" href="#2.2.-Parse"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Load the dataset into a pandas dataframe.</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"./cola_public/raw/in_domain_train.tsv"</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s1">'sentence_source'</span><span class="p">,</span> <span class="s1">'label'</span><span class="p">,</span> <span class="s1">'label_notes'</span><span class="p">,</span> <span class="s1">'sentence'</span><span class="p">])</span>

<span class="c1"># Report the number of sentences.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Number of training sentences: </span><span class="si">{:,}</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="c1"># Display 10 random rows from the data</span>
<span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of training sentences: 8,551

</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentence_source</th>
      <th>label</th>
      <th>label_notes</th>
      <th>sentence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1337</th>
      <td>r-67</td>
      <td>1</td>
      <td>NaN</td>
      <td>I want to peruse that contract before filing a...</td>
    </tr>
    <tr>
      <th>1670</th>
      <td>r-67</td>
      <td>1</td>
      <td>NaN</td>
      <td>This hat Tom said Al thought you wanted me to ...</td>
    </tr>
    <tr>
      <th>6952</th>
      <td>m_02</td>
      <td>1</td>
      <td>NaN</td>
      <td>Jane visits Emma.</td>
    </tr>
    <tr>
      <th>449</th>
      <td>bc01</td>
      <td>0</td>
      <td>??</td>
      <td>Who is he reading a book that criticizes?</td>
    </tr>
    <tr>
      <th>4988</th>
      <td>ks08</td>
      <td>1</td>
      <td>NaN</td>
      <td>The fact that scientists have now established ...</td>
    </tr>
    <tr>
      <th>1830</th>
      <td>r-67</td>
      <td>0</td>
      <td>*</td>
      <td>That informers they never use is claimed by th...</td>
    </tr>
    <tr>
      <th>6508</th>
      <td>d_98</td>
      <td>0</td>
      <td>*</td>
      <td>Every cat doesn't like mice, but Felix doesn't.</td>
    </tr>
    <tr>
      <th>8329</th>
      <td>ad03</td>
      <td>1</td>
      <td>NaN</td>
      <td>I asked did Medea poison Jason.</td>
    </tr>
    <tr>
      <th>1358</th>
      <td>r-67</td>
      <td>0</td>
      <td>*</td>
      <td>They will give me a hat that I won't like whic...</td>
    </tr>
    <tr>
      <th>5503</th>
      <td>b_73</td>
      <td>1</td>
      <td>NaN</td>
      <td>Her mother wants Mary to be such an eminent wo...</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)[[</span><span class="s1">'sentence'</span><span class="p">,</span> <span class="s1">'label'</span><span class="p">]]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentence</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>6544</th>
      <td>The table, I put Kim on which supported the book.</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5993</th>
      <td>Has Calvin a bowl?</td>
      <td>0</td>
    </tr>
    <tr>
      <th>379</th>
      <td>How do you wonder who could solve this problem.</td>
      <td>0</td>
    </tr>
    <tr>
      <th>588</th>
      <td>the branch dropped bare of its apple.</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7194</th>
      <td>Your desk before, this girl in the red coat wi...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Get the lists of sentences and their labels.</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sentence</span><span class="o">.</span><span class="n">values</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">values</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="3.-Tokenization-&amp;-Input-Formatting">
<a class="anchor" href="#3.-Tokenization-&amp;-Input-Formatting" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Tokenization &amp; Input Formatting<a class="anchor-link" href="#3.-Tokenization-&amp;-Input-Formatting"> </a>
</h1>
<h2 id="3.1.-BERT-Tokenizer">
<a class="anchor" href="#3.1.-BERT-Tokenizer" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1. BERT Tokenizer<a class="anchor-link" href="#3.1.-BERT-Tokenizer"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span>

<span class="c1"># Load the BERT tokenizer.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Loading BERT tokenizer...'</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-multilingual-uncased'</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Loading BERT tokenizer...
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Print the original sentence.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">' Original: '</span><span class="p">,</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Print the sentence split into tokens.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Tokenized: '</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="c1"># Print the sentence mapped to token ids.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Token IDs: '</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre> Original:  Our friends won't buy this analysis, let alone the next one we propose.
Tokenized:  ['our', 'friends', 'won', "'", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']
Token IDs:  [14008, 16119, 11441, 112, 162, 35172, 10372, 15559, 117, 12421, 19145, 10103, 12878, 10399, 11312, 25690, 119]
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">korean</span> <span class="o">=</span> <span class="s2">"안녕하세요. 반갑습니다. 너는 이름이 뭐니? 오늘 날씨가 맑고 좋구나."</span>
<span class="c1"># Print the original sentence.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">' Original: '</span><span class="p">,</span> <span class="n">korean</span><span class="p">)</span>

<span class="c1"># Print the sentence split into tokens.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Tokenized: '</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">korean</span><span class="p">))</span>

<span class="c1"># Print the sentence mapped to token ids.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Token IDs: '</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">korean</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre> Original:  안녕하세요. 반갑습니다. 너는 이름이 뭐니? 오늘 날씨가 맑고 좋구나.
Tokenized:  ['ᄋ', '##ᅡᆫ', '##녀', '##ᆼ', '##하', '##세', '##요', '.', 'ᄇ', '##ᅡᆫ', '##가', '##ᆸ', '##스', '##ᆸ니다', '.', 'ᄂ', '##ᅥ', '##는', '이', '##름이', 'ᄆ', '##ᅯ', '##니', '?', 'ᄋ', '##ᅩ', '##ᄂ', '##ᅳᆯ', '날', '##씨', '##가', 'ᄆ', '##ᅡ', '##ᆰ', '##고', 'ᄌ', '##ᅩ', '##ᇂ', '##구', '##나', '.']
Token IDs:  [1174, 26646, 49345, 13045, 35132, 25169, 47024, 119, 1170, 26646, 11376, 17360, 13212, 79427, 119, 1165, 33645, 11192, 12398, 89420, 1169, 97090, 25536, 136, 1174, 29347, 97071, 63277, 76818, 47928, 11376, 1169, 25539, 97098, 12300, 1175, 29347, 97109, 16336, 16801, 119]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When we actually convert all of our sentences, we'll use the <code>tokenize.encode</code> functio to handle both steps, rather than calling <code>tokenize</code> and <code>convert_tokens_to_ids</code> seperately.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.2.-Required-Formatting">
<a class="anchor" href="#3.2.-Required-Formatting" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2. Required Formatting<a class="anchor-link" href="#3.2.-Required-Formatting"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.3.-Tokenize-Dataset">
<a class="anchor" href="#3.3.-Tokenize-Dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3. Tokenize Dataset<a class="anchor-link" href="#3.3.-Tokenize-Dataset"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">max_len</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># For every sentence,</span>
<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>

    <span class="c1"># Tokenize the text and add `[CLS]` and `[SEP]` tokens.</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Update the maximum sentence langth.</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Max sentence length: '</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Max sentence length:  48
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Tokenize all of the sentences and map the tokens to their word IDs.</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">attention_masks</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># For every sentence,</span>
<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
    <span class="c1"># `encode_plus` will:</span>
    <span class="c1"># (1) Tokenize the sentence.</span>
    <span class="c1"># (2) Prepend the `[CLS]` token to the start.</span>
    <span class="c1"># (3) Append the `[SEP]` token to the end.</span>
    <span class="c1"># (4) Map tokens to their IDs.</span>
    <span class="c1"># (5) Pad or truncate the sentence to `max_length`</span>
    <span class="c1"># (6) Create attention masks for [PAD] tokens.</span>
    <span class="n">encoded_dict</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span>
        <span class="n">sent</span><span class="p">,</span>                                       <span class="c1"># Sentence to encode.</span>
        <span class="n">add_special_tokens</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>      <span class="c1"># Add '[CLS]' and '[SEP]'</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>                           <span class="c1"># Pad &amp; truncate all sentences.</span>
        <span class="n">pad_to_max_length</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>   <span class="c1"># Construct attn. masks.</span>
        <span class="n">return_Tensors</span> <span class="o">=</span> <span class="s1">'pt'</span><span class="p">,</span>                  <span class="c1"># Return pytorch tensors.</span>
    <span class="p">)</span>
    
    <span class="c1"># Add the encoded sentence to the list.</span>
    <span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoded_dict</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">])</span>

    <span class="c1"># And its attention mask (simply differentiates padding from non-padding.)</span>
    <span class="n">attention_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoded_dict</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">])</span>

<span class="c1"># Convert the lists into tensors.</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">attention_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">attention_masks</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="c1"># Print sentence 0, now as a list of IDs.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Original:  '</span><span class="p">,</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Token IDs: '</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Original:   Our friends won't buy this analysis, let alone the next one we propose.
Token IDs:  tensor([  101., 14008., 16119., 11441.,   112.,   162., 35172., 10372., 15559.,
          117., 12421., 19145., 10103., 12878., 10399., 11312., 25690.,   119.,
          102.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,
            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,
            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,
            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,
            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,
            0.])
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.4.-Training-&amp;-Validation-Split">
<a class="anchor" href="#3.4.-Training-&amp;-Validation-Split" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4. Training &amp; Validation Split<a class="anchor-link" href="#3.4.-Training-&amp;-Validation-Split"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">random_split</span>

<span class="c1"># Combine the training inputs into a TensorDataset.</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_masks</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="c1"># Create a 90-10 train-validation split.</span>

<span class="c1"># Calculate the number of samples to include in each set.</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
<span class="n">val_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">-</span> <span class="n">train_size</span>

<span class="c1"># Devide the dataset by randomly selecting samples.</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">random_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">train_size</span><span class="p">,</span> <span class="n">val_size</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="si">{:&gt;5}</span><span class="s1"> training samples'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_size</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="si">{:&gt;5}</span><span class="s1"> validation samples'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">val_size</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre> 7695 training samples
  856 validation samples
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">RandomSampler</span><span class="p">,</span> <span class="n">SequentialSampler</span>

<span class="c1"># The DataLoader needs to know our batch size for training, so we specify it here.</span>
<span class="c1"># For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1"># Create the DataLoaders for our training and validation sets.</span>
<span class="c1"># We'll take training samples in random order.</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>   <span class="c1"># The training samples.</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">RandomSampler</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">),</span>   <span class="c1"># Select batches randomly</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>   <span class="c1"># Trains with this batch size.</span>
<span class="p">)</span>

<span class="c1"># For validation the order doesn't matter, so we'll just read them sequentially.</span>
<span class="n">validation_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">val_dataset</span><span class="p">,</span>   <span class="c1"># The validation samples.</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">SequentialSampler</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">),</span>   <span class="c1"># Pull out batches sequentially.</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>   <span class="c1"># Evaluate with this batch size.</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="sparklingness/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An Blog for a wise NLP development.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/sparklingness" title="sparklingness"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/-" title="-"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
