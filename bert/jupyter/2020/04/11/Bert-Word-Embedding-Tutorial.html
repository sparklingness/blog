<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Bert Notebook Blog Post(2) | wAIz NLP</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Bert Notebook Blog Post(2)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A tutorial of BERT for Jupyter notebooks." />
<meta property="og:description" content="A tutorial of BERT for Jupyter notebooks." />
<link rel="canonical" href="https://sparklingness.github.io/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html" />
<meta property="og:url" content="https://sparklingness.github.io/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html" />
<meta property="og:site_name" content="wAIz NLP" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-11T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"A tutorial of BERT for Jupyter notebooks.","@type":"BlogPosting","headline":"Bert Notebook Blog Post(2)","dateModified":"2020-04-11T00:00:00-05:00","datePublished":"2020-04-11T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://sparklingness.github.io/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html"},"url":"https://sparklingness.github.io/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://sparklingness.github.io/blog/feed.xml" title="wAIz NLP" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Bert Notebook Blog Post(2) | wAIz NLP</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Bert Notebook Blog Post(2)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A tutorial of BERT for Jupyter notebooks." />
<meta property="og:description" content="A tutorial of BERT for Jupyter notebooks." />
<link rel="canonical" href="https://sparklingness.github.io/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html" />
<meta property="og:url" content="https://sparklingness.github.io/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html" />
<meta property="og:site_name" content="wAIz NLP" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-11T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"A tutorial of BERT for Jupyter notebooks.","@type":"BlogPosting","headline":"Bert Notebook Blog Post(2)","dateModified":"2020-04-11T00:00:00-05:00","datePublished":"2020-04-11T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://sparklingness.github.io/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html"},"url":"https://sparklingness.github.io/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://sparklingness.github.io/blog/feed.xml" title="wAIz NLP" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">wAIz NLP</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Bert Notebook Blog Post(2)</h1><p class="page-description">A tutorial of BERT for Jupyter notebooks.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-11T00:00:00-05:00" itemprop="datePublished">
        Apr 11, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#bert">bert</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/sparklingness/blog/tree/master/_notebooks/2020-04-11-Bert-Word-Embedding-Tutorial.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/sparklingness/blog/master?filepath=_notebooks%2F2020-04-11-Bert-Word-Embedding-Tutorial.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/sparklingness/blog/blob/master/_notebooks/2020-04-11-Bert-Word-Embedding-Tutorial.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#1.-Loading-Pre-Trained-BERT">1. Loading Pre-Trained BERT </a></li>
<li class="toc-entry toc-h2"><a href="#2.-Input-Formatting">2. Input Formatting </a>
<ul>
<li class="toc-entry toc-h3"><a href="#2.1.-Special-Tokens">2.1. Special Tokens </a></li>
<li class="toc-entry toc-h3"><a href="#2.2.-Tokenization">2.2. Tokenization </a></li>
<li class="toc-entry toc-h3"><a href="#2.3.-Segment-ID">2.3. Segment ID </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#3.-Extracting-Embeddings">3. Extracting Embeddings </a>
<ul>
<li class="toc-entry toc-h3"><a href="#3.1.-Running-BERT-on-our-text">3.1. Running BERT on our text </a></li>
<li class="toc-entry toc-h3"><a href="#3.2.-Understanding-the-Output">3.2. Understanding the Output </a></li>
<li class="toc-entry toc-h3"><a href="#3.3.-Creating-word-and-sentence-vectors-from-hidden-states">3.3. Creating word and sentence vectors from hidden states </a></li>
<li class="toc-entry toc-h3"><a href="#3.4.-Confirming-contextually-dependent-vectors">3.4. Confirming contextually dependent vectors </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-11-Bert-Word-Embedding-Tutorial.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>ref: <a href="http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/">http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.-Loading-Pre-Trained-BERT">
<a class="anchor" href="#1.-Loading-Pre-Trained-BERT" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Loading Pre-Trained BERT<a class="anchor-link" href="#1.-Loading-Pre-Trained-BERT"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span> pip install pytorch-pretrained-bert
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Collecting pytorch-pretrained-bert
  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 2.8MB/s 
Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.2)
Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.12.38)
Requirement already satisfied: torch&gt;=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)
Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)
Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.38.0)
Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)
Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;pytorch-pretrained-bert) (0.9.5)
Requirement already satisfied: botocore&lt;1.16.0,&gt;=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;pytorch-pretrained-bert) (1.15.38)
Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;pytorch-pretrained-bert) (0.3.3)
Requirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;pytorch-pretrained-bert) (1.24.3)
Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;pytorch-pretrained-bert) (2.8)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;pytorch-pretrained-bert) (2020.4.5.1)
Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;pytorch-pretrained-bert) (3.0.4)
Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore&lt;1.16.0,&gt;=1.15.38-&gt;boto3-&gt;pytorch-pretrained-bert) (2.8.1)
Requirement already satisfied: docutils&lt;0.16,&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore&lt;1.16.0,&gt;=1.15.38-&gt;boto3-&gt;pytorch-pretrained-bert) (0.15.2)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.16.0,&gt;=1.15.38-&gt;boto3-&gt;pytorch-pretrained-bert) (1.12.0)
Installing collected packages: pytorch-pretrained-bert
Successfully installed pytorch-pretrained-bert-0.6.2
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">pytorch_pretrained_bert</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertModel</span><span class="p">,</span> <span class="n">BertForMaskedLM</span>

<span class="c1"># OPTIONAL: if you want to have more information on what's happening, activate the logger as follows</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="c1"># logging.basicConfig(level=logging.INFO)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span> <span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># Load pre-trained model tokenizer (vocabulary)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231508/231508 [00:00&lt;00:00, 315280.35B/s]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.-Input-Formatting">
<a class="anchor" href="#2.-Input-Formatting" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Input Formatting<a class="anchor-link" href="#2.-Input-Formatting"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.1.-Special-Tokens">
<a class="anchor" href="#2.1.-Special-Tokens" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.1. Special Tokens<a class="anchor-link" href="#2.1.-Special-Tokens"> </a>
</h3>
<p>Special tokens to mark the beginning ([CLS]) and seprataion/end of sentences ([SEP])</p>
<p>BERT can take as input either one or two sentences, and expects special tokens to mark the beginning and end of each one:</p>
<p><strong>2 Sentence Input:</strong></p>
<p><code>[CLS] The man wnent to the store. [SEP] He bought a gallon of milk. [SEP]</code></p>
<p><strong>1 Sentence Input:</strong></p>
<p><code>[CLS] The man went to the store. [SEP]</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.2.-Tokenization">
<a class="anchor" href="#2.2.-Tokenization" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2. Tokenization<a class="anchor-link" href="#2.2.-Tokenization"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"Here is the sentence I want embeddings for."</span>
<span class="n">marked_text</span> <span class="o">=</span> <span class="s2">"[CLS] "</span> <span class="o">+</span> <span class="n">text</span> <span class="o">+</span> <span class="s2">" [SEP]"</span>

<span class="c1"># Tokenize our sentence with the BERT tokenizer.</span>
<span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">marked_text</span><span class="p">)</span>

<span class="c1"># Print out the tokens.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define a new example sentence with multiple meanings of the word "bank"</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank."</span>

<span class="c1"># Add the special tokens.</span>
<span class="n">marked_text</span> <span class="o">=</span> <span class="s2">"[CLS] "</span> <span class="o">+</span> <span class="n">text</span> <span class="o">+</span> <span class="s2">" [SEP]"</span>

<span class="c1"># Split the sentence into tokens.</span>
<span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">marked_text</span><span class="p">)</span>

<span class="c1"># Map the token strings to their vocabulary indeces.</span>
<span class="n">indexed_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>

<span class="c1"># Display the words with their indeces.</span>
<span class="k">for</span> <span class="n">tup</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">,</span> <span class="n">indexed_tokens</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="si">{:&lt;12}</span><span class="s1"> </span><span class="si">{:&gt;6,}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tup</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tup</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[CLS]           101
after         2,044
stealing     11,065
money         2,769
from          2,013
the           1,996
bank          2,924
vault        11,632
,             1,010
the           1,996
bank          2,924
robber       27,307
was           2,001
seen          2,464
fishing       5,645
on            2,006
the           1,996
mississippi   5,900
river         2,314
bank          2,924
.             1,012
[SEP]           102
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.3.-Segment-ID">
<a class="anchor" href="#2.3.-Segment-ID" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.3. Segment ID<a class="anchor-link" href="#2.3.-Segment-ID"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Mark each of the 22 tokens as belonging to sentence "1".</span>
<span class="n">segments_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">segments_ids</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.-Extracting-Embeddings">
<a class="anchor" href="#3.-Extracting-Embeddings" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Extracting Embeddings<a class="anchor-link" href="#3.-Extracting-Embeddings"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.1.-Running-BERT-on-our-text">
<a class="anchor" href="#3.1.-Running-BERT-on-our-text" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1. Running BERT on our text<a class="anchor-link" href="#3.1.-Running-BERT-on-our-text"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>model.eval()</code> evaluation mode turns off dropout regularization which is used in training</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Convert inputs to PyTorch tensors</span>
<span class="n">tokens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">indexed_tokens</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">)</span>
<span class="n">segments_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">segments_ids</span><span class="p">])</span>

<span class="c1"># Load pre-trained model (weights)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="c1"># Put the model in "evaluation" mode, meaning feed-forward operation.</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([[  101,  2044, 11065,  2769,  2013,  1996,  2924, 11632,  1010,  1996,
          2924, 27307,  2001,  2464,  5645,  2006,  1996,  5900,  2314,  2924,
          1012,   102]])
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 407873900/407873900 [00:35&lt;00:00, 11382845.79B/s]
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): BertLayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (1): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (2): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (3): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (4): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (5): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (6): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (7): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (8): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (9): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (10): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (11): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>torch.no_grad</code> deactivates the gradient calculations, saves memory, and speeds up computation. (we don't need gradients or backpropagation since we're just running a forward pass).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Predict hidden states features for each layer</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">encoded_layers</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">,</span> <span class="n">segments_tensors</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.2.-Understanding-the-Output">
<a class="anchor" href="#3.2.-Understanding-the-Output" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2. Understanding the Output<a class="anchor-link" href="#3.2.-Understanding-the-Output"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Number of layers:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_layers</span><span class="p">))</span>
<span class="n">layer_i</span> <span class="o">=</span> <span class="mi">0</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Number of batches:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_layers</span><span class="p">[</span><span class="n">layer_i</span><span class="p">]))</span>
<span class="n">batch_i</span> <span class="o">=</span> <span class="mi">0</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Number of tokens:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_layers</span><span class="p">[</span><span class="n">layer_i</span><span class="p">][</span><span class="n">batch_i</span><span class="p">]))</span>
<span class="n">token_i</span> <span class="o">=</span> <span class="mi">0</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Number of hidden units:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_layers</span><span class="p">[</span><span class="n">layer_i</span><span class="p">][</span><span class="n">batch_i</span><span class="p">][</span><span class="n">token_i</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of layers: 12
Number of batches: 1
Number of tokens: 22
Number of hidden units: 768
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># For the 5th token in our sentence, select its feature values from layer 5.</span>
<span class="n">token_i</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">layer_i</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">encoded_layers</span><span class="p">[</span><span class="n">layer_i</span><span class="p">][</span><span class="n">batch_i</span><span class="p">][</span><span class="n">token_i</span><span class="p">]</span>

<span class="c1"># Plot the values as a histogram to show their distribution.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># `encoded_layers` is a Python list.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'     Type of encoded_layers: '</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">encoded_layers</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_layers</span><span class="p">))</span>

<span class="c1"># Each layer in the list is a torch tensor.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Tensor shape for each layer: '</span><span class="p">,</span> <span class="n">encoded_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>     Type of encoded_layers:  &lt;class 'list'&gt; 12
Tensor shape for each layer:  torch.Size([1, 22, 768])
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Concatenate the tensors for all layers. We use `stack` here to </span>
<span class="c1"># create a new dimension in the tensor.</span>
<span class="n">token_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">encoded_layers</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">token_embeddings</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([12, 1, 22, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Remove dimension 1, the "batches".</span>
<span class="n">token_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">token_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">token_embeddings</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([12, 22, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Swap dimensions 0 and 1.</span>
<span class="n">token_embeddings</span> <span class="o">=</span> <span class="n">token_embeddings</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">token_embeddings</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([22, 12, 768])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.3.-Creating-word-and-sentence-vectors-from-hidden-states">
<a class="anchor" href="#3.3.-Creating-word-and-sentence-vectors-from-hidden-states" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3. Creating word and sentence vectors from hidden states<a class="anchor-link" href="#3.3.-Creating-word-and-sentence-vectors-from-hidden-states"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Word Vectors</strong></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Stores the token vectors, with shape [22 x 3,072] (4 x 768 = 3,072)</span>
<span class="n">token_vecs_cat</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># `token_embeddings` is a [22 x 12 x 768] tensor.</span>

<span class="c1"># For each token in the sentence...</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">token_embeddings</span><span class="p">:</span>

    <span class="c1"># `token` is a [12 x 768] tensor</span>

    <span class="c1"># Concatenate the vectors (that is, append them together) from the last four layers.</span>
    <span class="c1"># Each layer vector is 768 values, so `cat_vec` is length 3,072.</span>
    <span class="n">cat_vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">token</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">token</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">token</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="n">token</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Use `cat_vec` to represent `token`.</span>
    <span class="n">token_vecs_cat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cat_vec</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Shape is: </span><span class="si">%d</span><span class="s1"> x </span><span class="si">%d</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">token_vecs_cat</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_vecs_cat</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Shape is: 22 x 3072
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Stores the token vectors, with shape [22 x 768]</span>
<span class="n">token_vecs_sum</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># `token_embeddings` is a [22 x 12 x 768] tensor.</span>

<span class="c1"># For each token in the sentence...</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">token_embeddings</span><span class="p">:</span>

    <span class="c1"># `token` is a [12 x 768] tensor</span>

    <span class="c1"># Sum the vectors from the last four layers.</span>
    <span class="n">sum_vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">token</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Use `sum_vec` to represent `token`.</span>
    <span class="n">token_vecs_sum</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sum_vec</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Shape is: </span><span class="si">%d</span><span class="s1"> x </span><span class="si">%d</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">token_vecs_sum</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_vecs_sum</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Shape is: 22 x 768
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Sentence Vectors</strong></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># `encoded_layers` has shape [12 x 1 x 22 x 768]</span>

<span class="c1"># `token_vecs` is a tensor with shape [22 x 768]</span>
<span class="n">token_vecs</span> <span class="o">=</span> <span class="n">encoded_layers</span><span class="p">[</span><span class="mi">11</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Calculate the average of all 22 token vectors.</span>
<span class="n">sentence_embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">token_vecs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Our final sentence embedding vector of shape:"</span><span class="p">,</span> <span class="n">sentence_embedding</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Our final sentence embedding vector of shape: torch.Size([768])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.4.-Confirming-contextually-dependent-vectors">
<a class="anchor" href="#3.4.-Confirming-contextually-dependent-vectors" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4. Confirming contextually dependent vectors<a class="anchor-link" href="#3.4.-Confirming-contextually-dependent-vectors"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token_str</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">token_str</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>0 [CLS]
1 after
2 stealing
3 money
4 from
5 the
6 bank
7 vault
8 ,
9 the
10 bank
11 robber
12 was
13 seen
14 fishing
15 on
16 the
17 mississippi
18 river
19 bank
20 .
21 [SEP]
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'First 5 vector values for each instance of "bank".'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"bank vault   "</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">token_vecs_sum</span><span class="p">[</span><span class="mi">6</span><span class="p">][:</span><span class="mi">5</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"bank robber  "</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">token_vecs_sum</span><span class="p">[</span><span class="mi">10</span><span class="p">][:</span><span class="mi">5</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"river bank   "</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">token_vecs_sum</span><span class="p">[</span><span class="mi">19</span><span class="p">][:</span><span class="mi">5</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>First 5 vector values for each instance of "bank".

bank vault    tensor([ 2.1319, -2.1413, -1.6260,  0.8638,  3.3173])
bank robber   tensor([ 1.1868, -1.5298, -1.3770,  1.0648,  3.1446])
river bank    tensor([ 1.1295, -1.4725, -0.7296, -0.0901,  2.4970])
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cosine</span>

<span class="c1"># Calculate the cosine similarity between the word bank</span>
<span class="c1"># in "bank robber" vs "river bank" (different meanings).</span>
<span class="n">diff_bank</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">cosine</span><span class="p">(</span><span class="n">token_vecs_sum</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="n">token_vecs_sum</span><span class="p">[</span><span class="mi">19</span><span class="p">])</span>

<span class="c1"># Calculate the cosine similarity between the word bank</span>
<span class="c1"># in "bank robber" vs "bank vault" (same meaning).</span>
<span class="n">same_bank</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">cosine</span><span class="p">(</span><span class="n">token_vecs_sum</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="n">token_vecs_sum</span><span class="p">[</span><span class="mi">6</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Vector similarity for  *similar*  meanings: </span><span class="si">%.2f</span><span class="s1">'</span> <span class="o">%</span> <span class="n">same_bank</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Vector similarity for *different* meanings: </span><span class="si">%.2f</span><span class="s1">'</span> <span class="o">%</span> <span class="n">diff_bank</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Vector similarity for  *similar*  meanings: 0.95
Vector similarity for *different* meanings: 0.68
</pre>
</div>
</div>

</div>
</div>

</div>
    

</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="sparklingness/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An Blog for a wise NLP development.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/sparklingness" title="sparklingness"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/-" title="-"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
