<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://sparklingness.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://sparklingness.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-05-05T04:07:45-05:00</updated><id>https://sparklingness.github.io/blog/feed.xml</id><title type="html">wAIz NLP</title><subtitle>An Blog for a wise NLP development.</subtitle><entry><title type="html">BERT Document Classification Tutorial</title><link href="https://sparklingness.github.io/blog/bert/jupyter/2020/05/05/BERT_Document_Classification_Tutorial_with_Code.html" rel="alternate" type="text/html" title="BERT Document Classification Tutorial" /><published>2020-05-05T00:00:00-05:00</published><updated>2020-05-05T00:00:00-05:00</updated><id>https://sparklingness.github.io/blog/bert/jupyter/2020/05/05/BERT_Document_Classification_Tutorial_with_Code</id><content type="html" xml:base="https://sparklingness.github.io/blog/bert/jupyter/2020/05/05/BERT_Document_Classification_Tutorial_with_Code.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-05-05-BERT_Document_Classification_Tutorial_with_Code.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Classifying-Wikipedia-Comments-with-BERT&quot;&gt;Classifying Wikipedia Comments with BERT&lt;a class=&quot;anchor-link&quot; href=&quot;#Classifying-Wikipedia-Comments-with-BERT&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Contents&quot;&gt;Contents&lt;a class=&quot;anchor-link&quot; href=&quot;#Contents&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Part-I---Setup-&amp;amp;-Dataset-Prep&quot;&gt;Part I - Setup &amp;amp; Dataset Prep&lt;a class=&quot;anchor-link&quot; href=&quot;#Part-I---Setup-&amp;amp;-Dataset-Prep&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-Setup&quot;&gt;1. Setup&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-Setup&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Get the GPU device name.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;device_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gpu_device_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The device name should look like the following:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;/device:GPU:0&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Found GPU at : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;ne&quot;&gt;SystemError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;GPU device not found&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Found GPU at : /device:GPU:0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;tensorflow_version&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Currently selected TF version: 2.x
Available versions:
* 1.x
* 2.x
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# If there&amp;#39;s a GPU available,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Tell PyTorch to use the GPU.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;cuda&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;There are &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; GPU(s) available.&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;We will use the GPU:&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_device_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# IF not,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;No GPU available, using the CPU instead.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;cpu&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;There are 1 GPU(s) available.
We will use the GPU: Tesla P4
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; pip install transformers -q
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;     |████████████████████████████████| 573kB 42.4MB/s 
     |████████████████████████████████| 1.0MB 49.5MB/s 
     |████████████████████████████████| 3.7MB 50.7MB/s 
     |████████████████████████████████| 890kB 36.4MB/s 
  Building wheel for sacremoses (setup.py) ... done
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-Retrieve-&amp;amp;-Inspect-Dataset&quot;&gt;2. Retrieve &amp;amp; Inspect Dataset&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-Retrieve-&amp;amp;-Inspect-Dataset&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;urllib&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create the data subdirectory if not there.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;./data/&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mkdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;./data/&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;files&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
         &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;./data/attack_annotated_comments.tsv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;https://ndownloader.figshare.com/files/7554634&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
         &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;./data/attack_annotations.tsv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;https://ndownloader.figshare.com/files/7554637&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Download download if we don&amp;#39;t already have it!&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Download the dataset.&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Downloading&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;urllib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urlretrieve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;   DONE.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Downloading ./data/attack_annotated_comments.tsv
   DONE.
Downloading ./data/attack_annotations.tsv
   DONE.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Parsing the dataset .tsv file...&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;./data/attack_annotated_comments.tsv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index_col&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;annotations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;./data/attack_annotations.tsv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;   Done.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Parsing the dataset .tsv file...
   Done.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Display the first five rows of the table.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;comment&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;logged_in&lt;/th&gt;
      &lt;th&gt;ns&lt;/th&gt;
      &lt;th&gt;sample&lt;/th&gt;
      &lt;th&gt;split&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;rev_id&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;37675&lt;/th&gt;
      &lt;td&gt;`-NEWLINE_TOKENThis is not ``creative``.  Thos...&lt;/td&gt;
      &lt;td&gt;2002&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;article&lt;/td&gt;
      &lt;td&gt;random&lt;/td&gt;
      &lt;td&gt;train&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;44816&lt;/th&gt;
      &lt;td&gt;`NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...&lt;/td&gt;
      &lt;td&gt;2002&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;article&lt;/td&gt;
      &lt;td&gt;random&lt;/td&gt;
      &lt;td&gt;train&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;49851&lt;/th&gt;
      &lt;td&gt;NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...&lt;/td&gt;
      &lt;td&gt;2002&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;article&lt;/td&gt;
      &lt;td&gt;random&lt;/td&gt;
      &lt;td&gt;train&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;89320&lt;/th&gt;
      &lt;td&gt;Next, maybe you could work on being less cond...&lt;/td&gt;
      &lt;td&gt;2002&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;article&lt;/td&gt;
      &lt;td&gt;random&lt;/td&gt;
      &lt;td&gt;dev&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;93890&lt;/th&gt;
      &lt;td&gt;This page will need disambiguation.&lt;/td&gt;
      &lt;td&gt;2002&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;article&lt;/td&gt;
      &lt;td&gt;random&lt;/td&gt;
      &lt;td&gt;train&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;comment&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;split&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;split&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;comment&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;split&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;dev&lt;/th&gt;
      &lt;td&gt;23160&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;test&lt;/th&gt;
      &lt;td&gt;23178&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;train&lt;/th&gt;
      &lt;td&gt;69526&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;rev_id&lt;/th&gt;
      &lt;th&gt;worker_id&lt;/th&gt;
      &lt;th&gt;quoting_attack&lt;/th&gt;
      &lt;th&gt;recipient_attack&lt;/th&gt;
      &lt;th&gt;third_party_attack&lt;/th&gt;
      &lt;th&gt;other_attack&lt;/th&gt;
      &lt;th&gt;attack&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;107339&lt;/th&gt;
      &lt;td&gt;44745233&lt;/td&gt;
      &lt;td&gt;2584&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1225742&lt;/th&gt;
      &lt;td&gt;602195543&lt;/td&gt;
      &lt;td&gt;330&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1134782&lt;/th&gt;
      &lt;td&gt;534921976&lt;/td&gt;
      &lt;td&gt;478&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;964600&lt;/th&gt;
      &lt;td&gt;421581160&lt;/td&gt;
      &lt;td&gt;57&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;199505&lt;/th&gt;
      &lt;td&gt;70323643&lt;/td&gt;
      &lt;td&gt;1893&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Label a comment as an attack if the majority of annotators did so&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# The comments are uniquely identified by their &amp;#39;rev_id&amp;#39;.  The annotations table&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# has multiple rows for each comment because there were multiple labelers.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;rev_id&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;attack&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Join labels and comments&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;attack&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Remove newline and tab tokens&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;comment&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;comment&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;NEWLINE_TOKEN&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;comment&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;comment&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;TAB_TOKEN&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# fit a simple text classifier&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_comments&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;split==&amp;#39;train&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_comments&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;split==&amp;#39;test&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;textwrap&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Wrap text to 80 characters.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wrapper&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;textwrap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TextWrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Filter to just the &amp;quot;attack&amp;quot; comments.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;attack_examples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;attack&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;comment&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Randomly choose some examples.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attack_examples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wrapper&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attack_examples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;
I SHOULD BE UNBANNED THIS INSTANT! GO TO HELL Android79!


  == Booshakla ==  The guy was a sockpuppeteer here on Wikipedia. Quit acting
like a cunt.


  ====  FUCK YOU YOU PIECE OF SHIT! WHY THE FUCK HAVE YOU DELETED MY ARTICLES? I
WILL REPORT YOU YOU FUCKING PIECE OF SHIT!


  == douchebag ==  you&amp;#39;re an asshole.  charlie.


  == guess what ==  your retarded


  ::you&amp;#39;re right. allmusic is the last shit.


` ::::::: It&amp;#39;s not an encyclopedia.  Brittanica is an encyclopedia. Brittanica
doesn&amp;#39;t choose to employ [User:Essjay|lying pissants] as a reward for their
duplicitous behavior.  This online ``encyclopedia``, you self-righteous inbred
piece of shit, is a caricature.`


   ::::::::::::::::::::::;WHAT THE FUCK IS WRONG WITH YOU PEOPLE? STOP CRITIZING
US!!! YOU&amp;#39;RE WRITING IN OPINION!! FUCK STOP THIS! MADDOX IS A PRICK


  The Greater San Francisco Bay Area is greater than the typical Bay Area
Definition you idiot stop reverting my edits until YOU have sources obviously
like every other metro area a greater region is obviously bigger you idiot. Go
shove your internet awards some where because they dont matter when all you do
is type on a key board.


`  == Why ==  Why are you so stupid?   `

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_comments&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_attacks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;attack&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; of &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; comments are attacks (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:.2%}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;)&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_attacks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_attacks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;13,590 of 115,864 comments are attacks (11.73%)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prcnt_non_attack&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;attack&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Always predicting &amp;quot;not attack&amp;quot; will yeild &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:.2%}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; accuracy on the test set.&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prcnt_non_attack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Always predicting &amp;#34;not attack&amp;#34; will yeild 88.11% accuracy on the test set.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-BERT-Input-Length-Limitation&quot;&gt;3. BERT Input Length Limitation&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-BERT-Input-Length-Limitation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Load the BERT tokenizer.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Loading BERT tokenizer...&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;bert-base-uncased&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;do_lower_case&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Loading BERT tokenizer...

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Retrieve the text of the first comment.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Run the tokenizer to count up the number of tokens. The tokenizer will split&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# the text into words, punctuation, and subwords as needed.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Comment 0 (not an attack) contains &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; WordPiece tokens.&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;Original comment text:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wrapper&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Comment 0 (not an attack) contains 591 WordPiece tokens.

Original comment text:

`- This is not ``creative``.  Those are the dictionary definitions of the terms
``insurance`` and ``ensurance`` as properly applied to ``destruction``.  If you
don&amp;#39;t understand that, fine, legitimate criticism, I&amp;#39;ll write up ``three man
cell`` and ``bounty hunter`` and then it will be easy to understand why
``ensured`` and ``insured`` are different - and why both differ from
``assured``.  The sentence you quote is absolutely neutral.  You just aren&amp;#39;t
familiar with the underlying theory of strike-back (e.g. submarines as employed
in nuclear warfare) guiding the insurance, nor likely the three man cell
structure that kept the IRA from being broken by the British.  If that&amp;#39;s my
fault, fine, I can fix that to explain.  But ther&amp;#39;es nothing ``personal`` or
``creative`` about it.  I&amp;#39;m tired of arguing with you.  Re: the other article,
``multi-party`` turns up plenty, and there is more use of ``mutually`` than
``mutual``.  If I were to apply your standard I&amp;#39;d be moving ``Mutual Assured
Destruction`` to ``talk`` for not appealing to a Reagan voter&amp;#39;s biases about its
effectiveness, and for dropping the ``ly``.  There is a double standard in your
edits.  If it comes from some US history book, like ``peace movement`` or
&amp;#39;M.A.D.&amp;#39; as defined in 1950, you like it, even if the definition is totally
useless in 2002 and only of historical interest.    If it makes any even-obvious
connection or implication from the language chosen in multiple profession-
specific terms, you consider it somehow non-neutral...  Gandhi thinks ``eye for
an eye`` describes riots, death penalty, and war all at once, but you don&amp;#39;t.
What do you know that Gandhi doesn&amp;#39;t?  Guess what:  reality is not neutral.
Current use of terms is slightly more controversial.  Neutrality requires
negotiation, and some willingness to learn.  This is your problem not mine.  You
may dislike the writing, fine, that can be fixed.  But disregarding fundamental
axioms of philosphy with names that recur in multiple phrases, or failing to
make critical distinctions like &amp;#39;insurance&amp;#39; versus &amp;#39;assurance&amp;#39; versus
&amp;#39;ensurance&amp;#39; (which are made in one quote by an Air Force general in an in-
context quote), is just a disservice to the reader.  If someone comes here to
research a topic like MAD, they want some context, beyond history.  If this is a
history book, fine, it&amp;#39;s a history book.  But that wasn&amp;#39;t what it was claimed to
be... `
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Print out the list of tokens&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;==== First 512 tokens: ====&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wrapper&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))))&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;==== Remaining &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; tokens: ====&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wrapper&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]))))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;==== First 512 tokens: ====

` - this is not ` ` creative ` ` . those are the dictionary definitions of the
terms ` ` insurance ` ` and ` ` en ##sur ##ance ` ` as properly applied to ` `
destruction ` ` . if you don &amp;#39; t understand that , fine , legitimate criticism ,
i &amp;#39; ll write up ` ` three man cell ` ` and ` ` bounty hunter ` ` and then it
will be easy to understand why ` ` ensured ` ` and ` ` ins ##ured ` ` are
different - and why both differ from ` ` assured ` ` . the sentence you quote is
absolutely neutral . you just aren &amp;#39; t familiar with the underlying theory of
strike - back ( e . g . submarines as employed in nuclear warfare ) guiding the
insurance , nor likely the three man cell structure that kept the ira from being
broken by the british . if that &amp;#39; s my fault , fine , i can fix that to explain
. but the ##r &amp;#39; es nothing ` ` personal ` ` or ` ` creative ` ` about it . i &amp;#39; m
tired of arguing with you . re : the other article , ` ` multi - party ` ` turns
up plenty , and there is more use of ` ` mutually ` ` than ` ` mutual ` ` . if i
were to apply your standard i &amp;#39; d be moving ` ` mutual assured destruction ` `
to ` ` talk ` ` for not appealing to a reagan voter &amp;#39; s bias ##es about its
effectiveness , and for dropping the ` ` l ##y ` ` . there is a double standard
in your edit ##s . if it comes from some us history book , like ` ` peace
movement ` ` or &amp;#39; m . a . d . &amp;#39; as defined in 1950 , you like it , even if the
definition is totally useless in 2002 and only of historical interest . if it
makes any even - obvious connection or implication from the language chosen in
multiple profession - specific terms , you consider it somehow non - neutral . .
. gandhi thinks ` ` eye for an eye ` ` describes riots , death penalty , and war
all at once , but you don &amp;#39; t . what do you know that gandhi doesn &amp;#39; t ? guess
what : reality is not neutral . current use of terms is slightly more
controversial . neutrality requires negotiation , and some willingness to learn
. this is your problem not mine . you may dislike the writing , fine , that can
be fixed . but disregard ##ing fundamental ax ##ioms of phil ##os ##phy with
names that rec ##ur in multiple phrases , or failing to make critical
distinctions like &amp;#39; insurance &amp;#39; versus &amp;#39; assurance &amp;#39; versus &amp;#39; en ##sur ##ance &amp;#39;
( which


==== Remaining 79 tokens: ====

are made in one quote by an air force general in an in - context quote ) , is
just a di ##sser ##vic ##e to the reader . if someone comes here to research a
topic like mad , they want some context , beyond history . if this is a history
book , fine , it &amp;#39; s a history book . but that wasn &amp;#39; t what it was claimed to
be . . . `
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# First truncate the text to remove the last 79 tokens (which begin with the words &amp;quot;are made in&amp;quot;).&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;last_char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;are made in&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Trancate the text to only what fits in the 512 tokens.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last_char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Estimate the number of words in the comment by splitting it on whitespace.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# First remove all double spaces.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;  &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_words&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Comment contains ~&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; words.&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Estimate the number of sentences by counting up the periods.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_sens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;. &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Comment contains ~&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; sentences.&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_sens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Comment contains ~330 words.
Comment contains ~20 sentences.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Tokenize all of the sentences and map the tokens to their word IDs.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Record the length of each sequence (after truncating to 512).&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lengths&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Tokenizing comments...&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For every sentence,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sen&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Report progress.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;  Read &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; comments.&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# `encode` will:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#    (1) Tokenize the sentence.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#    (2) Prepend the `[CLS]` token to the start.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#    (3) Append the `[SEP]` token to the end.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#    (4) Map tokens to their IDs.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;encoded_sent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                                        &lt;span class=&quot;c1&quot;&gt;# Sentence to encode.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;add_special_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# Add [CLS]&amp;#39; and &amp;#39;[SEP]&amp;#39;&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#max_length = 512,                  # Truncate all sentences.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#return_tensors = &amp;#39;pt&amp;#39;,             # Return pytorch tensors.&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Add the encoded sentence to the list.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Record the truncated length.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lengths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;DONE.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;10,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; comments&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;Token indices sequence length is longer than the specified maximum sequence length for this model (591 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (541 &amp;gt; 512). Running this sequence through the model will result in indexing errors
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Tokenizing comments...
  Read 0 comments.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;Token indices sequence length is longer than the specified maximum sequence length for this model (1088 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (593 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1057 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (791 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (608 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (877 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (729 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1029 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1032 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (662 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (617 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1886 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (652 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (546 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (903 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (574 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (611 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (962 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1007 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (616 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (629 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1091 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1061 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1698 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1002 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1045 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (634 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (622 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (577 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (537 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1072 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1092 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (794 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (963 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (976 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (838 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (801 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (513 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1954 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (964 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (548 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (601 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (626 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (892 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (697 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1999 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (795 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (568 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (682 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (657 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (957 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (614 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1002 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1316 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2210 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2243 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1345 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1100 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (847 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (612 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1285 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (627 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (620 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1107 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2226 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2108 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (559 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1658 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (954 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (986 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (557 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (577 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (594 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1029 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (517 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1046 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (942 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2303 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (872 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (669 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1067 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1175 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1175 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1177 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (570 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (686 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (638 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1463 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (558 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (915 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (516 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (848 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1490 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1483 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1124 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (912 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1238 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1176 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1198 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2062 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1285 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1074 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1159 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1273 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1069 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (917 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (932 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1011 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (601 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (619 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1098 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1132 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (562 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (884 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (997 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (747 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (804 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1063 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (527 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1326 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2526 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1703 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1338 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (782 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (806 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (719 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (728 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (903 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1993 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (711 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (515 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1001 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1114 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (894 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (862 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (708 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (730 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1089 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1513 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (780 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (895 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3810 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (545 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1034 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (537 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (617 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (636 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (579 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (760 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1872 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (597 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (650 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1196 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (979 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (624 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1498 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1519 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3125 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1500 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1095 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (544 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (844 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3368 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (600 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1860 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (588 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (613 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (864 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1261 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (559 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1691 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (648 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (576 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (891 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1850 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (544 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (573 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1685 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2128 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (704 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (581 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (787 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (742 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (933 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (534 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (633 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (795 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (646 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (617 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (653 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1040 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2065 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (608 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1304 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2559 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (513 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1054 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (603 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (760 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (876 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2643 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1334 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (599 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (528 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1005 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1582 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (511 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (647 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1141 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (649 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (663 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2857 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (816 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1097 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1126 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (630 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1009 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2110 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (649 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1013 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (665 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (515 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (601 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1747 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1036 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1339 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (665 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (770 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1856 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (615 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (978 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (563 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1102 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (896 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (934 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (514 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1022 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (664 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (671 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (611 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (621 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (628 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (798 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1043 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2858 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (664 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (974 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (623 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (749 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (966 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (663 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (582 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (572 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (857 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (611 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (908 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (856 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1002 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (836 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (732 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (542 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (929 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (579 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1082 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2219 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2223 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (665 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (790 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (622 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (9322 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3078 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1401 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2031 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (760 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (920 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1907 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1905 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1903 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (571 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (653 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (929 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1026 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (516 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3624 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (918 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1022 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (552 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (652 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2698 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (794 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (977 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2253 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1747 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1743 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (741 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (540 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1473 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (566 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1749 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1257 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (986 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (615 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (546 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (556 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1197 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (844 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (553 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (565 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1021 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1473 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (824 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (870 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1236 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (588 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1109 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (609 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (675 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (944 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (650 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (562 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (896 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1467 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (972 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (670 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (838 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1284 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1122 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1084 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (640 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (928 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1718 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (739 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1093 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1273 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1351 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (517 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (912 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (748 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (637 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (785 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (764 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1671 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (959 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (642 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (884 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (638 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (640 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1218 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1167 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (532 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (534 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1144 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (517 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (522 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (530 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (726 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (532 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (832 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (870 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (857 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (911 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (911 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1012 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1154 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (873 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (888 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1229 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (904 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (939 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (618 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1867 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2325 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (587 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1017 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2173 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (655 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (846 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1012 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (836 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1156 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1802 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (837 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (907 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (707 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1181 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (849 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (568 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (845 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1012 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (785 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (805 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1015 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (632 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (909 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (878 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3130 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (849 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2102 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (705 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (556 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (563 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (911 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (925 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (547 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (814 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1556 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1666 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1038 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1036 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1927 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (663 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2089 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (820 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (663 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (656 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (800 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1091 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (574 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1578 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (518 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (819 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (566 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (754 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (548 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (552 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (970 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (819 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1086 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1338 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2496 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (4204 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (4559 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (909 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (971 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1044 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (886 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1235 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1173 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (663 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (737 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (899 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1650 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1273 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (689 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (888 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (521 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (879 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1033 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (721 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (817 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (692 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1385 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (930 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1528 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1098 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (815 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (805 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (515 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1944 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (966 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2049 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (690 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (875 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1746 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (780 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (907 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1317 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (953 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (673 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (625 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (521 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (819 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1113 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (660 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (943 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (627 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1004 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2106 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1950 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (986 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (668 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (512 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2391 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (925 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1225 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2095 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (952 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (744 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (559 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1085 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (515 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (766 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (635 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1306 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1320 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (983 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1302 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2507 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (574 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1737 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1113 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (962 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (523 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (878 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1121 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (619 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (526 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1158 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2430 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (535 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1747 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (593 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (777 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1036 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (745 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (602 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1650 &amp;gt; 512). Running this sequence through the model will result in indexing errors
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;  Read 20,000 comments.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;Token indices sequence length is longer than the specified maximum sequence length for this model (548 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (959 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (683 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (526 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1074 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1078 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (978 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (609 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1648 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (586 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (665 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (669 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1182 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (989 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (650 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1082 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (646 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (872 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (742 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (747 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (570 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (522 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (935 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (530 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1364 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1256 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (984 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (562 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1054 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (954 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (984 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1145 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (785 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1037 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1599 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (890 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1101 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (576 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (532 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (853 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1114 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (864 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (565 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (689 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1194 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (878 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1054 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (768 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (937 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (584 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (996 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (821 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1390 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1115 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1176 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (729 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (677 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (584 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (580 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1559 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1500 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (684 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (804 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (816 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (520 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (881 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1354 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (599 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1987 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (647 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2252 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1968 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (944 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1133 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1219 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2200 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1481 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (829 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (869 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3333 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2858 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (9859 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (521 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (720 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (758 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (602 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (604 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (611 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (515 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1011 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1636 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (538 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (715 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1638 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (777 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (4962 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (569 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (693 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (516 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (653 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (624 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (584 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (910 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1728 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (574 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2495 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (842 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (929 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1183 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (591 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (583 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3750 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1643 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2046 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2042 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (736 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (520 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (588 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (531 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (530 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1165 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (915 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (674 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (662 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1047 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1763 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1607 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1123 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1132 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (580 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (851 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (545 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (734 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (530 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (582 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1142 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (875 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (554 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (767 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1687 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (932 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1064 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (698 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (615 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (644 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (624 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (522 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (860 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (988 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1914 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (560 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (516 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (895 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (534 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3118 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (545 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2106 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (536 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (534 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (552 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1874 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1631 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (903 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1201 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (679 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3364 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3155 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1978 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2088 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1982 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (511 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (561 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (618 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (978 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (583 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (750 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (553 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1516 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1676 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (532 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (959 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1916 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1865 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (529 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (691 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1248 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (989 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (609 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (632 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (533 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1195 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1664 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (931 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (593 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (923 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (535 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1492 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (587 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1449 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (574 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (946 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1797 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (999 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (602 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (860 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (783 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (599 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (929 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (692 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (628 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (993 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2224 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2853 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (548 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2275 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2503 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2497 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1888 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2199 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (730 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2248 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (689 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (805 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3750 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1054 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2055 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1876 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (818 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1685 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (528 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1107 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (829 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (648 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1014 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1017 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (645 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (664 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (729 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1233 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1094 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (906 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (517 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (758 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (710 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2133 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1862 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (518 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1004 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (519 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (927 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2071 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (815 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1702 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (567 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (678 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (854 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (587 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (707 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (638 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (886 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (861 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (975 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (737 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (871 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (975 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (581 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1774 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (814 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (744 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3995 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (658 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2207 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1459 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (934 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (949 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (829 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (519 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (889 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1035 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1047 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1486 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1089 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1671 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (853 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1571 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1888 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1571 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1233 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (985 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (623 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1073 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (625 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (733 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2211 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (631 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (632 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (611 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (950 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (920 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (765 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (696 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1536 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1058 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1085 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (895 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (666 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (518 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2218 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1040 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (683 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2312 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1218 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (928 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1112 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1190 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1098 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (731 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1055 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1766 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1051 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1992 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (795 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (985 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1092 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2331 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (686 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (592 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1423 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (691 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (689 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1442 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (995 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1175 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (826 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (618 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1526 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1267 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1062 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1374 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (559 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1137 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2020 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3985 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (677 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (793 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (746 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (621 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1278 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1120 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (770 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (670 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (842 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (923 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (939 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (760 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2000 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3125 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (599 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (656 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1034 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3153 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2956 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1226 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1056 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (580 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (786 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1068 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (910 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1425 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (886 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (519 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (694 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (533 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (749 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (644 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (807 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3340 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1388 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (687 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1137 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (754 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (619 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (796 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2099 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1151 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1428 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (751 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (597 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (990 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1078 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (858 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (724 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (791 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (902 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1277 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (529 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1804 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (861 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (530 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1396 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (603 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (526 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (580 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (815 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2307 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (602 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (568 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1066 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (865 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1007 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1008 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (4239 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (800 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1107 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1078 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1520 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1441 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (910 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1245 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (985 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (613 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (700 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (884 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1577 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (781 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (973 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1079 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (856 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (847 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2170 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (843 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (879 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (816 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (629 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1002 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (665 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (575 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (639 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (688 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (538 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (971 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (695 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (626 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (640 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (883 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (682 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1046 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (905 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (511 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (524 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (513 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (932 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2223 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (807 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (668 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2121 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (858 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (680 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (736 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (588 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (842 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (538 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (840 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (761 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (546 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2723 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1000 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1108 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (826 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (760 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (577 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (619 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (588 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (748 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (875 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2060 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1792 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (639 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1250 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1477 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (704 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (593 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (624 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (827 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (644 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (980 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1034 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (4000 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3333 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1256 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (511 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2494 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1380 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2300 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1298 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2210 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (569 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (615 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1096 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1581 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1397 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1467 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (661 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (703 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (615 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (982 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1001 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (569 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1083 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (704 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (803 &amp;gt; 512). Running this sequence through the model will result in indexing errors
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;  Read 40,000 comments.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;Token indices sequence length is longer than the specified maximum sequence length for this model (629 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (588 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (516 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2054 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2041 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2037 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2040 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1079 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (668 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (641 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (726 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1028 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1809 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (560 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (931 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1065 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1124 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (646 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (589 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1177 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (795 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (803 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (878 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (554 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (554 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (812 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (563 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (559 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (615 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (603 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2351 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (726 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (745 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (549 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (792 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (614 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (736 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2282 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (749 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (795 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (836 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (789 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (560 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1653 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (844 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (538 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1509 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1509 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (845 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (745 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2255 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (617 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1724 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1000 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1324 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1086 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1203 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1011 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (805 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (517 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (723 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (766 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (559 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (529 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (836 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (4283 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (699 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (614 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (973 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (652 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (832 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (686 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (567 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (936 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1173 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (819 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (614 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (588 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (730 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1161 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (833 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (555 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (661 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (672 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (538 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (985 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2497 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (4082 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (835 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (512 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (632 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (536 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (538 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (836 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (792 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (713 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (913 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (606 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (536 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (559 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (519 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (899 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1162 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (581 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1117 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (806 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (596 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (919 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (835 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1043 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (540 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3333 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3182 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (723 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1934 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1195 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (552 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (539 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1348 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1046 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1000 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (552 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (739 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (971 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1044 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (619 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (777 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1762 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (696 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (522 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1581 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (656 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (607 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (580 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (721 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (548 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (995 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (990 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1555 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (626 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (567 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (604 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (807 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (661 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (833 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1571 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (719 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1782 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1561 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (521 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (807 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (895 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1093 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1589 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1681 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (940 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (729 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (896 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (515 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (566 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (637 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (793 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1090 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (815 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (769 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1620 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (959 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1146 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (547 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (611 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1151 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1620 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (543 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1007 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1039 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2173 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (911 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (990 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (781 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1051 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1114 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1473 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (908 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1753 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (953 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2128 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (713 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (969 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2098 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1100 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2492 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (810 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (776 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (519 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (514 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (543 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1023 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (541 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (526 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (567 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (598 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2657 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2657 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (534 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1166 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (578 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (850 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1616 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (615 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (535 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (517 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (617 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (723 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (647 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (583 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1049 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1902 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1025 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (595 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (829 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2068 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (861 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1139 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (566 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (542 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2117 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (536 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (698 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1115 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (588 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2677 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (958 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (793 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (872 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1162 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (878 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (711 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (749 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (707 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (896 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (752 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (653 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (796 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (801 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (820 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1677 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (566 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (596 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (675 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (678 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (808 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (843 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (913 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (904 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (697 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1894 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (705 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (610 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (513 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (841 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2058 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2549 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1638 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (574 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (640 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1025 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (543 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (606 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (653 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (976 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (627 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (511 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (728 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (651 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1825 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (852 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (631 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1133 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (897 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2667 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (788 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (913 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2191 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (563 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (538 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (766 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (757 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1042 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (551 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (533 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (538 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1370 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (569 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (575 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (771 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (635 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (822 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (575 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1077 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (596 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (958 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (582 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (822 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1553 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (536 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (528 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (553 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (521 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1876 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1872 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (623 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (903 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1383 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (755 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (939 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (572 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (632 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (721 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (553 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (674 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (575 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (523 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (575 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (573 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (668 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1034 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (538 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (575 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (578 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (532 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (772 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (952 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1743 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (553 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (729 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (513 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (547 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1574 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (564 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (525 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (781 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2183 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (794 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (619 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (620 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (988 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (985 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (925 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (880 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (576 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (911 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (576 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1431 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1312 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (563 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (671 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (820 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1483 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (518 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (709 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2625 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (555 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (576 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1482 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (576 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (632 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (696 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (592 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (598 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (832 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (515 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1274 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (574 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (716 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1258 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (999 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1573 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (708 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1685 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (772 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1029 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1845 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (537 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1161 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (779 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1326 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (522 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1402 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (911 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (672 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (910 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (561 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (528 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (582 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (603 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1277 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (610 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (779 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1091 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (676 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (763 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (711 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1520 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (853 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (533 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (599 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (585 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1162 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1012 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (740 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (958 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1001 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (712 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (574 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (626 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (780 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (512 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (547 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (652 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (558 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1209 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (528 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (599 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (663 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (595 &amp;gt; 512). Running this sequence through the model will result in indexing errors
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;  Read 60,000 comments.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;Token indices sequence length is longer than the specified maximum sequence length for this model (522 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (624 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1002 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1398 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (718 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (585 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1077 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (734 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (596 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (520 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (861 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (574 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (906 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (566 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (551 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1017 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (557 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (518 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2072 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1040 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2343 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (803 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (570 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (678 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (553 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (616 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (644 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (777 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (787 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (703 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (689 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (679 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (623 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (567 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (728 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (771 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1197 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1064 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2730 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2021 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1590 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1605 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (798 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (884 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (945 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (543 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (672 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1040 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (994 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (574 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1197 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1690 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1743 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (545 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1602 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (954 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1138 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (936 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (655 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (587 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1168 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (576 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (512 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (958 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1009 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (780 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (639 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1100 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (722 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (741 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (621 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (703 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (628 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (682 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (545 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (625 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (751 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (657 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (657 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (553 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (719 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (543 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (616 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1968 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (525 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (975 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (943 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (626 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (552 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1373 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2147 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1693 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1135 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (597 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (718 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (829 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (580 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (936 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (714 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (953 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (647 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1806 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (619 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (546 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2066 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2622 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (736 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (848 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (624 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (690 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (801 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (801 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (875 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (952 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (685 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (853 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (587 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (636 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2209 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2644 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (542 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (626 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (571 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (602 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (757 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (685 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1290 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1124 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1496 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (580 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (576 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (585 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (735 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (965 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (716 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (582 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1149 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (619 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (697 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (964 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (584 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (559 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (532 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (579 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (528 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1308 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (990 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (654 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (699 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (529 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (750 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (961 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1064 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1151 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (607 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (680 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (627 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (552 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (872 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (635 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1270 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (581 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1050 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (549 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (643 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (823 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (571 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (731 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (590 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1079 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (792 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (816 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (826 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (979 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (681 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (991 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1384 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (930 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1041 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (865 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (874 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (661 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (861 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1330 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1113 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1033 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (542 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (615 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (859 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (581 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2203 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (542 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1755 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1063 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (635 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (628 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (710 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (884 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (831 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (855 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1080 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1327 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (918 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (705 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (734 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (592 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (531 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (950 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1088 &amp;gt; 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (697 &amp;gt; 512). Running this sequence through the model will result in indexing errors
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;DONE.
    69,526 comments
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Also retrieve the labels as a list.&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Get the labels from the DataFrame, and conver from booleans to ints.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;7,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; positive (contains attack)&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;7,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; negative (not an attack)&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;  8,079 positive (contains attack)
 61,447 negative (not an attack)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;   Min length: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; tokens&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lengths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;   Max length: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; tokens&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lengths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Median length: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; tokens&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lengths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;   Min length: 2 tokens
   Max length: 9,861 tokens
Median length: 52.0 tokens
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sns&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;darkgrid&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Increase the plot size and font size.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;font_scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;figure.figsize&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Truncate any comment lengths greater than 512.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lengths&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lengths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plot the distribution of comment lengths.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lengths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kde&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Alternatively, you might try using a log scale on the x-axis, but this is tricky.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# See here for one approach:&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# https://stackoverflow.com/questions/47850202/plotting-a-histogram-on-a-log-scale-with-matplotlib?rq=1&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# plt.xscale(&amp;#39;log&amp;#39;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Comment Lengths&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Comment Length&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;# of Comments&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.
  import pandas.util.testing as tm
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;Text(0, 0.5, &amp;#39;# of Comments&amp;#39;)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAoUAAAFjCAYAAABL3HHWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeVzUdf4H8NdwjXKp6EAlHmhyJJd4A7YqZkiZmHJZsKRJbv7Wa9cFOzbTNkvRzQWyIjU1cskC0cU886jEO0FrPPMiA0dJboYBvr8/XL7bOCjfsRlmgNfz8fDxiM/3PZ95w8eJl99TJgiCACIiIiJq1yxM3QARERERmR5DIRERERExFBIRERERQyERERERgaGQiIiIiMBQSERERERgKCQiIjNTWFgIDw8PpKSkmLoVonaFoZCIjKa6uhqffPIJpkyZgiFDhqB///4IDAzE9OnTkZWVhbq6OlO3aLaUSiVSUlJQWFgo+TUpKSnw8PDAqVOnjNiZYZSVlSElJQWHDx82dStE9F8MhURkFFeuXEF4eDiWLFkCuVyOhIQELFq0CPHx8airq8OCBQuwYsUKU7dptpRKJVJTU/Hzzz+buhWjKCsrQ2pqKo4cOWLqVojov6xM3QARtT01NTV46aWXUFhYiJSUFIwdO1Zre0JCAgoKClrFHi0iovaCewqJyOA2bdqES5cu4YUXXtAJhI18fX3x3HPPaY3t3r0b0dHR8Pf3x4ABAxAdHY3du3frvHb06NGIjY3FmTNnEB8fjwEDBmD48OF45513UFdXB7VajXfffRcjRoyAj48PnnvuOVy8eFFrjqysLHh4eCAvLw+pqakYNWoUfH19ERERgZMnTwIAjhw5gpiYGPj7+yM4OBhpaWlNfi+nTp3CzJkzMXToUHh7e+PJJ5/EqlWrdA6Px8bGYvTo0SguLsa8efMwePBg+Pn5Ydq0abh06ZJYl5KSggULFgAA4uLi4OHhAQ8PDyQlJTXzk5fu4MGDmDp1KgYNGgQfHx+MHz8eGzdu1Klr/FlfvHgRCQkJGDBgAAYOHIhZs2ZBpVLp1J85cwZTp06Fv78/hg4disTERJSUlGj1f/jwYYSEhAAAUlNTxe9v9OjROvPt3bsXkyZNgo+PD4KDg/Huu+/q/FzPnz+PWbNmYcSIEfD29kZQUBBiY2Oxb98+A/ykiNoPy4ULFy40dRNE1LYsW7YM169fx9KlS9GpUydJr8nIyEBiYiJsbW0RFxeHwYMH4+TJk8jIyICzszO8vb3F2nXr1qGqqgqbNm3C0KFDMX78eNTV1eHLL79EbW0tMjIy8MsvvyAiIgKPPfYYdu7cif379+O5556DTCYDcOfw7J49e3DhwgX89NNPiIyMxKBBg7B//35kZWWhb9+++Otf/4onnngCoaGhuHnzJrKystCzZ094enqKvezbtw/Tpk0DAERHR2Ps2LGQyWRYv349Lly4gHHjxom12dnZKC4uRm5uLh5++GE8++yz6N27N7Zt24Zvv/0WMTExsLCwQKdOnSAIAn744QfMmDEDkZGReOKJJxAUFISHH374nj/DI0eO4MiRI4iMjISLi8s96zIzMzF37lx07doVkZGRGDVqFMrKyrBmzRpUVVUhODhY62ddXV2Nzz//HH5+fpgwYQKcnJywZcsWnD17FhMmTBBrL1++jKioKPzyyy9iAD5z5gwyMzOhUqng5eWFMWPGoEOHDnBxccG3336LJ554AjNmzMATTzyBkJAQ9OnTB2VlZVi/fj2qq6uxefNmPPXUU3jyySdRVlaGnJwcyOVyDBo0CADw66+/IiIiAtevX0dUVBTGjx8PT09PlJeXo7a2FsOGDZP094+IAAhERAY2ZMgQISAgQHL97du3BX9/f2HMmDFCeXm5OF5eXi6EhIQI/v7+QmlpqTg+atQowd3dXdi2bZvWPBMnThQ8PDyEGTNmCA0NDeL4unXrBHd3d+HAgQPi2Jdffim4u7sL4eHhglqtFsd3794tuLu7C4899phQUFAgjqvVaiEoKEiIjIwUx2pqaoTAwEBhypQpgkaj0epl7dq1gru7u3Do0CFx7Pnnnxfc3d2Fjz76SKs2PT39nv399vXN+de//iW4u7tr9X234uJiwdvbW5g3b57OtsWLFwuenp7C1atXxbHGn3Vubq5W7cKFCwV3d3fh4sWL4tisWbMEd3d34dixY1q1s2fPFtzd3YXExERx7Nq1a4K7u7vwr3/9S6ePxm1+fn7CtWvXxPGGhgbhqaeeEoKCgsSxxvW6uz8i0h8PHxORwVVUVMDOzk5y/XfffYeqqirExsbC3t5eHLe3t0dsbCyqqqpw8OBBrde4uLho7YUDgICAAAiCgNjYWHGPIABxr9KVK1d03jsmJgY2NjY6tb6+vvDx8RHHbWxs4OPjg8uXL2v1ffPmTTz77LMoKytDSUmJ+Ofxxx8Xa37LwsICcXFxWmONe7Oa6s/QduzYgdraWkyePFmr35KSEowePRoNDQ06P2tnZ2eEhYXdt+f6+nocOHAAvr6+GDhwoFbt1KlTH6jXkJAQuLq6il/LZDIMHToUKpUKlZWVAAAHBwcAwDfffIOKiooHeh8iuoMXmhCRwdnb24u/tKVovO1Kv379dLY1jl27dk1r/LdhoVHjoeq7tzk6OgIAbt++rfOaHj16SJqjcdtv52g8T/GVV17RqW108+ZNra+dnZ0hl8u1xjp37nzP/gytsef4+Ph71tzd890/I0C355KSElRVVcHNzU2ntqkxKZp7Xzs7OwwZMgTh4eHIysrC1q1b4e3tjcDAQISFheHRRx99oPclaq8YConI4Pr164ejR4/i2rVrTf5iNwRLS8t7brOwaPogiCAIkmvvN//d8/3tb3+Dl5dXkzXOzs6S522qP0NrfI93331Xp7dGd6+ZqXqW+r7vvvsupk2bhgMHDuDYsWNYu3YtPvjgA7zyyit4/vnnjdYfUVvDUEhEBjd27FgcPXoUmzZtwrx585qtbwwh58+fx/Dhw7W2XbhwQavGnPTu3RsA0LFjRwQGBhp07t8e/jakxp67dOli0J6dnJxga2urdRV1o6bGDP39ubu7w93dHS+++CLKysoQERGB5cuXa11cRET3x3MKicjgIiIi4ObmhjVr1jR5SxkAOH36NDIyMgAAQUFBsLW1xaeffqp1XlhFRQU+/fRT2NraIigoqEV610dwcDC6du2K9PT0Jg/91tTUPPB5bra2tgCA0tLS39Xj3caNGwcbGxukpKSgpqZGZ3vjVbv6srS0xIgRI1BQUIDjx49rbVuzZo1OvaG+v9u3b6OhoUFrzNHREa6urqiuroZarf5d8xO1J9xTSEQG17FjR3z44YdISEjAzJkzERwcjMDAQHTu3BklJSU4fPgwvv32W7z44osA7vwS/+tf/4pFixYhMjISEydOBHDnFi5XrlzBokWLxAsKzImtrS3effddzJw5E6GhoZg0aRJ69eqFsrIy/PTTT9i1axdSU1MxdOhQvef28fGBhYUFPvjgA5SWlsLW1haurq7w8/Nr9rVffvklvvnmG53x/v374w9/+AMWLlyI1157DWFhYXjmmWfQvXt3lJSU4Ny5c9i9ezdyc3ObPKeyOXPmzBHX9fnnn8dDDz2Effv2oaSkBID23sEuXbqgV69eyM3NRY8ePdCtWzd07NixyXsV3s/mzZuxbt06jBkzBr169YKVlRWOHj2Kb7/9FuPGjUOHDh30/j6I2iuGQiIyil69emHz5s3IzMzEjh078MEHH6CqqgqdOnWCt7c33nnnHYwfP16sf+655+Ds7IzVq1eLN4n29PREWloaxowZY6pvo1kjRozAF198gY8++ghbtmzBr7/+CkdHR/Ts2RPx8fHw8PB4oHkfeeQRvP3220hPT8ebb74JjUaDiRMnSgqFTd2EGgCioqLwhz/8AZMmTULv3r2xZs0aZGZmory8HJ07d4abmxtmz54NhULxQD336dMHGRkZePfdd7F+/XrI5XKMHDkSf//73zFmzBidC2ySk5Px9ttv45///Ceqq6vRvXt3vUPh0KFDoVQqsW/fPqhUKlhYWMDV1RWJiYk8n5BITzKhJc5sJiKiduv06dOYNGkS/vKXvyAhIcHU7RDRPfCcQiIiMpi7z1MUBAEff/wxABj8YhwiMiwePiYiIoOZMGEChg0bBnd3d1RXV2Pv3r04duwYwsLCtB5VSETmh4ePiYjIYJYuXYq9e/eiqKgIdXV1cHV1xfjx4zF9+nRYW1ubuj0iug+GQiIiIiLiOYVERERExFBIREREROCFJgbz66+VaGgw/JH4rl3tcevWgz0RgQyP62E+uBbmg2thXrge5sMc18LCQoYuXeya3MZQaCANDYJRQmHj3GQ+uB7mg2thPrgW5oXrYT5a01rw8DERERERMRQSEREREUMhEREREYGhkIiIiIjAUEhEREREYCgkIiIiIjAUEhEREREYComIiIgIDIVEREREBD7RpM2pawDUmrpm6+TWVrDiPwmIiIjovxgK2xi1pg5HlcXN1g32coGVnMtPREREd3BfERERERExFBIRERERQyERERERgaGQiIiIiMBQSERERERgKCQiIiIiMBQSERERERgKiYiIiAgMhUREREQEhkIiIiIiAkMhEREREYGhkIiIiIjAUEhEREREYCgkIiIiIjAUEhEREREYComIiIgIDIVEREREBIZCIiIiIgJDIRERERGBoZCIiIiIwFBIRERERGAoJCIiIiIwFBIRERERGAqJiIiICAyFRERERASGQiIiIiICQyERERERgaGQiIiIiABYmboBMg2ZhQyV6jpJtXJrK1jxnw9ERERtGkNhO6XW1CP/nEpS7WAvF1jJ+VeFiIioLeP+HyIiIiJiKCQiIiIiE4fCy5cvY86cOXj88cfh7++PsLAwfPTRR6itrdWqO3HiBGJiYuDn54egoCC89dZbqK6u1pmvtrYWy5YtQ3BwMHx9fREZGYm8vLwm31vqnERERETtgclOFCsuLkZERAQcHBzw/PPPo1OnTjh27BiWL1+O8+fPY9myZQAApVKJ+Ph4PProo0hKSkJRURHWrFmDwsJCfPDBB1pzJiUlYefOnYiLi0OvXr2QnZ2N6dOnY8OGDRgwYIBYp8+cRERERO2ByUJhTk4OysrK8Nlnn6Ffv34AgKioKKjVamzbtg1vv/02rK2tsWLFCnTu3BkbNmyAnZ0dAMDV1RWvvfYa8vLyMHz4cABAQUEBcnNzsWDBAsTHxwMAwsPD8fTTTyM5ORkZGRnie0udk4iIiKi9MNnh48rKSgBA165dtca7desGKysrWFpaoqKiAgcPHkR4eLgY3gBgwoQJsLW1xVdffSWObd++HdbW1oiIiBDH5HI5Jk+ejOPHj+PGjRsAoNecRERERO2FyULh4MGDAQCvvvoqzpw5g19++QVbtmwRD/laWFjg7NmzqKurg7e3t9ZrbWxs4OXlBaVSKY4plUq4ublpBT0A8PX1hSAIYq0+cxIRERG1FyY7fBwcHIzZs2fjww8/xNdffy2Oz5o1CzNnzgQAqFR37qOnUCh0Xq9QKHDy5Enxa5VKBRcXlybrAIh7CvWZk4iIiKi9MOkdiV1dXTFkyBA88cQT6Ny5M/bt24eUlBQ4OTkhJiYGNTU1AO7sxbubXC4XtwNATU0NrK2tm6wDALVaLdZJnVMfXbvaP9DrpFAoHCTXCiVVcLDv0GydtbWVpDoAsLWVQ+FkK7mHtk6f9SDj4lqYD66FeeF6mI/WtBYmC4W5ubl44403sH37dnEP39ixYyEIApYuXYqwsDB06HAntNx9ixrgTshr3A4AHTp0gEajabIO+F841GdOfdy6VYGGBuGBXns/CoUDVKpyyfVV6jqUVzQfbDUaaXUAUFWlhqq+XnIPbZm+60HGw7UwH1wL88L1MB/muBYWFrJ77sgy2TmFn332Gfr3769zyHf06NGoqqrCmTNnxEO8jYd8f0ulUsHZ2Vn8WqFQiIeI764DINbqMycRERFRe2GyUHjz5k3UN7H3qXFvX319Pdzd3WFlZYXTp09r1dTW1kKpVMLLy0sc8/T0xKVLl8Srmhvl5+eL2wHoNScRERFRe2GyUOjm5obTp0/j6tWrWuO5ubmwtLSEh4cHHBwcMHz4cOTk5GiFvZycHFRVVSE0NFQcCw0NhUajwaZNm8Sx2tpaZGVlISAgQNwjqc+cRERERO2Fyc4pnDZtGg4cOICYmBg899xz6NSpE/bt24cDBw4gOjpavH/h3LlzER0djdjYWERERKCoqAhr167F448/jsDAQHE+Pz8/hIaGIjk5GSqVCj179kR2djauX7+OJUuWaL231DmJiIiI2guZIAiGvzpCooKCAqSkpECpVOL27dvo3r07Jk2ahGnTpsHS0lKsO3bsGJKTk/Hjjz/C3t4eYWFhmDdvHmxtta+IVavVeO+997B161aUlpbCw8MD8+bNazLoSZ1TKnO50KRSXYejyuJm6/zcFcg/p3teZVMGe7nATm7SC9XNhjmeNNxecS3MB9fCvHA9zIc5rsX9LjQxaShsSxgK2wdz/IC3V1wL88G1MC9cD/NhjmthllcfExEREZH5YCgkIiIiIoZCIiIiImIoJCIiIiIwFBIRERERGAqJiIiICAyFRERERAQDhMLTp0/ju+++g1qtNkQ/RERERGQCku9IvHr1ahw9ehQffPCBOPaXv/wF27ZtAwD06NEDn332Gbp162b4LomIiIjIqCTvKczNzcXDDz8sfp2Xl4fc3FyEhYVh7ty5UKlU+Pjjj43SJBEREREZl+Q9hT///DOeffZZ8es9e/ZAoVAgOTkZMpkMv/76K77++mskJSUZpVEiIiIiMh7Jewqrq6shl8vFrw8dOoTAwEDIZDIAQN++fVFc3Pwzd4mIiIjI/EgOhS4uLjh37hyAO3sNL1y4gMGDB4vby8rKYGNjY/gOiYiIiMjoJB8+HjVqFD777DPU19cjPz8fNjY2GDlypLj9/Pnz6N69uzF6JCIiIiIjkxwKZ86cibNnz+Kzzz6DjY0NXnnlFfFK45qaGuzatQuTJ082WqNEREREZDySQ2GnTp2wbt06VFRUQC6Xw9raWmv7p59+qnV1MhERERG1HpLPKUxNTcW5c+dgb2+vEwg7dOgAS0tLbNiwweANEhEREZHx6RUKz549e8/t58+fR1pamkGaIiIiIqKWZbBnH6vValhaWhpqOiIiIiJqQfc9p7CiogJlZWXi17dv38b169d16kpLS7F161aeU9hGySxkqFTXNVsnt7aClcH+mUFEREQt6b6h8JNPPhEPCctkMrz99tt4++23m6wVBAHz5883fIdkcmpNPfLPqZqtG+zlAiu55GuXiIiIyIzc9zf4kCFDANwJfGlpaXjiiSfg4eGhU2dnZwc/Pz8EBAQYp0siIiIiMqpmQ2FjMLx+/Tqio6Ph5+fXIo0RERERUcuRfKxvyZIlxuyDiIiIiExI7xPALl++jCtXruDXX39tcnt4ePjvboqIiIiIWpbkUHjz5k0kJibi4MGDAO6cZ3g3mUzGUEhERETUCkkOhYsWLcLBgwcRExODYcOGoXPnzsbsi4iIiIhakORQePDgQURHR+Pvf/+7MfshIiIiIhOQfKvhhoYGeHp6GrMXIiIiIjIRyaFw0KBBOHPmjDF7ISIiIiITkRwKk5KSsGvXLuzYscOY/RARERGRCUg+p3DhwoWws7PDnDlz4OzsjB49esDCQjtTymQyrFu3zuBNEhEREZFxSQ6FhYWFAICHH34YwJ0nnBARERFR2yA5FH799dfG7IOIiIiITEjyOYVERERE1Hbp/Zi7wsJC5OXl4ebNmxg/fjxcXV1RW1uLmzdvolu3brCxsTFGn0RERERkRHqFwmXLluGTTz5BfX09ZDIZ/P39xVD41FNPYfbs2YiPjzdSq0RERERkLJIPH//73//G6tWrMWXKFKxZs0br2cf29vYYPXo09u7da5QmiYiIiMi4JO8p/Oyzz/DEE0/g1Vdfxa+//qqz3cPDA0ePHjVoc0RERETUMiTvKbx8+TICAwPvub1Lly5NhkUiIiIiMn+SQ6FcLkd1dfU9t1+/fh2Ojo4GaYqIiIiIWpbkUOjr64tdu3Y1uU2tViMnJwcBAQF6N1BQUICEhAQMHjwYAwYMwDPPPIOsrCytmj179mDixInw8fHByJEjkZqairq6Op25ysrK8Prrr2PYsGHw9/dHXFwclEplk+8rdU4iIiKi9kByKJw2bRpOnjyJ+fPn4+zZswCAmzdv4ptvvkFsbCyKi4sxdepUvd58//79mDJlCurq6jB79mwkJiYiMDAQv/zyi1bNzJkz0alTJ7z++usYM2YM0tLSsGTJEq25GhoakJCQgNzcXDz//POYP38+bt26hdjYWFy9elXnfaXMSURERNReSL7QJDAwEAsXLsQ//vEP/Oc//wEA/O1vfwMAWFtbY/HixRgwYIDkNy4vL8eCBQsQHR2N11577Z51S5cuxWOPPYbVq1fD0tISAGBnZ4ePPvoIsbGx6N27NwBg+/bt+P7775GWloYxY8YAAMaNG4cnn3wSqampWLp0qd5zEhEREbUXej3RJCoqCnv27MErr7yCmJgYREVFITExEbt27cKzzz6r1xtv3boVZWVlmD17NgCgoqJC6zY3AHDhwgVcuHABUVFRYngDgClTpqChoQE7d+4Ux3bs2AFnZ2eEhISIY05OThg3bhx2794NjUaj95xERERE7YXeTzRRKBSIjY393W+cl5eHPn36YP/+/Vi2bBmKiorg6OiIqKgozJ07F5aWlvjxxx8BAN7e3lqvdXFxwUMPPSRuBwClUon+/ftDJpNp1fr4+CAzMxNXr15F37599ZqTiIiIqL0w2bOPr1y5gqKiIiQlJWHixIlISUnBmDFjkJ6ejnfeeQcAoFKpANwJondTKBS4ceOG+LVKpYKzs7NOXeNYY60+cxIRERG1F3rtKTxx4gQyMjJw5coV3L59W+dwr0wmw+7duyXNVVVVhdLSUvzlL39BQkICAGDs2LGoqqrCxo0b8ac//Qk1NTUA0OTzlO++RU5NTU2TdY1jjXPpM6c+una1f6DXSaFQOEiuFUqq4GDfodk6a2srSXX61NrayqFwspU0Z2umz3qQcXEtzAfXwrxwPcxHa1oLyaHw888/xxtvvAFra2u4ubnh4Ycf/l1v3KHDnZDx9NNPa42PHz8e27dvx6lTp8Sa2tpander1Wpxe+N8TdU1jjXW6jOnPm7dqkBDg9B8oZ4UCgeoVOWS66vUdSivqGm2TqORVqdPbVWVGqr6eklztlb6rgcZD9fCfHAtzAvXw3yY41pYWMjuuSNLcij84IMP4OXlhY8//hhOTk6/uymFQoHz58+jW7duWuONX5eWloqHeJs6NKxSqbSudr7Xod/GscbX6zMnERERUXsh+ZzCW7duYdKkSQYJhADQv39/AEBxcbHWeFFREYA7Vw57eXkBAE6fPq1VU1xcjKKiInE7AHh6euKHH37QOaRdUFAAW1tb9OzZEwD0mpOIiIiovZAcCvv27YuysjKDvXFoaCgA4IsvvhDHBEHApk2bYGtrC39/f/Tr1w99+vRBZmYm6n9zWHLjxo2wsLDA2LFjtea7ceMG9uzZI46VlJRg+/btCAkJgbW1NQDoNScRERFRe2G5cOHChVIKu3TpgrS0NDz11FOwt//9F1U4OzujsLAQGRkZKCoqQlFREdLS0nDgwAHMmTMHw4YNAwB0794dn3zyCU6cOIHa2lpkZ2dj7dq1iIqKwsSJE8X5+vTpg++++w6ZmZnQaDQ4f/48Fi9ejPLycqxYsQKdO3cWa6XOqY/q6loIhj+lEHZ2clRV6Z7/eC+a+gZcv1nZbN1DXe1QfKtK0pxSa7sr7GFjZbIL2luEvutBxsO1MB9cC/PC9TAf5rgWMpkMtra6F9sCgEy4+3jrfeTk5GDx4sUICQlB9+7dYWGhHQBkMhlmzpwpubHa2lq8//772Lx5M27evAlXV1fEx8cjOjpaq2737t1ITU3FxYsX4eTkhEmTJuHll1+GlZX2KZGlpaVYunQpdu/eDbVaDR8fHyQlJYmHqh9kTqnM5UKTSnUdjiqLm63zc1cg/5xK0pxSawd7ucBO/mA/v9bCHE8abq+4FuaDa2FeuB7mwxzX4n4XmkgOhZcuXcKLL76In3/++Z41MpkMSqXywbps5RgKgSH9H4Ig4Wcgt7ZCa92haI4f8PaKa2E+uBbmhethPsxxLQxy9fGbb76JkpISvPrqqxg0aBAcHR0N1iC1DWpNveQ9ilZtfI8iERFRayP5N/PJkycxbdo0gzzijoiIiIjMi+SDePb29ga7HQ0RERERmRfJoXDcuHHYuXOnMXshIiIiIhORHAqjo6NRWVmJl19+GXl5ebh27RquX7+u84eIiIiIWh/J5xQ+9dRTkMlkOH36NPbu3XvPuvZ69TERERFRayY5FM6cORMymcyYvRARERGRiUgOhX/+85+N2QcRERERmVArvYUwERERERmS3ncQvnz5Mq5cuYJff/21ye3h4eG/uykiIiIialmSQ+GNGzeQlJSEvLw8AEBTT8eTyWQMhUREREStkORQ+Pe//x2HDx/GH//4Rz7mjoiIiKiNkRwKDx06hLi4OCQmJhqzHyIiIiIyAckXmtja2qJnz57G7IWIiIiITERyKBw5cqR4PiERERERtS2SQ2FSUhIKCwvx9ttv49q1a01eaEJERERErZPkcwodHR0RHh6OJUuWYMOGDU3WyGQy/PjjjwZrjtommYUMleq6Zuvk1law4p00iYiIWoTkUJieno4VK1aga9eu8PX1RadOnYzZF7Vhak098s+pmq0b7OUCK7net9IkIiKiByD5N+6nn36KIUOG4OOPP4a1tbUxeyIiIiKiFib54FxpaSnGjRvHQEhERETUBkkOhZ6envjll1+M2QsRERERmYjkUDhnzhxkZmbi1KlTxuyHiIiIiExA8jmFOTk5cHFxQVRUFPz9/dGjRw9YWGhnSplMhrffftvgTRIRERGRcUkOhdnZ2eJ/nzhxAidOnNCpYSgkIiIiap0kh8IzZ84Ysw8iIiIiMmw+np4AACAASURBVCHeGpiIiIiIpO8pbCQIAn788Udcu3YNANCjRw889thjkMlkBm+OiIiIiFqGXqHwwIEDePPNN3H9+nWt8e7du+ONN97AiBEjDNocEREREbUMyaHw+PHjePnll9GxY0fExcXh0UcfBQBcuHAB2dnZ+NOf/oT169cjICDAaM0SERERkXFIDoXvv/8+unXrhs8//xzOzs5a26ZNm4bIyEikpaVh9erVBm+SgLoGQK2pa7auQWiBZoiIiKjNkRwK8/PzMXXqVJ1ACADOzs6IiIjA2rVrDdoc/Y9aU4ejyuJm6/zcFS3QDREREbU1kq8+1mg0sLOzu+d2e3t7aDQagzRFRERERC1Lcijs27cvtm3bhro63UOYdXV1+Oqrr9C3b1+DNkdERERELUNyKIyJiUF+fj7i4+Oxb98+XLt2DdeuXcPevXsRHx+P/Px8xMTEGLNXIiIiIjISyecURkRE4PLly1izZg2OHz+us33atGmIiIgwaHNERERE1DL0uk/h/PnzMXnyZOzZsweFhYUA7ty8evTo0XBzczNKg0RERERkfHo/0cTNzQ0vvviiMXohIiIiIhNp9pzCjRs3Ytu2bfet2bZtGzIzMw3WFBERERG1rPuGwl27dmHRokXo1KnTfSdxdHTEwoULsW/fPkP2RkREREQt5L6hcOvWrfDz80NQUNB9JwkODkZAQACys7MN2hwRERERtYz7hsL8/Hz84Q9/kDTRiBEjkJ+fb5CmiIiIiKhl3TcU3rp1Cy4uLpImcnZ2xq1bt35XM+np6fDw8MCECRN0tp04cQIxMTHinsu33noL1dXVOnW1tbVYtmwZgoOD4evri8jISOTl5TX5flLnJCIiImrr7hsKO3bsiIqKCkkTVVRUoEOHDg/ciEqlwqpVq2Bra6uzTalUIj4+Hmq1GklJSZg8eTIyMzMxd+5cndqkpCSsW7cOzzzzDF599VVYWFhg+vTp+P777x94TiIiIqK27r63pOnVqxeOHj2KuLi4Zic6duwYevXq9cCNLF++HN7e3hAEAWVlZVrbVqxYgc6dO2PDhg3i85ddXV3x2muvIS8vD8OHDwcAFBQUIDc3FwsWLEB8fDwAIDw8HE8//TSSk5ORkZGh95xERERE7cF99xSOHDkSX3/9tc5etrudPHkSu3fvxqhRox6oiYKCAmzZsgULFizQ2VZRUYGDBw8iPDxcDG8AMGHCBNja2uKrr74Sx7Zv3w5ra2utJ6vI5XJMnjwZx48fx40bN/Sek4iIiKg9uG8ojIuLQ5cuXZCQkIDPP/8ctbW1Wttra2uxadMmJCQkoGvXroiNjdW7AUEQsHjxYoSHh8PLy0tn+9mzZ1FXVwdvb2+tcRsbG3h5eUGpVIpjSqUSbm5uWkEPAHx9fSEIglirz5xERERE7cF9Dx87Ojri/fffx4wZM/DGG2/grbfegpubG+zt7VFZWYmffvoJGo0GXbp0wfvvvw9HR0e9G9i8eTMuXLiAtLS0JrerVCoAgEKh0NmmUChw8uRJrdqmLoxpfG3jnkJ95iQiIiJqD5p9zJ2vry+2bNmCjz/+GDt37sTZs2fFbY888gjGjh2LF198Ed26ddP7zSsqKrB8+XIkJCTA2dm5yZqamhoAd/bi3U0ul4vbG2utra2brAMAtVqt95xSde1qr/drpFIoHCCUVMHBvvkLeaytrQxaZ4w5pdbZ2sqhcNK98MjUFAoHU7dA/8W1MB9cC/PC9TAfrWktJD37uFu3bkhKSkJSUhIqKytRUVEBe3t7ncO0+lq1ahWsra3xwgsv3LOm8Yrmuw9dA3dC3m+veO7QoQM0Gk2TdcD/wqE+c0p161YFGhoEvV/XHIXCASpVOarUdSivaD6sajSGrTPGnFLrqqrUUNXXS+qxpTSuB5ke18J8cC3MC9fDfJjjWlhYyO65I0tSKPwtOzu73x0GgTuHctetW4fZs2fj5s2b4rharYZGo0FhYSEcHBzEQ7yNh3x/S6VSae1hVCgU4iHiu+sAiLX6zElERETUHtz3QhNjunXrFjQaDZKTkxESEiL+yc/Px8WLFxESEoL09HS4u7vDysoKp0+f1np9bW0tlEql1sUpnp6euHTpEiorK7VqG5+04unpCQB6zUlERETUHui9p9BQXF1dm7y45L333kNVVRVeeeUV9O7dGw4ODhg+fDhycnLw0ksviXspc3JyUFVVhdDQUPG1oaGhWLNmDTZt2iTep7C2thZZWVkICAgQL0LRZ04iIiKi9sBkodDBwQFjxozRGV+3bh0sLS21ts2dOxfR0dGIjY1FREQEioqKsHbtWjz++OMIDAwU6/z8/BAaGork5GSoVCr07NkT2dnZuH79OpYsWaL1PlLnJNORWchQqa5rtk5ubQUrk+3zJiIiahtMFgr10b9/f6xduxbJyclYsmQJ7O3tERkZiXnz5unULl26FO+99x5ycnJQWloKDw8PfPTRRxg4cOADz0mmodbUI/+c7nmfdxvs5QIreav4q0xERGS27vmbNDU1FWPHjoW7uzsA4Pr163BycvpdzzeWYsOGDU2ODxo0CP/+97+bfb1cLkdiYiISExObrZU6JxEREVFbd8+DbqmpqVr3JAwJCcGuXbtapCkiIiIialn3DIWOjo4oKysTvxYEw9+Dj4iIiIjMwz0PH3t5eWH16tWoq6tDp06dAADHjh1DfTM3Ew4PDzdsh0RERERkdPcMhQsWLMD//d//iVftymQyZGZmIjMz856TyWQyhkIiIiKiVuieodDT0xM7duzAtWvXoFKpEBsbixkzZvB2LURERERt0H3v42FpaYnevXujd+/eGDx4MIYOHYohQ4a0VG9ERERE1EIk39ztXreKISIiIqLWT687/jY0NCA7Oxu7du1CYWEhgDuPqxs7dizCw8NhYcHHShARERG1RpJDYU1NDaZPn45jx45BJpNBoVAAAA4cOID9+/dj8+bNSE9Ph1wuN1qzRERERGQcknftrVq1CkePHsULL7yAvLw87N+/H/v378ehQ4cwdepUHDlyBKtWrTJmr0RERERkJJJD4bZt2zBu3Dj87W9/E+9bCNy5yfX8+fMxbtw45ObmGqVJIiIiIjIuyaGwqKjovlceDx48GEVFRQZpioiIiIhaluRQ6OjoiKtXr95z+9WrV+Ho6GiQpoiIiIioZUkOhYGBgcjIyMA333yjs+3bb7/Fxo0bERwcbNDmiIiIiKhlSL76eM6cOfj222+RkJAALy8v9OvXDwBw/vx5KJVKdOnSBbNmzTJao0RERERkPJJDYffu3fHll19i+fLl2Lt3L3788UcAgJ2dHZ566inMmzcPjzzyiNEaJSIiIiLj0evm1Y888giWL18OQRBQUlICAHBycoJMJjNKc0RERETUMvQKhY1kMhm6du1q6F6IiIiIyEQeKBQSmROZhQyV6rpm6+TWVrDikxiJiIiaxFBIrZ5aU4/8c6pm6wZ7ucBKzr/yRERETeF+EyIiIiJiKCQiIiIihkIiIiIiAkMhEREREUGPUFhRUYG4uDjxptVERERE1HZIDoUajQZHjhxBaWkpAKCqqgoLFizAxYsXjdYcEREREbWM+4bCWbNm4ZNPPkF+fj5qa2u1tqnVamzevBk3btwwaoNEREREZHz3vWlbdXU10tLSUF5eDisrK8hkMnz11VewtbWFq6srBEFoqT6JiIiIyIjuGwrT09MhCALOnj2L7777DsuWLcPWrVvx+eefw9bWFjKZDPv27UOnTp3g5eXFZyCTWeOTT4iIiO6t2cc7yGQyeHp6wsXFBcuWLcP7778PJycnfP3111i5ciUyMjKwfv162NvbIyAgAB9++GFL9E2kNz75hIiI6N7u+5tv2rRpGDhwIAYOHIgePXoAuBMSPTw8oFAosHLlSnz44YdwdHTE0aNHcezYsRZpmoiIiIgM676h0MbGBhs2bMC//vUvWFpaQiaTITs7GwDQp08fAIClpSV8fHzg4+ODqVOnGr9jIiIiIjK4+4bCVatWAQAuX76M7777DosXL8bevXuRk5MDuVwOmUyGnTt3okOHDvD29oaVFQ+5EREREbVGkk6n7927N8LCwgAAK1euxFdffYWZM2dCEARkZ2cjOjoagwcPRnx8vDF7JSIiIiIjeaBrLN3c3BAREQEAeP/995Gbm4v58+fDycnJoM0RERERUcuQfLxXLpdj4sSJcHZ21tnWt29f9O3bF1OmTDFoc0RERETUMiSHQltbWyxZskT8+n4hkYiIiIhalwe+MuTukEhERERErRef20BEREREDIVERERExFBIRERERDBhKCwoKMCbb76JsLAw+Pv7Y+TIkZg7dy6uXLmiU3vixAnExMTAz88PQUFBeOutt1BdXa1TV1tbi2XLliE4OBi+vr6IjIxEXl5ek+8vdU4iIiKi9sBkofDjjz/Grl27EBgYiFdffRWRkZE4cuQIwsPDcfHiRbFOqVQiPj4earUaSUlJmDx5MjIzMzF37lydOZOSkrBu3To888wzePXVV2FhYYHp06fj+++/16rTZ04iIiKi9sBkz6WLj49HcnIybGxsxLGwsDCMHz8e6enpeOeddwAAK1asQOfOnbFhwwbY2dkBAFxdXfHaa68hLy8Pw4cPB3Bnz2Nubi4WLFggPlklPDwcTz/9NJKTk5GRkSG+j9Q5qX2SWchQqa5rcptQUoWq32yTW1vBiidhEBFRG2CyX2cBAQFagRC48zi9fv36iXsKKyoqcPDgQYSHh4vhDQAmTJgAW1tbfPXVV+LY9u3bYW1tLT5pBbhzL8XJkyfj+PHjuHHjht5zUvuk1tTjqLK4yT8nzt7Q+lqtaTo8EhERtTZmtY9DEATcvHkTXbp0AQCcPXsWdXV18Pb21qqzsbGBl5cXlEqlOKZUKuHm5qYV9ADA19cXgiCItfrMSURERNRemOzwcVO2bNmC4uJi8dw+lUoFAFAoFDq1CoUCJ0+eFL9WqVRwcXFpsg6AuKdQnzn10bWr/QO9TgqFwgFCSRUc7Ds0W2ttbWXQOmPM2drrfrvN1lYOhZNts3OScSgUDqZugf6La2FeuB7mozWthdmEwosXL2LRokUYOHAgJkyYAACoqakBAJ3DzMCdQ8ON2xtrra2tm6wDALVarfec+rh1qwINDcIDvfZ+FAoHqFTlqFLXobyi+d40GsPWGWPO1lznYN9Ba1t1TS0uF6qbnZPnHhpe42eDTI9rYV64HubDHNfCwkJ2zx1ZZhEKVSoVXnrpJXTq1AkrV66EhcWd354dOtzZI1NbW6vzGrVaLW5vrNVoNE3WAf8Lh/rMSdQctaYe+edUzdYN9nKBldwsPm5ERERNMvlvqfLyckyfPh3l5eXYuHGj1mHdxv9uPOT7WyqVCs7Ozlq1jYeI764DINbqMycRERFRe2HSA1pqtRozZszA5cuX8eGHH6JPnz5a293d3WFlZYXTp09rjdfW1kKpVMLLy0sc8/T0xKVLl1BZWalVm5+fL27Xd04iIiKi9sJkobC+vh5z5szByZMnsXLlSvj7++vUODg4YPjw4cjJydEKezk5OaiqqkJoaKg4FhoaCo1Gg02bNoljtbW1yMrKQkBAgHgRij5zEhEREbUXJjt8/M477+Drr7/GqFGjcPv2beTk5Ijb7OzsMGbMGADA3LlzER0djdjYWERERKCoqAhr167F448/jsDAQPE1fn5+CA0NRXJyMlQqFXr27Ins7Gxcv34dS5Ys0XpvqXMSERERtRcmC4VnzpwBAOzduxd79+7V2ta9e3cxFPbv3x9r165FcnIylixZAnt7e0RGRmLevHk6cy5duhTvvfcecnJyUFpaCg8PD3z00UcYOHCgVp0+cxIRERG1ByYLhRs2bJBcO2jQIPz73/9utk4ulyMxMRGJiYkGm5OIiIjo96hrgKQnYJn69mUmv/qYiIiIqC1Ta+pwVFncbJ2pb1/G2+kSEREREUMhERERETEUEhEREREYComIiIgIvNCEqEXILGSoVJv/lWdERNR+MRQStQC1ph7553Sft303U195RkRE7Rf3SRARERERQyERERERMRQSEREREXhOIZFZ4QUpRERkKgyFRGaEF6QQEZGpcF8DERERETEUEhEREREPHxO1Sjz3kIiIDI2hkKgV4rmHRERkaNyHQERERETcU0jUlvEwMxERScVQSNSG8TAzERFJxd8CRMQ9ikRExFBIRNyjSEREvNCEiIiIiMBQSERERERgKCQiIiIiMBQSERERERgKiYiIiAgMhUREREQE3pKGiPQg9X6GAGBtZQVNHe99SETUWjAUEpFkUu9nCAB+7gre+5CIqBXhv8+JiIiIiKGQiIiIiHj4mIhMjM9dJiIyDwyFRGRSUs9THNL/Iag1gvi1UFKFqibCJC9wISJ6MAyFRNQq3B0eHew7oLyiRqeOF7gQET0Y/h+RiNolHrYmItLGUEhE7dKDHra+F4ZHImrtGAqJiO7D0OERYIAkIvPEUEhEZAD63Nibex+JyBwxFBIRtTBD733kFddEZAgMhUREZkpqeJR6xTX3UBLR/TAUEhG1E1JDJm/XQ9Q+tetPfW1tLVauXImcnByUlZXB09MTc+fOxfDhw03dGhGRyUi9XU/jYet73Uj87jopuJeSyHTadShMSkrCzp07ERcXh169eiE7OxvTp0/Hhg0bMGDAAFO3R0RkEvoetr7XjcTvrpPC0OdRmqqO4ZZao3YbCgsKCpCbm4sFCxYgPj4eABAeHo6nn34aycnJyMjIMG2DRETtkKHPozRVnaHDLcCgScbXbkPh9u3bYW1tjYiICHFMLpdj8uTJ+Oc//4kbN27A2dnZhB0SEVFrZehwC0gPmrLbVXod/jfXOn1CcF0DoNZwD+7v1W5DoVKphJubG+zs7LTGfX19IQgClEqlXqHQwkJm6Ba15raytIBtB+tmaw1dZ4w5W3NdR7kV6uusJdWaqkdzqGuJ9757LVrqfX9vXWvoUd+6e62FvvMZs8fWXgcA9Q0ClJdKmq3zdVfgBwl1Xm5OkuYzVZ2fuwL1ddJuCN8gQNL3LHVOKytL1NXV/+66m7eroa5rgIWF9L8PxswTwP3zikwQBGk/8Tbm6aefhouLC1avXq01fuHCBTz11FN46623tPYiEhEREbVl7XYnak1NDaytdVO7XC4HAKjV6pZuiYiIiMhk2m0o7NChAzQajc54YxhsDIdERERE7UG7DYUKhQI3btzQGVep7pzwy4tMiIiIqD1pt6HQ09MTly5dQmVlpdZ4fn6+uJ2IiIiovWi3oTA0NBQajQabNm0Sx2pra5GVlYWAgAC4uLiYsDsiIiKiltVub0nj5+eH0NBQJCcnQ6VSoWfPnsjOzsb169exZMkSU7dHRERE1KLa7S1pgDsXlbz33nvYunUrSktL4eHhgXnz5iEwMNDUrRERERG1qHYdComIiIjojnZ7TiERERER/Q9DIRERERExFJqj2tpaLFu2DMHBwfD19UVkZCTy8vJM3VabcuPGDSQnJyM2NhYDBgyAh4cHDh8+3GTtnj17MHHiRPj4+GDkyJFITU1FXRMPcy8rK8Prr7+OYcOGwd/fH3FxcVAqlcb+Vlq9goICvPnmmwgLC4O/vz9GjhyJuXPn4sqVKzq1J06cQExMDPz8/BAUFIS33noL1dXVOnX8DD2YU6dOYebMmRg1ahR8fX0RFBSEadOm4cSJEzq1XIuWl56eDg8PD0yYMEFnG9fDuA4fPgwPD48m/1y8eFGrtjWvBc8pNEPz5s3Dzp07ERcXh169eiE7OxunT5/Ghg0bMGDAAFO31yYcPnxY/Pk6OTnh+++/x/r16zF06FCtuv379+Oll17CsGHDEBYWhnPnziEjIwNTpkzB66+/LtY1NDRgypQpOHfuHKZOnYouXbrgs88+Q3FxMbKystCzZ8+W/hZbjVmzZuHEiRMIDQ2Fh4cHVCoVMjIyUFVVhS+++AJ9+/YFACiVSkRFReHRRx9FREQEioqKsGbNGgQFBeGDDz7QmpOfoQezbds2bNmyBb6+vlAoFCgvL8fWrVtx9uxZpKenIygoCADXwhRUKhWefPJJCIKAnj17IicnR9zG9TC+xt8Zf/zjH9G/f3+tbSEhIbC3twfQBtZCILOSn58vuLu7C2vXrhXHampqhDFjxghTpkwxXWNtTHl5uVBSUiIIgiDs2rVLcHd3Fw4dOqRTFxYWJkycOFGoq6sTx1asWCF4enoKly5dEsdyc3MFd3d3YdeuXeLYrVu3hEGDBgnz58833jfSBhw/flxQq9VaY5cuXRK8vb2FxMREcezFF18URowYIVRUVIhjn3/+ueDu7i4cPHhQHONnyLCqqqqEwMBAISEhQRzjWrS8xMREITY2Vnj++eeFZ555Rmsb18P4Dh06pPP/+Ka09rXg4WMzs337dlhbWyMiIkIck8vlmDx5Mo4fP97ko/lIf/b29ujSpct9ay5cuIALFy4gKioKlpaW4viUKVPQ0NCAnTt3imM7duyAs7MzQkJCxDEnJyeMGzcOu3fvbvI523RHQEAAbGxstMZ69+6Nfv36iYdlKioqcPDgQYSHh8POzk6smzBhAmxtbfHVV1+JY/wMGVbHjh3h5OSEsrIyAFwLUygoKMCWLVuwYMECnW1cj5ZXUVHR5ClEbWEtGArNjFKphJubm9ZfKADw9fWFIAg8R60F/fjjjwAAb29vrXEXFxc89NBD4nbgzrr1798fMplMq9bHxweVlZW4evWq8RtuQwRBwM2bN8XgfvbsWdTV1emshY2NDby8vLQ+F/wM/X4VFRUoKSnBTz/9hBUrVuDcuXMYPnw4AK5FSxMEAYsXL0Z4eDi8vLx0tnM9Wtb8+fMxcOBA+Pn5YerUqTh79qy4rS2sRbt9oom5UqlUTT5iT6FQAAD/JdeCVCoVgP/97H9LoVBorYVKpcKwYcN06pydnQHcWbfGc+OoeVu2bEFxcTHmzp0LoPm1OHnypPg1P0O/3yuvvIIdO3YAAKytrREdHY0ZM2YA4Fq0tM2bN+PChQtIS0trcjvXo2VYW1vjySefxOOPP44uXbrg7NmzWLNmDaZMmYIvvvgCbm5ubWItGArNTE1NDaytrXXG5XI5gDtPYaGWUVNTAwA6hzaBO+vx26vJampqmqxrHGuci5p38eJFLFq0CAMHDhSvsmxuLX778+Vn6PebOXMmoqKiUFRUhJycHNTW1kKj0cDGxoZr0YIqKiqwfPlyJCQkiP/AvBvXo2UEBAQgICBA/DokJASjR4/GpEmTkJqaiuXLl7eJteDhYzPToUOHJs8/a/wL0vgXhoyvQ4cOAO7cNuBuarVa3N5Y21Rd49hva+neVCoVXnrpJXTq1AkrV66EhcWd/0Xpuxb8DP0+Hh4eCAoKwqRJk7B69Wr88MMP4vlsXIuWs2rVKlhbW+OFF164Zw3Xw3Q8PT0xfPhwHDp0CEDbWAuGQjNz92HJRo27pe/1r0UyvMbd+I0/+99SqVRaa3GvdWsc47o1r7y8HNOnT0d5eTk+/vhjrUMwhlgLfoYejLW1NUJCQrBz507U1NRwLVrIjRs3sG7dOkyZMgU3b95EYWEhCgsLoVarodFoUFhYiNLSUq6HiT388MMoLS0F0Db+P8VQaGY8PT1x6dIlVFZWao3n5+eL26llNJ7Uffr0aa3x4uJiFBUVaZ307enpiR9++AHCXbf9LCgogK2tLe9T2Ay1Wo0ZM2bg8uXL+PDDD9GnTx+t7e7u7rCystJZi9raWiiVSp214GfIsGpqaiAIAiorK7kWLeTWrVvQaDRITk5GSEiI+Cc/Px8XL15ESEgI0tPTuR4mdu3aNfGCuLawFgyFZiY0NBQajQabNm0Sx2pra5GVlYWAgIAmT0wl4+jXrx/69OmDzMxM1NfXi+MbN26EhYUFxo4dK46Fhobixo0b2LNnjzhWUlKC7du3IyQkpMlzR+iO+vp6zJkzBydPnsTKlSvh7++vU+Pg4IDhw4cjJydH63+iOTk5qKqqQmhoqDjGz9CDKykp0RmrqKjAjh078PDDD6Nr165cixbi6uqKtLQ0nT/9+vVD9+7dkZaWhvDwcK5HC2nqs3Hs2DEcPnwYwcHBANrG/6f4RBMzNHv2bOzZswd//OMf0bNnT/Eu5+vWrcPAgQNN3V6b8f777wO4c2HDf/7zH0yaNAmurq5wdHTE888/DwDYu3cv/vSnP+k80SQqKgoLFy4U56qvr8eUKVNw/vx58YkmGzduxC+//IKsrCz06tXLFN9iq/CPf/wD69evx6hRozBu3DitbXZ2dhgzZgwA4IcffkB0dDT69esnPilg7dq1GDp0KNLT07Vex8/Qg4mLi4NcLseAAQOgUCjEv79FRUVYsWIFwsLCAHAtTCk2NhZlZWVaTzThehhfXFwcOnbsiAEDBqBLly44f/48MjMz4eDggC+++AKPPPIIgNa/FgyFZkitVuO9997D1q1bUVpaCg8PD8ybNw+BgYGmbq1N8fDwaHK8e/fu+Prrr8Wvd+/ejdTUVFy8eBFOTk6YNGkSXn75ZVhZaV+8X1paiqVLl2L37t1Qq9Xw8fFBUlKSziORSFtsbCyOHDnS5La71+LYsWNITk7Gjz/+CHt7e4SFhWHevHmwtbXVeh0/Qw/miy++QE5ODi5cuICysjI4ODjA398fU6dOxZAhQ7RquRam0VQoBLgexrZ+/Xps3boVV69eRUVFBZycnBAcHIw///nPYiBs1JrXgqGQiIiIiHhOIRERERExFBIRERERGAqJiIiICAyFRERERASGQiIiIiICQyERERERgaGQiIiIiMBQSERErUBhYSE8PDyQkpJi6laI2iyGQiJqUdXV1fjkk08wZcoUDBkyBP3790dgYCCmT5+OrKws1NXVmbpFs6VUKpGSkoLCwkLJr0lJSYGHhwdOnTplxM4Mo6ysDCkpKTh8+LCpWyFqlxgKiajFkCaDdwAACjNJREFUXLlyBeHh4ViyZAnkcjkSEhKwaNEixMfHo66uDgsWLMCKFStM3abZUiqVSE1Nxc8//2zqVoyirKwMqamp93zsIREZl1XzJUREv19NTQ1eeuklFBYWIiUlBWPHjtXanpCQgIKCglaxR4uIqC3inkIiahGbNm3CpUuX8MILL+gEwka+vr547rnntMZ2796N6Oho+Pv7Y8CAAYiOjsbu3bt1Xjt69GjExsbizJkziI+Px4ABAzB8+HC88847qKurg1qtxrvvvosRI0bAx8cHzz33HC5evKg1R1ZWFjw8PJCXl4fU1FSMGjUKvr6+iIiIwMmTJwEAR44cQUxMDPz9/REcHIy0tLQmv5dTp05h5syZGDp0KLy9vfHkk09i1apVOofHY2NjMXr0aBQXF2PevHkYPHgw/Pz8MG3aNFy6dEmsS0lJwYIFCwAAcXFx8PDwgIeHB5KSkpr5yUt38OBBTJ06FYMGDfr/9u41JKquiwP4XyXNybzWmKl5iU5qjppalqP1eCtTzNQcMy+IlQiBlkVCRFiBpAVBFzAtI2kIM8sEBbvgJYu8BCEGViqakIky6WijWbnfD71z6Dia1mv68rB+4IdZe5096+wRZnv2PkdIJBKEhYXh9u3bGnnqse7o6EBKSgrWr18PDw8PpKWlob+/XyO/ra0NycnJcHNzg5eXFzIzM6FQKAT1NzQ0ICAgAABw+fJl/vz8/f01+quurkZUVBQkEgl8fHyQk5ND2w4ImQM6WVlZWQtdBCHk3+/cuXP48OEDcnNzYWRkNKtj5HI5MjMzIRKJkJiYiA0bNuDVq1eQy+UQi8Vwdnbmc2/evAmVSoWSkhJ4eXkhLCwM3759Q2lpKcbHxyGXy9Hb24vo6Gg4OTnh4cOHqK2tRVxcHLS0tAD8WJ598uQJ2tvb0dnZCZlMBk9PT9TW1uLevXtYvXo1jh49iqCgIAQHB2NgYAD37t3DqlWr4ODgwNdSU1ODffv2AQD27NmDbdu2QUtLC0VFRWhvb8eOHTv43Pv376Ovrw8VFRWwsLBAZGQkbG1tUVlZifr6esTGxkJbWxtGRkZgjOH169dITU2FTCZDUFAQpFIpLCwsph3DxsZGNDY2QiaTwdzcfNq84uJiHD58GGZmZpDJZPDz84NSqURhYSFUKhV8fHwEYz06Ooo7d+7A1dUV4eHhMDU1RXl5Od68eYPw8HA+t6urCzExMejt7eUnwG1tbSguLkZ/fz8cHR0RGBiIxYsXw9zcHPX19QgKCkJqaiqCgoIQEBAAe3t7KJVKFBUVYXR0FGVlZQgNDcX27duhVCrx4MED6OnpwdPTc1a/V4SQaTBCCJkHGzduZO7u7rPOHxwcZG5ubiwwMJANDw/z8eHhYRYQEMDc3NzY0NAQH/fz82Mcx7HKykpBPxEREWzt2rUsNTWVTUxM8PGbN28yjuNYXV0dHystLWUcx7Fdu3axL1++8PHHjx8zjuOYk5MTa2lp4eNfvnxhUqmUyWQyPjY2Nsa8vb3Z3r172devXwW13Lhxg3Ecx168eMHH4uPjGcdxLD8/X5BbUFAwbX0/Hz+TixcvMo7jBHVP1tfXx5ydnVlGRoZG25kzZ5iDgwN7//49H1OPdUVFhSA3KyuLcRzHOjo6+FhaWhrjOI41NzcLctPT0xnHcSwzM5OP9fT0MI7j2MWLFzXqULe5urqynp4ePj4xMcFCQ0OZVCr9xSgQQmaDlo8JIfNiZGQES5YsmXX+s2fPoFKpkJCQAAMDAz5uYGCAhIQEqFQqPH/+XHCMubm54CocALi7u4MxhoSEBP6KIAD+qlJ3d7fGe8fGxkJXV1cj18XFBRKJhI/r6upCIpGgq6tLUPfAwAAiIyOhVCqhUCj4ny1btvA5P9PW1kZiYqIgtmnTpmnrm2tVVVUYHx/H7t27BfUqFAr4+/tjYmJCY6zFYjFCQkJ+WfP3799RV1cHFxcXeHh4CHKTk5P/qNaAgABYWVnxr7W0tODl5YX+/n58/vz5j/okhPxAN5oQQuaFgYHBb31pqx+7smbNGo02daynp0cQ/3myoKZeqp7cZmhoCAAYHBzUOMba2npWfajbfu5DvU/x+PHjGrlqAwMDgtdisRh6enqCmLGx8bT1zTV1zUlJSdPmTK558hgBmjUrFAqoVCrY2dlp5E4Vm42Z3vd3/vAghAjRpJAQMi/WrFmDpqYm9PT0TPnFPhd0dHSmbdPWnnphhDE269xf9T+5v2PHjsHR0XHKHLFYPOt+p6pvrqnfIycnR6M2tcmf2ULVvNBjRci/GU0KCSHzYtu2bWhqakJJSQkyMjJmzFdPQt69e4fNmzcL2trb2wU5/09sbW0BAPr6+vD29p7Tvn9e/p5L6ppNTEzmtGZTU1OIRCLBXdRqU8X+1vkRQmaH9hQSQuZFdHQ07OzsUFhYOOUjZQCgtbUVcrkcACCVSiESiXDr1i2MjIzwOSMjI7h16xZEIhGkUum81P47fHx8YGZmhoKCgimXfsfGxgTn8ztEIhEAYGho6H+qcbIdO3ZAV1cXly5dwtjYmEb78PAwxsfHf7tfHR0d+Pr6oqWlBS9fvhS0FRYWauT/rfMjhMwOXSkkhMwLfX19XL16FSkpKTh48CB8fHzg7e0NY2NjKBQKNDQ0oL6+Hvv37wfwY8/f0aNHcfr0achkMkRERAD48QiX7u5unD59GkuXLl3IU5qSSCRCTk4ODh48iODgYERFRcHGxgZKpRKdnZ149OgRLl++DC8vr9/uWyKRQFtbG3l5eRgaGoJIJIKVlRVcXV1nPLa0tBRPnz7ViK9btw5bt25FVlYWTpw4gZCQEOzcuROWlpZQKBR4+/YtHj9+jIqKiin3VM7k0KFD/OcaHx+PFStWoKamBgqFAoDw6qCJiQlsbGxQUVEBa2trLFu2DPr6+lM+q5AQMvdoUkgImTc2NjYoKytDcXExqqqqkJeXB5VKBSMjIzg7O+Ps2bMICwvj8+Pi4iAWi3H9+nX+IdEODg64cuUKAgMDF+o0ZuTr64u7d+8iPz8f5eXl+PTpEwwNDbFq1SokJSVh7dq1f9TvypUrkZ2djYKCApw6dQpfv35FRETErCaFUz2EGgBiYmKwdetWREVFwdbWFoWFhSguLsbw8DCMjY1hZ2eH9PR0LF++/I9qtre3h1wuR05ODoqKiqCnp4d//vkHJ0+eRGBgoMYNNufPn0d2djYuXLiA0dFRWFpa0qSQkHmixWhnLiGEkHnW2tqKqKgoHDlyBCkpKQtdDiEEtKeQEELIXzZ5nyJjDNeuXQOAOb8ZhxDy52j5mBBCyF8VHh6OTZs2geM4jI6Oorq6Gs3NzQgJCRH8q0JCyMKi5WNCCCF/VW5uLqqrq/Hx40d8+/YNVlZWCAsLw4EDB7Bo0aKFLo8Q8l80KSSEEEIIIbSnkBBCCCGE0KSQEEIIIYSAJoWEEEIIIQQ0KSSEEEIIIaBJISGEEEIIAU0KCSGEEEIIgP8A6uiuJmtvvd4AAAAASUVORK5CYII=
&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Count the number of sentences that had to be truncated to 512 tokens.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_truncated&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lengths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Compare this to the total number of training sentences.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_sentences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lengths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prcnt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_truncated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; of &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; sentences (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:.1%}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;) in the training set are longer than 512 tokens.&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_truncated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prcnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;1,668 of 69,526 sentences (2.4%) in the training set are longer than 512 tokens.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Tally up how many of the truncated sentences are positive vs. negative examples.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_neg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Iterate through the comment lengths,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lengths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# If the sentence was truncated,&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Tally up whether it contains a personal attack or not.&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;num_pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;num_neg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    
&lt;span class=&quot;c1&quot;&gt;# Report the total.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:.1%}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;) of the truncated examples contain a personal attack&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_neg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;0 (0.0%) of the truncated examples contain a personal attack
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# We&amp;#39;ll borrow the `pad_sequences` utility function to do this.&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.preprocessing.sequence&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_sequences&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Set the required sequnce length.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;MAX_LEN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;Padding/truncating all sentences to &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; values...&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_LEN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;Padding token: &amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;quot;, ID: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad_token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad_token_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Pad our input tokens with value 0.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MAX_LEN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;long&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;truncating&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;post&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;post&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;Done.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;
Padding/truncating all sentences to 128 values...

Padding token: &amp;#34;[PAD]&amp;#34;, ID: 0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;Using TensorFlow backend.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;
Done.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;array([  101,  1036,  1011,  2023,  2003,  2025,  1036,  1036,  5541,
        1036,  1036,  1012,  2216,  2024,  1996,  9206, 15182,  1997,
        1996,  3408,  1036,  1036,  5427,  1036,  1036,  1998,  1036,
        1036,  4372, 26210,  6651,  1036,  1036,  2004,  7919,  4162,
        2000,  1036,  1036,  6215,  1036,  1036,  1012,  2065,  2017,
        2123,  1005,  1056,  3305,  2008,  1010,  2986,  1010, 11476,
        6256,  1010,  1045,  1005,  2222,  4339,  2039,  1036,  1036,
        2093,  2158,  3526,  1036,  1036,  1998,  1036,  1036, 17284,
        4477,  1036,  1036,  1998,  2059,  2009,  2097,  2022,  3733,
        2000,  3305,  2339,  1036,  1036, 16316,  1036,  1036,  1998,
        1036,  1036, 16021, 12165,  1036,  1036,  2024,  2367,  1011,
        1998,  2339,  2119, 11234,  2013,  1036,  1036,  8916,  1036,
        1036,  1012,  1996,  6251,  2017, 14686,  2003,  7078,  8699,
        1012,  2017,  2074,  4995,  1005,  1056,  5220,  2007,  1996,
       10318,  3399])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;array([  101,  2995,  2030,  6270,  1010,  1996,  3663,  2004,  1997,
        2233,  2526,  2001,  2107,  1024,  1037,  8174,  6378,  1997,
        2455,  2005,  3521,  1998,  5038,  2011,  2035,  5424,  3032,
        2001,  2081,  1012,  1996,  2154,  1996,  6378,  2001,  2000,
        2022,  2081,  5337,  2011,  1996,  5424,  2223,  2001,  1996,
        2154,  1996,  5611,  1005,  1055,  2104,  1996,  3094,  1997,
       16126, 10666,  2211,  1996,  5274,  1997,  1996,  9302,  2969,
        1011,  3627,  2752,  1012,  5310,  1024,  5424,  1012,   102,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Create attention masks&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For each sentence,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sent&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Create the attention mask.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#   - If a token ID is 0, then it&amp;#39;s padding, set the mask to 0.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#   - If a token ID is &amp;gt; 0, then it&amp;#39;s a real token, set the mask to 1.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;att_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token_id&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Store the attention mask for this sentence.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;att_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Use train_test_split to split our data into train and validation sets for training&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Use 90% for training and 10% for validation.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2018&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Do the same for the masks.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2018&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Convert all inputs and labels into torch tensors, the required datatype for our model.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validation_inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validation_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validation_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SequentialSampler&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The DataLoader needs to know our batch size for training, so we specify it here.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create the DataLoader for our training set.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_sampler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_sampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create the DataLoader for our validation set&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validation_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validation_sampler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SequentialSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validation_dataloader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation_sampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Part-II---BERT-Fine-Tuning&quot;&gt;Part II - BERT Fine-Tuning&lt;a class=&quot;anchor-link&quot; href=&quot;#Part-II---BERT-Fine-Tuning&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Train-Our-Classification-Model&quot;&gt;Train Our Classification Model&lt;a class=&quot;anchor-link&quot; href=&quot;#Train-Our-Classification-Model&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForSequenceClassification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdamW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertConfig&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Load BertForSequenceClassification, the pretrained BERT model with a single&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# linear classification layer on top.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForSequenceClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;bert-base-uncased&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Use the 12-layer BERT model, with an uncased vocab.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# The number of output labels--2 for binary classification.&lt;/span&gt;
                                &lt;span class=&quot;c1&quot;&gt;# You can increase this for multi-class tasks.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output_attentions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Whether the model returns attentions weights.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output_hidden_states&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Whether the model returns all hidden-states.&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Tell pytorch to run this model on the GPU.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Note: AdamW is a class from the huggingface library (as opposed to pytorch)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# I believe the &amp;#39;W&amp;#39; stands for &amp;#39;Weight Decay fix&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdamW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2e-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# args.learning_rate - default is 5e-5, our notebook had 2e-5&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-8&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# args.adam_epsilon - default is 1e-8.&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_linear_schedule_with_warmup&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Number of training epochs (authors recommend between 2 and 4)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Total number of training steps is number of batches * number of epochs.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;total_steps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create the learning rate scheduler.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_linear_schedule_with_warmup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                            &lt;span class=&quot;n&quot;&gt;num_warmup_steps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Default value in run_glue.py&lt;/span&gt;
                                            &lt;span class=&quot;n&quot;&gt;num_training_steps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;7824
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Function to calculate the accuracy of our predictions vs labels&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;flat_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pred_flat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;labels_flat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_flat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels_flat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels_flat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;format_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Takes a time in seconds and returns a string hh:mm:ss&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Round to the nearest second.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;elapsed_rounded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Format as hh:mm:ss&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elapsed_rounded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# This training code is based on the `run_glue.py` script here:&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Set the seed value all over the place to make this reproducible.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;seed_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;manual_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;manual_seed_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Store the average loss after each epoch so we can plot them.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For each epoch,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch_i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# =======================================&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#                               Training&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# =======================================&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Perform one full pass over the training set.&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;======== Epoch &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; / &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; ========&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch_i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Training...&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Measure how long the training epoch takes.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Reset the total loss for this epoch.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Put the model into training mode. Don&amp;#39;t be mislead--the call to `train` just changes the *mode*,&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# it doesn&amp;#39;t *perform* the training. `dropout` and `batchnorm` layers behave differently during &lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# training vs. test &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# For each batch of training data,&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Progress update every 100 batches.&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Calculate elapsed time in minutes.&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;format_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# Report progress.&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;  Batch &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;5,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; of &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:5,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;.    Elapsed: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;.&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Unpack this training batch from our dataloader.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# As we unpack the batch, we&amp;#39;ll also copy each tensor to the GPU using the `to` method.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# `batch` contains three pytorch tensors:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#   [0]: input ids&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#   [1]: attention masks&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#   [2]: labels&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b_input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b_input_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Always clear any previously calculated gradients before performing a backward pass.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# PyTorch doesn&amp;#39;t do this automatically because accumulating the gradients is &lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# &amp;quot;convenient while trainig RNNs&amp;quot;.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Perform a forwad pass (evaluate the model on this training batch).&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# This will return the loss (rather than the model output) because we have provided the `labels`.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# The documentation for this `model` function is here:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;token_type_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_input_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# The call to `model` always returns a tuple, so we need to pull the loss value out of the tuple.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Accumulate the training loss over all of the batches so that we can calculate the average loss at the end.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# `loss` is a Tensor containing a single value; the `.item()` function just returns the Python value from the tensor.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;total_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Perform a backward pass to calculate the gradients.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Clip the norm of the gradients to 1.0.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# This is to help prevent the &amp;quot;exploding gradients&amp;quot; problem.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clip_grad_norm_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Update parameters and take a step using the computed gradient.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# The optimizer dictates the &amp;quot;update rule&amp;quot;--how the parameters are modified&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# based on their gradients, the learning rate, etc.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Update the learning rate.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Calculate the average loss over the training data.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;avg_train_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Store the loss value for plotting the learning curve.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss_values&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_train_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;  Average training loss: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{0:.2f}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_train_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;  Training epoch took: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# =======================================&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#                               Validation&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ======================================= &lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# After the completion of each training epoch, measure our performance on our validation set.&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Running Validation...&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Put the model in evaluation mode--the dropout layers behave differently during evaluation.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Tracking variables &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;eval_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval_accuracy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nb_eval_steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_eval_examples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;  

    &lt;span class=&quot;c1&quot;&gt;# Evaluate data for one epoch&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Add  batch to GPU&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Unpack the inputs from our dataloader&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b_input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_input_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Telling the model not to compute or store gradients, saving memory and speeding up validataion&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# Forward pass, calculate logit predictions.&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# This will return the logits rather than the loss because we have not provided labels.&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# token_type_ids is the same as the &amp;quot;segment ids&amp;quot;, which differentiates sentence 1 and 2 in 2-sentence task.&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                           &lt;span class=&quot;n&quot;&gt;token_type_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                           &lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_input_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            
        &lt;span class=&quot;c1&quot;&gt;# Get the &amp;quot;logits&amp;quot; output by the model. The &amp;quot;logits&amp;quot; are the output values prior to applying &lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# an activation function like the softmax.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Move logits and labels to CPU&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;label_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;cpu&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Calculate the accuracy for this batch of test sentences.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tmp_eval_accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flat_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Accumulate the total accuracy.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;eval_accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp_eval_accuracy&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Track the number of batches&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nb_eval_steps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Report the final accuracy for this validation run.&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;  Accuracy: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{0:.2f}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval_accuracy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_eval_steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;  Validation took: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Training complete!&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;
======== Epoch 1 / 4 ========
Training...
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;  Batch   100 of 1,956.    Elapsed: 0:01:14.
  Batch   200 of 1,956.    Elapsed: 0:02:28.
  Batch   300 of 1,956.    Elapsed: 0:03:43.
  Batch   400 of 1,956.    Elapsed: 0:04:57.
  Batch   500 of 1,956.    Elapsed: 0:06:12.
  Batch   600 of 1,956.    Elapsed: 0:07:26.
  Batch   700 of 1,956.    Elapsed: 0:08:40.
  Batch   800 of 1,956.    Elapsed: 0:09:55.
  Batch   900 of 1,956.    Elapsed: 0:11:09.
  Batch 1,000 of 1,956.    Elapsed: 0:12:24.
  Batch 1,100 of 1,956.    Elapsed: 0:13:38.
  Batch 1,200 of 1,956.    Elapsed: 0:14:53.
  Batch 1,300 of 1,956.    Elapsed: 0:16:07.
  Batch 1,400 of 1,956.    Elapsed: 0:17:22.
  Batch 1,500 of 1,956.    Elapsed: 0:18:36.
  Batch 1,600 of 1,956.    Elapsed: 0:19:51.
  Batch 1,700 of 1,956.    Elapsed: 0:21:06.
  Batch 1,800 of 1,956.    Elapsed: 0:22:20.
  Batch 1,900 of 1,956.    Elapsed: 0:23:35.

  Average training loss: 0.14
  Training epoch took: 0:24:16

Running Validation...
  Accuracy: 0.96
  Validation took: 0:00:52

======== Epoch 2 / 4 ========
Training...
  Batch   100 of 1,956.    Elapsed: 0:01:15.
  Batch   200 of 1,956.    Elapsed: 0:02:29.
  Batch   300 of 1,956.    Elapsed: 0:03:44.
  Batch   400 of 1,956.    Elapsed: 0:04:58.
  Batch   500 of 1,956.    Elapsed: 0:06:13.
  Batch   600 of 1,956.    Elapsed: 0:07:28.
  Batch   700 of 1,956.    Elapsed: 0:08:43.
  Batch   800 of 1,956.    Elapsed: 0:09:58.
  Batch   900 of 1,956.    Elapsed: 0:11:12.
  Batch 1,000 of 1,956.    Elapsed: 0:12:27.
  Batch 1,100 of 1,956.    Elapsed: 0:13:41.
  Batch 1,200 of 1,956.    Elapsed: 0:14:56.
  Batch 1,300 of 1,956.    Elapsed: 0:16:11.
  Batch 1,400 of 1,956.    Elapsed: 0:17:26.
  Batch 1,500 of 1,956.    Elapsed: 0:18:40.
  Batch 1,600 of 1,956.    Elapsed: 0:19:55.
  Batch 1,700 of 1,956.    Elapsed: 0:21:10.
  Batch 1,800 of 1,956.    Elapsed: 0:22:24.
  Batch 1,900 of 1,956.    Elapsed: 0:23:39.

  Average training loss: 0.09
  Training epoch took: 0:24:20

Running Validation...
  Accuracy: 0.95
  Validation took: 0:00:52

======== Epoch 3 / 4 ========
Training...
  Batch   100 of 1,956.    Elapsed: 0:01:15.
  Batch   200 of 1,956.    Elapsed: 0:02:29.
  Batch   300 of 1,956.    Elapsed: 0:03:44.
  Batch   400 of 1,956.    Elapsed: 0:04:58.
  Batch   500 of 1,956.    Elapsed: 0:06:13.
  Batch   600 of 1,956.    Elapsed: 0:07:27.
  Batch   700 of 1,956.    Elapsed: 0:08:42.
  Batch   800 of 1,956.    Elapsed: 0:09:56.
  Batch   900 of 1,956.    Elapsed: 0:11:11.
  Batch 1,000 of 1,956.    Elapsed: 0:12:25.
  Batch 1,100 of 1,956.    Elapsed: 0:13:40.
  Batch 1,200 of 1,956.    Elapsed: 0:14:54.
  Batch 1,300 of 1,956.    Elapsed: 0:16:09.
  Batch 1,400 of 1,956.    Elapsed: 0:17:24.
  Batch 1,500 of 1,956.    Elapsed: 0:18:38.
  Batch 1,600 of 1,956.    Elapsed: 0:19:53.
  Batch 1,700 of 1,956.    Elapsed: 0:21:07.
  Batch 1,800 of 1,956.    Elapsed: 0:22:22.
  Batch 1,900 of 1,956.    Elapsed: 0:23:36.

  Average training loss: 0.05
  Training epoch took: 0:24:18

Running Validation...
  Accuracy: 0.95
  Validation took: 0:00:52

======== Epoch 4 / 4 ========
Training...
  Batch   100 of 1,956.    Elapsed: 0:01:14.
  Batch   200 of 1,956.    Elapsed: 0:02:29.
  Batch   300 of 1,956.    Elapsed: 0:03:44.
  Batch   400 of 1,956.    Elapsed: 0:04:58.
  Batch   500 of 1,956.    Elapsed: 0:06:12.
  Batch   600 of 1,956.    Elapsed: 0:07:27.
  Batch   700 of 1,956.    Elapsed: 0:08:41.
  Batch   800 of 1,956.    Elapsed: 0:09:56.
  Batch   900 of 1,956.    Elapsed: 0:11:10.
  Batch 1,000 of 1,956.    Elapsed: 0:12:25.
  Batch 1,100 of 1,956.    Elapsed: 0:13:39.
  Batch 1,200 of 1,956.    Elapsed: 0:14:54.
  Batch 1,300 of 1,956.    Elapsed: 0:16:09.
  Batch 1,400 of 1,956.    Elapsed: 0:17:23.
  Batch 1,500 of 1,956.    Elapsed: 0:18:38.
  Batch 1,600 of 1,956.    Elapsed: 0:19:52.
  Batch 1,700 of 1,956.    Elapsed: 0:21:07.
  Batch 1,800 of 1,956.    Elapsed: 0:22:21.
  Batch 1,900 of 1,956.    Elapsed: 0:23:35.

  Average training loss: 0.03
  Training epoch took: 0:24:17

Running Validation...
  Accuracy: 0.95
  Validation took: 0:00:52

Training complete!
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sns&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Use plot styling from seaborn.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;darkgrid&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Increase the plot size and font size.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;font_scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;figure.figsize&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plot the learning curve.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;b-o&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Label the plot.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Training loss&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Epoch&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Loss&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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
&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Performance-On-Test-Set&quot;&gt;Performance On Test Set&lt;a class=&quot;anchor-link&quot; href=&quot;#Performance-On-Test-Set&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Tokenize all of the sentences and map the tokens to their word IDs.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For every sentence,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sen&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Report progress.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;  Read &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; comments.&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# `encode` will:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#   (1) Tokenize the sentence.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#   (2) Prepend the `[CLS]` token to the start.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#   (3) Append the `[SEP]` token to the end.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#   (4) Map tokens to their IDs.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;encoded_sent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                                        &lt;span class=&quot;c1&quot;&gt;# Sentence to encode.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;add_special_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# Add &amp;#39;[CLS]&amp;#39; and &amp;#39;[SEP]&amp;#39;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_LEN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;# Truncate all sentences.&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Add the encoded sentence to the list.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;test_input_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;DONE.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;10,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; test comments&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Also retrieve the labels as a list.&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Get the labels from the DataFrame, and convert from booleans to ints.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;10,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; positive (contains attack)&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;10,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; negative (not an attack)&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Pad our input tokens&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MAX_LEN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;long&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                               &lt;span class=&quot;n&quot;&gt;truncating&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;post&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;post&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create attention masks&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_attention_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create a mask of 1s for each token followed by 0s for padding&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;seq_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;test_attention_masks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Convert to tensors.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_attention_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Set the batch size.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create the DataLoader.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_sampler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SequentialSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_dataloader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_sampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;DONE.

    23,178 test comments
     2,756 positive (contains attack)
    20,422 negative (not an attack)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Prediction on test set&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Predicting labels for &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; test sentences...&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Put model in evaluation mode&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Tracking variables&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;true_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Measure elapsed time.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Predict&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Add batch to GPU&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Progress update every 100 batches.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Calculate elaped time in minutes.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;format_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Report progress.&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;  Batch &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;5,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; of &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:5,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;.    Elapsed: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;.&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Unpack the inputs from our dataloader&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b_input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_input_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Telling the model not to compute or store gradients, saving memory and speeding up prediction&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Forward pass, calculate logit predictions&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token_type_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                        &lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_input_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Move logits and labels to CPU&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;label_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;cpu&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Store predictions and true labels&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;true_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;    DONE.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Predicting labels for 23,178 test sentences...
  Batch   100 of   725.    Elapsed: 0:00:24.
  Batch   200 of   725.    Elapsed: 0:00:48.
  Batch   300 of   725.    Elapsed: 0:01:12.
  Batch   400 of   725.    Elapsed: 0:01:36.
  Batch   500 of   725.    Elapsed: 0:02:00.
  Batch   600 of   725.    Elapsed: 0:02:25.
  Batch   700 of   725.    Elapsed: 0:02:49.
    DONE.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;[array([[ 5.43135  , -4.0807185],
        [ 4.597425 , -3.4614418],
        [ 5.2029805, -3.8499084],
        [ 5.43569  , -4.196341 ],
        [ 5.240312 , -3.9164932],
        [ 5.4701114, -4.1476755],
        [ 5.4822345, -4.063522 ],
        [ 4.17534  , -3.1728623],
        [ 5.5162745, -4.17683  ],
        [ 5.4228077, -3.9708762],
        [ 5.3999577, -4.0484023],
        [ 5.4844804, -4.143347 ],
        [ 5.554893 , -4.2400174],
        [ 5.3096027, -3.868986 ],
        [ 5.4283338, -4.090388 ],
        [ 5.326691 , -3.9882448],
        [ 3.5734868, -2.635085 ],
        [ 4.595947 , -3.5288842],
        [ 5.4501967, -4.1388345],
        [ 5.5220246, -4.186985 ],
        [ 5.0823264, -3.7486613],
        [ 5.5255437, -4.2840915],
        [ 5.192801 , -3.9431345],
        [ 5.3387275, -4.0219254],
        [ 5.560451 , -4.240357 ],
        [ 5.3670015, -3.9867408],
        [ 5.392772 , -4.103932 ],
        [ 4.2843475, -3.2214487],
        [ 5.487993 , -4.157152 ],
        [ 4.837236 , -3.5553339],
        [ 4.913093 , -3.7768197],
        [ 5.472227 , -4.2356243]], dtype=float32),
 array([[ 5.2538056, -3.8715463],
        [ 5.2556586, -3.9228978],
        [ 5.5078926, -4.181313 ],
        [ 4.3217835, -3.0952156],
        [ 4.9776473, -3.7838545],
        [ 5.533898 , -4.2590494],
        [ 5.1893706, -3.8123336],
        [ 5.3296304, -4.048206 ],
        [ 4.8392463, -3.3599744],
        [ 5.514237 , -4.198028 ],
        [ 4.203855 , -3.106302 ],
        [ 4.9724193, -3.650119 ],
        [ 5.333487 , -4.105561 ],
        [ 5.376324 , -4.1128936],
        [ 4.2694583, -3.1707146],
        [ 4.951589 , -3.7181838],
        [ 5.5396237, -4.2052727],
        [ 5.4250503, -4.1939263],
        [ 5.2637324, -3.8830388],
        [ 5.3814473, -4.130902 ],
        [ 5.134685 , -3.8901913],
        [ 5.2697234, -3.994122 ],
        [ 5.265741 , -4.0217566],
        [ 5.476533 , -4.18317  ],
        [ 5.381158 , -4.0330024],
        [ 5.479093 , -4.175233 ],
        [ 4.0906634, -3.0154393],
        [ 5.488876 , -4.1607084],
        [ 5.3916135, -4.0865245],
        [ 5.3949404, -4.181269 ],
        [ 5.4594636, -4.103906 ],
        [ 5.0248437, -3.7683067]], dtype=float32),
 array([[ 5.300899 , -4.035556 ],
        [ 5.395021 , -4.0742126],
        [ 5.3767295, -4.0745296],
        [ 5.385198 , -4.046394 ],
        [ 5.3743854, -3.986636 ],
        [ 5.457197 , -4.2560277],
        [ 5.0604186, -3.7781377],
        [ 5.556199 , -4.2197604],
        [ 5.318427 , -4.0005083],
        [ 5.48931  , -4.2283764],
        [ 5.445115 , -4.110128 ],
        [ 5.3118157, -4.0336213],
        [ 5.491487 , -4.2422705],
        [ 5.1997533, -3.8949559],
        [ 5.5384912, -4.190333 ],
        [ 5.135718 , -3.6955633],
        [ 4.976919 , -3.6015885],
        [ 5.4156137, -4.1589346],
        [ 5.423242 , -4.173223 ],
        [ 4.4196258, -3.3494275],
        [ 5.3858056, -4.0824265],
        [ 5.37742  , -4.122572 ],
        [ 5.4962883, -4.251161 ],
        [ 5.418966 , -4.063055 ],
        [-1.286079 ,  0.7293226],
        [ 5.402163 , -4.107389 ],
        [ 5.3998604, -4.2119117],
        [ 5.2807593, -4.0225797],
        [ 5.343671 , -3.9670048],
        [ 4.368339 , -3.1019998],
        [ 5.2154574, -3.9437592],
        [ 5.2475038, -3.8830495]], dtype=float32),
 array([[ 5.5114827, -4.2829566],
        [ 5.5235353, -4.171074 ],
        [ 5.4299884, -4.1302404],
        [ 4.463454 , -3.1880848],
        [ 4.927576 , -3.5457666],
        [ 5.531424 , -4.2008467],
        [-3.145138 ,  2.149257 ],
        [ 5.206561 , -3.8794465],
        [ 5.518431 , -4.1594043],
        [ 5.051986 , -3.6589901],
        [ 5.3859887, -4.131961 ],
        [ 5.1433153, -3.8335874],
        [ 5.528437 , -4.2033815],
        [ 5.196643 , -3.8455179],
        [ 5.255228 , -4.015091 ],
        [ 5.225014 , -3.9305272],
        [ 5.444698 , -4.1221886],
        [ 5.462351 , -4.210379 ],
        [ 5.4832816, -4.1626678],
        [ 5.314896 , -4.0713444],
        [ 5.3126583, -4.090319 ],
        [ 5.496307 , -4.090698 ],
        [ 5.446159 , -4.129005 ],
        [ 5.4019475, -4.1180854],
        [ 4.3167434, -3.1993456],
        [ 4.9754267, -3.617138 ],
        [ 5.3815904, -4.1416698],
        [ 5.2965603, -3.963538 ],
        [-1.2883178,  0.7328278],
        [ 4.935533 , -3.6571178],
        [ 3.400453 , -2.543452 ],
        [ 5.4850397, -4.2034473]], dtype=float32),
 array([[ 5.426343 , -4.058358 ],
        [ 5.355119 , -4.006176 ],
        [ 5.196791 , -3.9755495],
        [ 5.4531174, -4.104737 ],
        [ 5.452077 , -4.188066 ],
        [-2.5248263,  1.7298604],
        [ 4.8599544, -3.6085203],
        [ 2.5870051, -1.8053671],
        [ 4.799905 , -3.7399406],
        [ 5.384679 , -4.0612016],
        [ 1.9848813, -1.4291253],
        [ 4.8865104, -3.631117 ],
        [ 5.042236 , -3.7061255],
        [ 5.3766055, -4.1468015],
        [ 5.253485 , -3.9272726],
        [ 5.0634665, -3.8075929],
        [ 2.4534297, -1.7532713],
        [ 3.3397276, -2.3743272],
        [ 5.2756653, -3.9090054],
        [ 4.641894 , -3.5683382],
        [-2.1660845,  1.4977252],
        [ 5.541515 , -4.2125726],
        [ 5.5152283, -4.2085767],
        [ 5.467479 , -4.161997 ],
        [ 5.410911 , -4.0653553],
        [ 5.4061475, -4.0659604],
        [ 4.4883223, -3.2679074],
        [ 4.3587704, -3.3385775],
        [ 5.5541663, -4.2766557],
        [ 5.013972 , -3.7639925],
        [ 5.2121844, -3.9890063],
        [ 5.31105  , -3.9589643]], dtype=float32),
 array([[ 5.2220817 , -3.8852181 ],
        [ 5.096519  , -3.8301609 ],
        [ 4.5275893 , -3.3427517 ],
        [-2.279029  ,  1.5349158 ],
        [ 5.3037395 , -3.9923763 ],
        [ 5.4391146 , -4.198975  ],
        [ 4.831823  , -3.544483  ],
        [ 3.2818186 , -2.356049  ],
        [ 5.3940644 , -4.1183677 ],
        [ 4.5440726 , -3.4263175 ],
        [ 5.3488674 , -4.0014596 ],
        [ 3.9753976 , -2.9137933 ],
        [ 5.5004687 , -4.174517  ],
        [ 5.2306914 , -3.887057  ],
        [ 5.468411  , -4.1401105 ],
        [ 1.8992587 , -1.1993924 ],
        [ 5.298141  , -3.9230304 ],
        [ 5.0579286 , -3.7088711 ],
        [ 4.9830604 , -3.7070842 ],
        [ 5.460772  , -4.1708426 ],
        [ 5.2573924 , -3.8962035 ],
        [ 5.4351697 , -4.162905  ],
        [ 5.1305127 , -3.9049475 ],
        [ 5.478809  , -4.141188  ],
        [ 5.1301684 , -3.9068274 ],
        [ 5.369158  , -4.0585637 ],
        [ 4.89111   , -3.7031734 ],
        [ 4.9608164 , -3.6122632 ],
        [-0.28882384, -0.08296825],
        [ 5.342772  , -4.118634  ],
        [ 5.4631066 , -4.0267425 ],
        [ 5.426874  , -4.127703  ]], dtype=float32),
 array([[ 5.4500875, -4.1427283],
        [ 5.272012 , -3.922525 ],
        [ 5.3324876, -4.0124974],
        [ 5.241836 , -3.988877 ],
        [ 5.430305 , -4.1839437],
        [ 5.314261 , -4.0438604],
        [ 2.7459657, -2.062171 ],
        [ 4.590529 , -3.3443534],
        [ 4.1439905, -3.046077 ],
        [ 4.952759 , -3.6524353],
        [ 3.406953 , -2.374637 ],
        [ 2.3458736, -1.8320854],
        [ 5.1831036, -3.7904568],
        [ 4.8084626, -3.5252323],
        [ 3.6671154, -2.6256115],
        [ 4.947343 , -3.5901263],
        [ 5.410084 , -4.152666 ],
        [ 5.566479 , -4.1783257],
        [ 5.5365787, -4.2330713],
        [ 4.4909205, -3.4299731],
        [ 5.1837406, -3.8058364],
        [ 5.493726 , -4.1462436],
        [ 5.516855 , -4.2894855],
        [ 5.461716 , -4.144648 ],
        [ 4.2099686, -3.1970298],
        [ 5.4657083, -4.0766892],
        [ 3.5141003, -2.6443436],
        [ 4.574696 , -3.411751 ],
        [ 4.479477 , -3.4387724],
        [ 4.129355 , -2.9603298],
        [ 5.4946685, -4.226595 ],
        [ 4.7603784, -3.5759366]], dtype=float32),
 array([[ 5.22998  , -3.8989506],
        [ 4.62727  , -3.5161207],
        [ 5.2467384, -4.003106 ],
        [ 5.0838833, -3.7301426],
        [ 4.624779 , -3.3708727],
        [ 5.501288 , -4.1779866],
        [ 5.5021577, -4.1358433],
        [ 5.387923 , -4.0268197],
        [ 5.39391  , -4.0430226],
        [ 5.3329606, -3.9436512],
        [ 5.457484 , -4.2270913],
        [ 4.831526 , -3.6528254],
        [ 5.461303 , -4.117969 ],
        [ 5.2780957, -3.8968644],
        [ 5.278866 , -3.9510126],
        [ 5.355661 , -4.1682024],
        [ 5.074125 , -3.7028506],
        [ 5.5256524, -4.2130594],
        [ 5.387303 , -4.0357842],
        [ 5.1998754, -3.8378658],
        [-2.5835028,  1.7536069],
        [ 4.712457 , -3.3902724],
        [ 5.494749 , -4.2219605],
        [ 5.377855 , -4.1340117],
        [ 5.161177 , -3.9164536],
        [-1.891978 ,  1.3880231],
        [ 5.07652  , -3.8013415],
        [-1.4576713,  1.0712835],
        [ 4.5357213, -3.4483316],
        [ 5.187512 , -3.9807591],
        [ 5.488303 , -4.174469 ],
        [ 5.2502856, -4.036819 ]], dtype=float32),
 array([[ 4.5356584, -3.3985226],
        [ 5.393577 , -4.086617 ],
        [ 5.463293 , -4.21639  ],
        [ 5.54695  , -4.2858906],
        [ 5.2682576, -3.9987485],
        [-2.6624324,  1.8077208],
        [ 4.798633 , -3.6184127],
        [ 5.3184495, -3.9698799],
        [ 4.5881543, -3.383561 ],
        [ 5.447883 , -4.1521564],
        [ 5.4165   , -4.1261272],
        [ 4.494101 , -3.3133528],
        [ 4.767463 , -3.6006236],
        [ 5.354958 , -4.050154 ],
        [ 5.448631 , -4.181068 ],
        [ 5.4187636, -4.1287675],
        [ 5.3159976, -4.013481 ],
        [ 5.3802915, -4.1061697],
        [ 5.3652625, -4.086414 ],
        [ 2.7618191, -1.9567709],
        [ 4.444735 , -3.213555 ],
        [ 4.760546 , -3.4369195],
        [ 5.239843 , -3.980441 ],
        [ 5.330209 , -4.011886 ],
        [ 5.1778207, -3.8983636],
        [ 5.491265 , -4.1538916],
        [ 5.5505404, -4.201356 ],
        [ 4.3914275, -3.1794152],
        [-2.728721 ,  1.8481518],
        [ 5.0268064, -3.768466 ],
        [ 5.385312 , -4.012403 ],
        [ 5.441138 , -4.173414 ]], dtype=float32),
 array([[ 5.4384995, -4.2335277],
        [ 4.8552275, -3.586826 ],
        [ 5.4932003, -4.276974 ],
        [ 5.3816824, -4.030221 ],
        [ 5.156941 , -3.686106 ],
        [ 4.438695 , -3.3080924],
        [ 5.051719 , -3.8141997],
        [ 5.375688 , -4.0709853],
        [ 5.471972 , -4.204346 ],
        [ 5.236156 , -3.9004257],
        [ 3.6281152, -2.6599073],
        [ 5.177928 , -3.9476264],
        [ 3.9968965, -2.8652222],
        [-2.2053506,  1.5181979],
        [ 5.5276327, -4.201619 ],
        [ 5.332174 , -4.1147814],
        [ 5.5003433, -4.2656736],
        [ 5.313503 , -3.9444892],
        [ 5.4858646, -4.1904445],
        [ 5.2496657, -3.8906143],
        [-3.4954479,  2.3633578],
        [ 5.428599 , -4.1713705],
        [ 5.304045 , -4.048304 ],
        [ 5.229869 , -3.9223626],
        [ 5.343846 , -4.0001264],
        [ 4.7234535, -3.4158018],
        [ 5.5213265, -4.198597 ],
        [ 5.3841186, -4.0770264],
        [ 5.333826 , -4.0571227],
        [ 5.4549546, -4.1659894],
        [ 5.388159 , -4.064118 ],
        [ 4.6759048, -3.497959 ]], dtype=float32)]&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Combine the results across the batches.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;true_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;array([[ 5.43135  , -4.0807185],
       [ 4.597425 , -3.4614418],
       [ 5.2029805, -3.8499084],
       [ 5.43569  , -4.196341 ],
       [ 5.240312 , -3.9164932],
       [ 5.4701114, -4.1476755],
       [ 5.4822345, -4.063522 ],
       [ 4.17534  , -3.1728623],
       [ 5.5162745, -4.17683  ],
       [ 5.4228077, -3.9708762]], dtype=float32)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Our performance metric for the test set.&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roc_auc_score&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Use the model output for label 1 as our predictions.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Calculate the ROC AUC.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;auc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roc_auc_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Test ROC AUC: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.3f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auc&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Test ROC AUC: 0.973
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Save-Trained-Model&quot;&gt;Save Trained Model&lt;a class=&quot;anchor-link&quot; href=&quot;#Save-Trained-Model&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Saving best-practices: if you use defaults name for the model, you can reload it using from_pretrained()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;./model_save/&amp;#39;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create output directory if needed&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makedirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Saving model to &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Save a trained model, configuration and tokenizer using `save_pretrained()`.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# They can then be reloaded using `from_pretrained()`&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_to_save&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;module&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;hasattr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;module&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Take care of distributed/parallel training&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_to_save&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Good practice: save your training arguments together with the trained model&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# torch.save(args, os.path.join(output_dir, &amp;#39;training_args.bin&amp;#39;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Saving model to ./model_save/
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(&amp;#39;./model_save/vocab.txt&amp;#39;,
 &amp;#39;./model_save/special_tokens_map.json&amp;#39;,
 &amp;#39;./model_save/added_tokens.json&amp;#39;)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Mount Google Drive to this Notebook instance.&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;google.colab&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;drive&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;drive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;/content/drive&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;amp;response_type=code&amp;amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly

Enter your authorization code:
··········
Mounted at /content/drive
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gdrive_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;./drive/My Drive/BERT Document Classification Tutorial/model_save/&amp;#39;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create output directory if needed&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gdrive_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makedirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gdrive_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Copy the model files to a directory in your Google Drive.&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;cp -r ./model_save/ &lt;span class=&quot;s2&quot;&gt;&amp;quot;./drive/My Drive/BERT Document Classification Tutorial/model_save/&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Part-III---Semantic-Similarity&quot;&gt;Part III - Semantic Similarity&lt;a class=&quot;anchor-link&quot; href=&quot;#Part-III---Semantic-Similarity&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Vectorize-Comments&quot;&gt;Vectorize Comments&lt;a class=&quot;anchor-link&quot; href=&quot;#Vectorize-Comments&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_hidden_states&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Mount Google Drive to this Notebook instance.&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;google.colab&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;drive&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;drive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;/content/drive&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&amp;#34;/content/drive&amp;#34;, force_remount=True).
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Copy the model files back from Google Drive to the Colab instance. &lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;cp -r &lt;span class=&quot;s2&quot;&gt;&amp;quot;./drive/My Drive/BERT Document Classification Tutorial/model_save/&amp;quot;&lt;/span&gt; ./model_save/
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForSequenceClassification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The name of the folder containing the model files.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;./model_save/&amp;#39;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Load our fine-tuned model, and configure it to return the &amp;quot;hidden states&amp;quot;, &lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# from which we will be taking our text embeddings.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForSequenceClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output_hidden_states&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Whether the model returns all hidden-states.&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# Load the tokenizer.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Copy the model to the GPU.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.preprocessing.sequence&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_sequences&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;text_to_embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Uses the provided BERT `model` and `tokenizer` to generate a vector representation of the input string, `in_text`.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Returns the vector stored as a numpy ndarray.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# ===========================&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#         STEP 1: Tokenization&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ===========================&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;MAX_LEN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# `encode` will:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#   (1) Tokenize the sentence.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#   (2) Truncate the sentence to MAX_LEN if necessary.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#   (3) Prepend the `[CLS]` token to the start.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#   (4) Append the `[SEP]` token to the end. (After truncating!)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#   (5) Map tokens to their IDs.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;in_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                                    &lt;span class=&quot;c1&quot;&gt;# Sentence to encode.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;add_special_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# Add &amp;#39;[CLS]&amp;#39; and &amp;#39;[SEP]&amp;#39;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_LEN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;# Truncate all sentences.&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Pad our input tokens. Truncation was handled above by the `encode` function,&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# which also makes sure that the `[SEP]` token is placed at the end *after* truncating.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Note: `pad_sequences` expects a list of lists, but we only have one piece of text, &lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# so we surround `input_ids` with an extra set of brackets.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MAX_LEN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;long&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;truncating&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;post&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;post&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Remove the outer list.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Create attention masks&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;attn_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Cast to tensors.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;attn_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attn_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Add an extra dimension for the &amp;quot;batch&amp;quot; (even though there is only one input in this batch.)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;attn_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# ===========================&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#         STEP 2: BERT Model&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ===========================&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Put the model in evaluation mode--the dropout layers behave differently during evaluation.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Copy the inputs to the GPU&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Note -- I got stuck here for a while because I didn&amp;#39;t assign the result back to the variable! Geez!&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;attn_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Telling the model not to build the backwards graph will make this a little quicker.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Forward pass, return hidden states and predictions.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# This will return the logits rather than the loss because we have not provided labels.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoded_layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;token_type_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attn_mask&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Retrieve our sentence embedding--take the `[CLS]` embedding from the final layer.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;layer_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# The last BERT layer before the classifier.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Only one input in the batch.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;token_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# The first token, corresponding to [CLS]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Grab the embedding.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoded_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Move to the CPU and convert to numpy ndarray.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Get the text from one of the comments.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Use `textwrap` to print the sentence nicely.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wrapper&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;textwrap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TextWrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initial_indent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;    &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subsequent_indent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;    &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Getting embedding for sentence:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wrapper&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Use the BERT model and tokenizer to generate an embedding for `input_text`.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_to_embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;Done. Embedding shape:&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Getting embedding for sentence:

       :Correct. Full biographical details will put down his birth details, etc.
    It is just a marker to me at the moment to detail the WR aspect. He
    certainly wasn&amp;#39;t Belarus; as a geo-political entity it had no real existence
    at the time. I have put a tbc marker on this article for now.

Done. Embedding shape: (768,)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;format_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Takes a time in seconds and returns a string hh:mm:ss&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Round to the nearest second.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;elapsed_rounded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Format as hh:mm:ss&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elapsed_rounded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Track the time.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Store the set of embeddings.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_comments&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Generating sentence embeddings for all &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; comments...&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;row_num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For each row of the dataframe,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iterrows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Progress update every 2,000 comments.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Calculate elapsed time and format it.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;format_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Calculate the time remaining based on our progress.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rows_per_sec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_num&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;remaining_sec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rows_per_sec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_comments&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;remaining&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;format_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remaining_sec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Report progress.&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;  Comment &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;7,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; of &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;7,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;.    Elapsed: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;. Remaining: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_comments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remaining&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Vectorize this comment.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_to_embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Store the embeddings.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;row_num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Generating sentence embeddings for all 115,864 comments...
  Comment   2,000 of 115,864.    Elapsed: 0:00:31. Remaining: 0:29:42
  Comment   4,000 of 115,864.    Elapsed: 0:01:03. Remaining: 0:29:22
  Comment   6,000 of 115,864.    Elapsed: 0:01:34. Remaining: 0:28:47
  Comment   8,000 of 115,864.    Elapsed: 0:02:06. Remaining: 0:28:12
  Comment  10,000 of 115,864.    Elapsed: 0:02:36. Remaining: 0:27:32
  Comment  12,000 of 115,864.    Elapsed: 0:03:07. Remaining: 0:26:58
  Comment  14,000 of 115,864.    Elapsed: 0:03:38. Remaining: 0:26:26
  Comment  16,000 of 115,864.    Elapsed: 0:04:09. Remaining: 0:25:52
  Comment  18,000 of 115,864.    Elapsed: 0:04:40. Remaining: 0:25:22
  Comment  20,000 of 115,864.    Elapsed: 0:05:11. Remaining: 0:24:49
  Comment  22,000 of 115,864.    Elapsed: 0:05:41. Remaining: 0:24:17
  Comment  24,000 of 115,864.    Elapsed: 0:06:12. Remaining: 0:23:45
  Comment  26,000 of 115,864.    Elapsed: 0:06:43. Remaining: 0:23:13
  Comment  28,000 of 115,864.    Elapsed: 0:07:14. Remaining: 0:22:42
  Comment  30,000 of 115,864.    Elapsed: 0:07:44. Remaining: 0:22:09
  Comment  32,000 of 115,864.    Elapsed: 0:08:15. Remaining: 0:21:38
  Comment  34,000 of 115,864.    Elapsed: 0:08:46. Remaining: 0:21:07
  Comment  36,000 of 115,864.    Elapsed: 0:09:17. Remaining: 0:20:36
  Comment  38,000 of 115,864.    Elapsed: 0:09:49. Remaining: 0:20:06
  Comment  40,000 of 115,864.    Elapsed: 0:10:19. Remaining: 0:19:35
  Comment  42,000 of 115,864.    Elapsed: 0:10:50. Remaining: 0:19:03
  Comment  44,000 of 115,864.    Elapsed: 0:11:20. Remaining: 0:18:31
  Comment  46,000 of 115,864.    Elapsed: 0:11:51. Remaining: 0:18:00
  Comment  48,000 of 115,864.    Elapsed: 0:12:22. Remaining: 0:17:29
  Comment  50,000 of 115,864.    Elapsed: 0:12:53. Remaining: 0:16:58
  Comment  52,000 of 115,864.    Elapsed: 0:13:24. Remaining: 0:16:27
  Comment  54,000 of 115,864.    Elapsed: 0:13:55. Remaining: 0:15:56
  Comment  56,000 of 115,864.    Elapsed: 0:14:26. Remaining: 0:15:25
  Comment  58,000 of 115,864.    Elapsed: 0:14:56. Remaining: 0:14:54
  Comment  60,000 of 115,864.    Elapsed: 0:15:27. Remaining: 0:14:23
  Comment  62,000 of 115,864.    Elapsed: 0:15:58. Remaining: 0:13:52
  Comment  64,000 of 115,864.    Elapsed: 0:16:28. Remaining: 0:13:21
  Comment  66,000 of 115,864.    Elapsed: 0:16:59. Remaining: 0:12:50
  Comment  68,000 of 115,864.    Elapsed: 0:17:29. Remaining: 0:12:19
  Comment  70,000 of 115,864.    Elapsed: 0:18:00. Remaining: 0:11:48
  Comment  72,000 of 115,864.    Elapsed: 0:18:31. Remaining: 0:11:17
  Comment  74,000 of 115,864.    Elapsed: 0:19:01. Remaining: 0:10:46
  Comment  76,000 of 115,864.    Elapsed: 0:19:32. Remaining: 0:10:15
  Comment  78,000 of 115,864.    Elapsed: 0:20:03. Remaining: 0:09:44
  Comment  80,000 of 115,864.    Elapsed: 0:20:33. Remaining: 0:09:13
  Comment  82,000 of 115,864.    Elapsed: 0:21:04. Remaining: 0:08:42
  Comment  84,000 of 115,864.    Elapsed: 0:21:35. Remaining: 0:08:11
  Comment  86,000 of 115,864.    Elapsed: 0:22:05. Remaining: 0:07:40
  Comment  88,000 of 115,864.    Elapsed: 0:22:36. Remaining: 0:07:09
  Comment  90,000 of 115,864.    Elapsed: 0:23:07. Remaining: 0:06:39
  Comment  92,000 of 115,864.    Elapsed: 0:23:37. Remaining: 0:06:08
  Comment  94,000 of 115,864.    Elapsed: 0:24:08. Remaining: 0:05:37
  Comment  96,000 of 115,864.    Elapsed: 0:24:39. Remaining: 0:05:06
  Comment  98,000 of 115,864.    Elapsed: 0:25:10. Remaining: 0:04:35
  Comment 100,000 of 115,864.    Elapsed: 0:25:41. Remaining: 0:04:04
  Comment 102,000 of 115,864.    Elapsed: 0:26:11. Remaining: 0:03:34
  Comment 104,000 of 115,864.    Elapsed: 0:26:42. Remaining: 0:03:03
  Comment 106,000 of 115,864.    Elapsed: 0:27:13. Remaining: 0:02:32
  Comment 108,000 of 115,864.    Elapsed: 0:27:44. Remaining: 0:02:01
  Comment 110,000 of 115,864.    Elapsed: 0:28:15. Remaining: 0:01:30
  Comment 112,000 of 115,864.    Elapsed: 0:28:45. Remaining: 0:01:00
  Comment 114,000 of 115,864.    Elapsed: 0:29:16. Remaining: 0:00:29
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Convert the list of vectors into a 2D array.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vecs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;vecs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(115864, 768)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;./model_save/&amp;#39;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create output directory if needed&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makedirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Use numpy to write out the matrix of embeddings.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Saving embeddings to: ./model_save/embeddings.npy&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;./model_save/embeddings.npy&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Copy the embeddings to a directory in your Google Drive.&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;cp -r ./model_save/embeddings.npy &lt;span class=&quot;s2&quot;&gt;&amp;quot;./drive/My Drive/BERT Document Classification Tutorial/model_save/&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Saving embeddings to: ./model_save/embeddings.npy
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Semantic-Similarity-Search&quot;&gt;Semantic Similarity Search&lt;a class=&quot;anchor-link&quot; href=&quot;#Semantic-Similarity-Search&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install faiss -q
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install faiss-gpu -q
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;faiss&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# =====================================&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#            FAISS Setup&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# =====================================&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Build a flat (CPU) index&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cpu_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;faiss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IndexFlatL2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vecs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Use 1 GPU.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n_gpu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print the number of available GPU.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Number of available GPUs: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;    Using: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;faiss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_num_gpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_gpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# If using multiple GPUs, enable sharding so that the dataset is devided across&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# the GPUs rather than replicated.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;co&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;faiss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GpuMultipleClonerOptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;co&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shard&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Make it into a gpu index&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gpu_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;faiss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index_cpu_to_all_gpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;co&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;co&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ngpu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_gpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add vecs to our GPU index&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Adding dataset to index...&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# gpu_index.add(vecs)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gpu_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Building index took &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; seconds&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Number of available GPUs: 1    Using: 1
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_text output_error&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;ansi-red-fg&quot;&gt;---------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;ansi-red-fg&quot;&gt;RuntimeError&lt;/span&gt;                              Traceback (most recent call last)
&lt;span class=&quot;ansi-green-fg&quot;&gt;&amp;lt;ipython-input-100-d61aa5ee9ffb&amp;gt;&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;&amp;lt;module&amp;gt;&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;     20&lt;/span&gt; 
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;     21&lt;/span&gt; &lt;span class=&quot;ansi-red-fg&quot;&gt;# Make it into a gpu index&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;---&amp;gt; 22&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt; &lt;/span&gt;gpu_index &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; faiss&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;index_cpu_to_all_gpus&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;cpu_index&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; co&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;co&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; ngpu&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;n_gpu&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;     23&lt;/span&gt; 
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;     24&lt;/span&gt; &lt;span class=&quot;ansi-red-fg&quot;&gt;# Add vecs to our GPU index&lt;/span&gt;

&lt;span class=&quot;ansi-green-fg&quot;&gt;/usr/local/lib/python3.6/dist-packages/faiss/__init__.py&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;index_cpu_to_all_gpus&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;(index, co, ngpu)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    516&lt;/span&gt; 
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    517&lt;/span&gt; &lt;span class=&quot;ansi-green-fg&quot;&gt;def&lt;/span&gt; index_cpu_to_all_gpus&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;index&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; co&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;ansi-green-fg&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; ngpu&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ansi-cyan-fg&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;--&amp;gt; 518&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;     &lt;/span&gt;index_gpu &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; index_cpu_to_gpus_list&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;index&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; co&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;co&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; gpus&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;ansi-green-fg&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; ngpu&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;ngpu&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    519&lt;/span&gt;     &lt;span class=&quot;ansi-green-fg&quot;&gt;return&lt;/span&gt; index_gpu
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    520&lt;/span&gt; 

&lt;span class=&quot;ansi-green-fg&quot;&gt;/usr/local/lib/python3.6/dist-packages/faiss/__init__.py&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;index_cpu_to_gpus_list&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;(index, co, gpus, ngpu)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    528&lt;/span&gt;         gpus &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; range&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;ngpu&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    529&lt;/span&gt;     res &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;ansi-blue-fg&quot;&gt;[&lt;/span&gt;StandardGpuResources&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ansi-green-fg&quot;&gt;for&lt;/span&gt; _ &lt;span class=&quot;ansi-green-fg&quot;&gt;in&lt;/span&gt; gpus&lt;span class=&quot;ansi-blue-fg&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;--&amp;gt; 530&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;     &lt;/span&gt;index_gpu &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; index_cpu_to_gpu_multiple_py&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;res&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; index&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; co&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; gpus&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    531&lt;/span&gt;     &lt;span class=&quot;ansi-green-fg&quot;&gt;return&lt;/span&gt; index_gpu
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    532&lt;/span&gt; 

&lt;span class=&quot;ansi-green-fg&quot;&gt;/usr/local/lib/python3.6/dist-packages/faiss/__init__.py&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;index_cpu_to_gpu_multiple_py&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;(resources, index, co, gpus)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    510&lt;/span&gt;         vdev&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;push_back&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;i&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    511&lt;/span&gt;         vres&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;push_back&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;res&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;--&amp;gt; 512&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;     &lt;/span&gt;index &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; index_cpu_to_gpu_multiple&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;vres&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; vdev&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; index&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; co&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    513&lt;/span&gt;     index&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;referenced_objects &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; resources
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    514&lt;/span&gt;     &lt;span class=&quot;ansi-green-fg&quot;&gt;return&lt;/span&gt; index

&lt;span class=&quot;ansi-green-fg&quot;&gt;/usr/local/lib/python3.6/dist-packages/faiss/swigfaiss.py&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;index_cpu_to_gpu_multiple&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;(resources, devices, index, options)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;   5054&lt;/span&gt; 
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;   5055&lt;/span&gt; &lt;span class=&quot;ansi-green-fg&quot;&gt;def&lt;/span&gt; index_cpu_to_gpu_multiple&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;resources&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; devices&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; index&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; options&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;ansi-green-fg&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;-&amp;gt; 5056&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;ansi-green-fg&quot;&gt;return&lt;/span&gt; _swigfaiss&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;index_cpu_to_gpu_multiple&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;resources&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; devices&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; index&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; options&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;   5057&lt;/span&gt; 
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;   5058&lt;/span&gt; &lt;span class=&quot;ansi-green-fg&quot;&gt;def&lt;/span&gt; swig_ptr&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;a&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;ansi-red-fg&quot;&gt;RuntimeError&lt;/span&gt;: Error in void faiss::gpu::allocMemorySpaceV(faiss::gpu::MemorySpace, void**, size_t) at gpu/utils/MemorySpace.cpp:26: Error: &amp;#39;err == cudaSuccess&amp;#39; failed: failed to cudaMalloc 1073741824 bytes (error 2 out of memory)&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;faiss&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# =====================================&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#            FAISS Setup&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# =====================================&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Build a flat (CPU) index&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cpu_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;faiss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IndexFlatL2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vecs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Use 1 GPU.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n_gpu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print the number of available GPU.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Number of available GPUs: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;    Using: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;faiss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_num_gpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_gpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# If using multiple GPUs, enable sharding so that the dataset is devided across&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# the GPUs rather than replicated.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;co&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;faiss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GpuMultipleClonerOptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;co&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shard&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Make it into a gpu index&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# gpu_index = faiss.index_cpu_to_all_gpus(cpu_index, co=co, ngpu=n_gpu)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add vecs to our GPU index&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Adding dataset to index...&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# gpu_index.add(vecs)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cpu_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Building index took &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; seconds&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Number of available GPUs: 1    Using: 1
Adding dataset to index...
Building index took 0.24 seconds
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Coment number 4 is short and sweet.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;==== Input Comment ====&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Comment #4:&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wrapper&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Let&amp;#39;s find the 5 most similar comments.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cpu_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;768&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;==== Top 5 Results ====&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For each result,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Look up the comment row number for this result.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Look up the text for this comment.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Comment #&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;:&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;L2 Distance: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wrapper&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;quot;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;quot;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;==== Input Comment ====
Comment #4:
    This page will need disambiguation.

==== Top 5 Results ====
Comment #4:
L2 Distance: 0.00
    &amp;#34;This page will need disambiguation. &amp;#34;

Comment #8,612:
L2 Distance: 16.44
    &amp;#34;why can&amp;#39;t i edit this page?&amp;#34;

Comment #39,578:
L2 Distance: 16.64
    &amp;#34;  This page needs to be expand.   &amp;#34;

Comment #2,051:
L2 Distance: 16.74
    &amp;#34; The article is somewhat confusing on this point - perhaps someone can
    offer some clarification.&amp;#34;

Comment #59,036:
L2 Distance: 17.31
    &amp;#34;  *As these issues remain uneddressed, this article has now been delisted.
    &amp;#34;

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;The meadning of this page needs to be clarified.&amp;quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Vectorize a new piece of text.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;query_vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_to_embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Let&amp;#39;s find the 5 most similar comments.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cpu_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_vec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;768&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;==== Top 5 Results ====&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For each result,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Look up the comment row number for this result.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Look up the text for this comment.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Comment #&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;:&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;L2Distance: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wrapper&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;quot;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;quot;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;
==== Top 5 Results ====
Comment #39,578:
L2Distance: 17.01
    &amp;#34;  This page needs to be expand.   &amp;#34;

Comment #88,451:
L2Distance: 17.39
    &amp;#34;  I intend to expand the article so it will be more complete. [  ]&amp;#34;

Comment #78,013:
L2Distance: 17.54
    &amp;#34;  == Full re-write ==  The article is in need of a full re-write; as
    written, it contains large guide-oriented sections and otherwise lacks
    encyclopedic character.   I will undertake rectifying these issues over the
    next few days; developing or expanding the content will not be addressed by
    me at this time.  &amp;#34;

Comment #3,035:
L2Distance: 17.57
    &amp;#34;Perhaps this should be moved to the page devoted to the album.  &amp;#34;

Comment #82,620:
L2Distance: 17.82
    &amp;#34;  ==Flags in table == This issue was addressed and resolved  here in 2008.
    Consensus may change; but further discussion is needed before disturbing the
    well-settled status quo.   &amp;#34;

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;This article really needs more citations.&amp;quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Vectorize a new piece of text.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;query_vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_to_embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Let&amp;#39;s find the 5 most similar comments.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cpu_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_vec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;768&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;==== Top 5 Results ====&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For each result,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Look up the comment row number for this result.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Look up the text for this comment.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Comment #&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;:&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;L2Distance: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wrapper&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;quot;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;quot;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;
==== Top 5 Results ====
Comment #77,417:
L2Distance: 13.69
    &amp;#34;       A couple of these images should be added to the article.   &amp;#34;

Comment #2,051:
L2Distance: 13.93
    &amp;#34; The article is somewhat confusing on this point - perhaps someone can
    offer some clarification.&amp;#34;

Comment #32,761:
L2Distance: 14.98
    &amp;#34; ::However, feel free to move that information deeper into the article.  &amp;#34;

Comment #59,036:
L2Distance: 15.46
    &amp;#34;  *As these issues remain uneddressed, this article has now been delisted.
    &amp;#34;

Comment #66,209:
L2Distance: 15.95
    &amp;#34;  ===Lead=== I&amp;#39;ll leave this until last, as it may need to reflect changes
    in the rest of the article.&amp;#34;

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;/div&gt;

&lt;script type=&quot;application/vnd.jupyter.widget-state+json&quot;&gt;
{&quot;830258b9784b453bb107ae35e9251adc&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;HBoxModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;HBoxView&quot;, &quot;_dom_classes&quot;: [], &quot;_model_name&quot;: &quot;HBoxModel&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.5.0&quot;, &quot;box_style&quot;: &quot;&quot;, &quot;layout&quot;: &quot;IPY_MODEL_0d17e83adf55405a9f25cec34ba0b3df&quot;, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;children&quot;: [&quot;IPY_MODEL_65f3c1ee6a3147ccbd1e49fe42875900&quot;, &quot;IPY_MODEL_499cc78cc5ff4bf7bd8b82d7187f8301&quot;]}}, &quot;0d17e83adf55405a9f25cec34ba0b3df&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;model_name&quot;: &quot;LayoutModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;LayoutView&quot;, &quot;grid_template_rows&quot;: null, &quot;right&quot;: null, &quot;justify_content&quot;: null, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;overflow&quot;: null, &quot;_model_module_version&quot;: &quot;1.2.0&quot;, &quot;_view_count&quot;: null, &quot;flex_flow&quot;: null, &quot;width&quot;: null, &quot;min_width&quot;: null, &quot;border&quot;: null, &quot;align_items&quot;: null, &quot;bottom&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;top&quot;: null, &quot;grid_column&quot;: null, &quot;overflow_y&quot;: null, &quot;overflow_x&quot;: null, &quot;grid_auto_flow&quot;: null, &quot;grid_area&quot;: null, &quot;grid_template_columns&quot;: null, &quot;flex&quot;: null, &quot;_model_name&quot;: &quot;LayoutModel&quot;, &quot;justify_items&quot;: null, &quot;grid_row&quot;: null, &quot;max_height&quot;: null, &quot;align_content&quot;: null, &quot;visibility&quot;: null, &quot;align_self&quot;: null, &quot;height&quot;: null, &quot;min_height&quot;: null, &quot;padding&quot;: null, &quot;grid_auto_rows&quot;: null, &quot;grid_gap&quot;: null, &quot;max_width&quot;: null, &quot;order&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;grid_template_areas&quot;: null, &quot;object_position&quot;: null, &quot;object_fit&quot;: null, &quot;grid_auto_columns&quot;: null, &quot;margin&quot;: null, &quot;display&quot;: null, &quot;left&quot;: null}}, &quot;65f3c1ee6a3147ccbd1e49fe42875900&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;IntProgressModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;ProgressView&quot;, &quot;style&quot;: &quot;IPY_MODEL_ff785394483b465bb7cf10cb8efcfe24&quot;, &quot;_dom_classes&quot;: [], &quot;description&quot;: &quot;Downloading: 100%&quot;, &quot;_model_name&quot;: &quot;IntProgressModel&quot;, &quot;bar_style&quot;: &quot;success&quot;, &quot;max&quot;: 231508, &quot;_view_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;value&quot;: 231508, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.5.0&quot;, &quot;orientation&quot;: &quot;horizontal&quot;, &quot;min&quot;: 0, &quot;description_tooltip&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;layout&quot;: &quot;IPY_MODEL_65cab788a0d84e97a61a0094cfa3988f&quot;}}, &quot;499cc78cc5ff4bf7bd8b82d7187f8301&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;HTMLModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;HTMLView&quot;, &quot;style&quot;: &quot;IPY_MODEL_95c1e160c0154e1498a6d80958a66f01&quot;, &quot;_dom_classes&quot;: [], &quot;description&quot;: &quot;&quot;, &quot;_model_name&quot;: &quot;HTMLModel&quot;, &quot;placeholder&quot;: &quot;\u200b&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;value&quot;: &quot; 232k/232k [00:51&amp;lt;00:00, 4.49kB/s]&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.5.0&quot;, &quot;description_tooltip&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;layout&quot;: &quot;IPY_MODEL_3dcfb2dfb62a40b695b9823d2f75db89&quot;}}, &quot;ff785394483b465bb7cf10cb8efcfe24&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;ProgressStyleModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;StyleView&quot;, &quot;_model_name&quot;: &quot;ProgressStyleModel&quot;, &quot;description_width&quot;: &quot;initial&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;bar_color&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;}}, &quot;65cab788a0d84e97a61a0094cfa3988f&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;model_name&quot;: &quot;LayoutModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;LayoutView&quot;, &quot;grid_template_rows&quot;: null, &quot;right&quot;: null, &quot;justify_content&quot;: null, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;overflow&quot;: null, &quot;_model_module_version&quot;: &quot;1.2.0&quot;, &quot;_view_count&quot;: null, &quot;flex_flow&quot;: null, &quot;width&quot;: null, &quot;min_width&quot;: null, &quot;border&quot;: null, &quot;align_items&quot;: null, &quot;bottom&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;top&quot;: null, &quot;grid_column&quot;: null, &quot;overflow_y&quot;: null, &quot;overflow_x&quot;: null, &quot;grid_auto_flow&quot;: null, &quot;grid_area&quot;: null, &quot;grid_template_columns&quot;: null, &quot;flex&quot;: null, &quot;_model_name&quot;: &quot;LayoutModel&quot;, &quot;justify_items&quot;: null, &quot;grid_row&quot;: null, &quot;max_height&quot;: null, &quot;align_content&quot;: null, &quot;visibility&quot;: null, &quot;align_self&quot;: null, &quot;height&quot;: null, &quot;min_height&quot;: null, &quot;padding&quot;: null, &quot;grid_auto_rows&quot;: null, &quot;grid_gap&quot;: null, &quot;max_width&quot;: null, &quot;order&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;grid_template_areas&quot;: null, &quot;object_position&quot;: null, &quot;object_fit&quot;: null, &quot;grid_auto_columns&quot;: null, &quot;margin&quot;: null, &quot;display&quot;: null, &quot;left&quot;: null}}, &quot;95c1e160c0154e1498a6d80958a66f01&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;DescriptionStyleModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;StyleView&quot;, &quot;_model_name&quot;: &quot;DescriptionStyleModel&quot;, &quot;description_width&quot;: &quot;&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;}}, &quot;3dcfb2dfb62a40b695b9823d2f75db89&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;model_name&quot;: &quot;LayoutModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;LayoutView&quot;, &quot;grid_template_rows&quot;: null, &quot;right&quot;: null, &quot;justify_content&quot;: null, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;overflow&quot;: null, &quot;_model_module_version&quot;: &quot;1.2.0&quot;, &quot;_view_count&quot;: null, &quot;flex_flow&quot;: null, &quot;width&quot;: null, &quot;min_width&quot;: null, &quot;border&quot;: null, &quot;align_items&quot;: null, &quot;bottom&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;top&quot;: null, &quot;grid_column&quot;: null, &quot;overflow_y&quot;: null, &quot;overflow_x&quot;: null, &quot;grid_auto_flow&quot;: null, &quot;grid_area&quot;: null, &quot;grid_template_columns&quot;: null, &quot;flex&quot;: null, &quot;_model_name&quot;: &quot;LayoutModel&quot;, &quot;justify_items&quot;: null, &quot;grid_row&quot;: null, &quot;max_height&quot;: null, &quot;align_content&quot;: null, &quot;visibility&quot;: null, &quot;align_self&quot;: null, &quot;height&quot;: null, &quot;min_height&quot;: null, &quot;padding&quot;: null, &quot;grid_auto_rows&quot;: null, &quot;grid_gap&quot;: null, &quot;max_width&quot;: null, &quot;order&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;grid_template_areas&quot;: null, &quot;object_position&quot;: null, &quot;object_fit&quot;: null, &quot;grid_auto_columns&quot;: null, &quot;margin&quot;: null, &quot;display&quot;: null, &quot;left&quot;: null}}, &quot;417f42664d2849e689c02ca1543c51e6&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;HBoxModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;HBoxView&quot;, &quot;_dom_classes&quot;: [], &quot;_model_name&quot;: &quot;HBoxModel&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.5.0&quot;, &quot;box_style&quot;: &quot;&quot;, &quot;layout&quot;: &quot;IPY_MODEL_0a81eb8b1535423f8fe3d2b1230d3f85&quot;, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;children&quot;: [&quot;IPY_MODEL_686cda0af1574bec969f237bac80d328&quot;, &quot;IPY_MODEL_fbdb96657af041cea5df84bd5bdfb99f&quot;]}}, &quot;0a81eb8b1535423f8fe3d2b1230d3f85&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;model_name&quot;: &quot;LayoutModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;LayoutView&quot;, &quot;grid_template_rows&quot;: null, &quot;right&quot;: null, &quot;justify_content&quot;: null, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;overflow&quot;: null, &quot;_model_module_version&quot;: &quot;1.2.0&quot;, &quot;_view_count&quot;: null, &quot;flex_flow&quot;: null, &quot;width&quot;: null, &quot;min_width&quot;: null, &quot;border&quot;: null, &quot;align_items&quot;: null, &quot;bottom&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;top&quot;: null, &quot;grid_column&quot;: null, &quot;overflow_y&quot;: null, &quot;overflow_x&quot;: null, &quot;grid_auto_flow&quot;: null, &quot;grid_area&quot;: null, &quot;grid_template_columns&quot;: null, &quot;flex&quot;: null, &quot;_model_name&quot;: &quot;LayoutModel&quot;, &quot;justify_items&quot;: null, &quot;grid_row&quot;: null, &quot;max_height&quot;: null, &quot;align_content&quot;: null, &quot;visibility&quot;: null, &quot;align_self&quot;: null, &quot;height&quot;: null, &quot;min_height&quot;: null, &quot;padding&quot;: null, &quot;grid_auto_rows&quot;: null, &quot;grid_gap&quot;: null, &quot;max_width&quot;: null, &quot;order&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;grid_template_areas&quot;: null, &quot;object_position&quot;: null, &quot;object_fit&quot;: null, &quot;grid_auto_columns&quot;: null, &quot;margin&quot;: null, &quot;display&quot;: null, &quot;left&quot;: null}}, &quot;686cda0af1574bec969f237bac80d328&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;IntProgressModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;ProgressView&quot;, &quot;style&quot;: &quot;IPY_MODEL_64dd5c9ea43d40199e4d7a7298788478&quot;, &quot;_dom_classes&quot;: [], &quot;description&quot;: &quot;Downloading: 100%&quot;, &quot;_model_name&quot;: &quot;IntProgressModel&quot;, &quot;bar_style&quot;: &quot;success&quot;, &quot;max&quot;: 433, &quot;_view_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;value&quot;: 433, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.5.0&quot;, &quot;orientation&quot;: &quot;horizontal&quot;, &quot;min&quot;: 0, &quot;description_tooltip&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;layout&quot;: &quot;IPY_MODEL_ddc00730143944c2aa5b22d2ddc13efd&quot;}}, &quot;fbdb96657af041cea5df84bd5bdfb99f&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;HTMLModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;HTMLView&quot;, &quot;style&quot;: &quot;IPY_MODEL_83826c3a18324b588c89d50b4e5396a4&quot;, &quot;_dom_classes&quot;: [], &quot;description&quot;: &quot;&quot;, &quot;_model_name&quot;: &quot;HTMLModel&quot;, &quot;placeholder&quot;: &quot;\u200b&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;value&quot;: &quot; 433/433 [01:10&amp;lt;00:00, 6.12B/s]&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.5.0&quot;, &quot;description_tooltip&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;layout&quot;: &quot;IPY_MODEL_6e36c64770a94099b5c64cdd141b6a91&quot;}}, &quot;64dd5c9ea43d40199e4d7a7298788478&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;ProgressStyleModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;StyleView&quot;, &quot;_model_name&quot;: &quot;ProgressStyleModel&quot;, &quot;description_width&quot;: &quot;initial&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;bar_color&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;}}, &quot;ddc00730143944c2aa5b22d2ddc13efd&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;model_name&quot;: &quot;LayoutModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;LayoutView&quot;, &quot;grid_template_rows&quot;: null, &quot;right&quot;: null, &quot;justify_content&quot;: null, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;overflow&quot;: null, &quot;_model_module_version&quot;: &quot;1.2.0&quot;, &quot;_view_count&quot;: null, &quot;flex_flow&quot;: null, &quot;width&quot;: null, &quot;min_width&quot;: null, &quot;border&quot;: null, &quot;align_items&quot;: null, &quot;bottom&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;top&quot;: null, &quot;grid_column&quot;: null, &quot;overflow_y&quot;: null, &quot;overflow_x&quot;: null, &quot;grid_auto_flow&quot;: null, &quot;grid_area&quot;: null, &quot;grid_template_columns&quot;: null, &quot;flex&quot;: null, &quot;_model_name&quot;: &quot;LayoutModel&quot;, &quot;justify_items&quot;: null, &quot;grid_row&quot;: null, &quot;max_height&quot;: null, &quot;align_content&quot;: null, &quot;visibility&quot;: null, &quot;align_self&quot;: null, &quot;height&quot;: null, &quot;min_height&quot;: null, &quot;padding&quot;: null, &quot;grid_auto_rows&quot;: null, &quot;grid_gap&quot;: null, &quot;max_width&quot;: null, &quot;order&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;grid_template_areas&quot;: null, &quot;object_position&quot;: null, &quot;object_fit&quot;: null, &quot;grid_auto_columns&quot;: null, &quot;margin&quot;: null, &quot;display&quot;: null, &quot;left&quot;: null}}, &quot;83826c3a18324b588c89d50b4e5396a4&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;DescriptionStyleModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;StyleView&quot;, &quot;_model_name&quot;: &quot;DescriptionStyleModel&quot;, &quot;description_width&quot;: &quot;&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;}}, &quot;6e36c64770a94099b5c64cdd141b6a91&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;model_name&quot;: &quot;LayoutModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;LayoutView&quot;, &quot;grid_template_rows&quot;: null, &quot;right&quot;: null, &quot;justify_content&quot;: null, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;overflow&quot;: null, &quot;_model_module_version&quot;: &quot;1.2.0&quot;, &quot;_view_count&quot;: null, &quot;flex_flow&quot;: null, &quot;width&quot;: null, &quot;min_width&quot;: null, &quot;border&quot;: null, &quot;align_items&quot;: null, &quot;bottom&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;top&quot;: null, &quot;grid_column&quot;: null, &quot;overflow_y&quot;: null, &quot;overflow_x&quot;: null, &quot;grid_auto_flow&quot;: null, &quot;grid_area&quot;: null, &quot;grid_template_columns&quot;: null, &quot;flex&quot;: null, &quot;_model_name&quot;: &quot;LayoutModel&quot;, &quot;justify_items&quot;: null, &quot;grid_row&quot;: null, &quot;max_height&quot;: null, &quot;align_content&quot;: null, &quot;visibility&quot;: null, &quot;align_self&quot;: null, &quot;height&quot;: null, &quot;min_height&quot;: null, &quot;padding&quot;: null, &quot;grid_auto_rows&quot;: null, &quot;grid_gap&quot;: null, &quot;max_width&quot;: null, &quot;order&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;grid_template_areas&quot;: null, &quot;object_position&quot;: null, &quot;object_fit&quot;: null, &quot;grid_auto_columns&quot;: null, &quot;margin&quot;: null, &quot;display&quot;: null, &quot;left&quot;: null}}, &quot;e06f31d75f094d51beac92b3f6f14fd0&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;HBoxModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;HBoxView&quot;, &quot;_dom_classes&quot;: [], &quot;_model_name&quot;: &quot;HBoxModel&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.5.0&quot;, &quot;box_style&quot;: &quot;&quot;, &quot;layout&quot;: &quot;IPY_MODEL_8ae425e1daaa4d369bbfc2651c7af998&quot;, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;children&quot;: [&quot;IPY_MODEL_a79d01067f3a4b3f9fb8b9c4bddd8d7a&quot;, &quot;IPY_MODEL_7e91f60fb1c94ca2b661629b38ebbce2&quot;]}}, &quot;8ae425e1daaa4d369bbfc2651c7af998&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;model_name&quot;: &quot;LayoutModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;LayoutView&quot;, &quot;grid_template_rows&quot;: null, &quot;right&quot;: null, &quot;justify_content&quot;: null, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;overflow&quot;: null, &quot;_model_module_version&quot;: &quot;1.2.0&quot;, &quot;_view_count&quot;: null, &quot;flex_flow&quot;: null, &quot;width&quot;: null, &quot;min_width&quot;: null, &quot;border&quot;: null, &quot;align_items&quot;: null, &quot;bottom&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;top&quot;: null, &quot;grid_column&quot;: null, &quot;overflow_y&quot;: null, &quot;overflow_x&quot;: null, &quot;grid_auto_flow&quot;: null, &quot;grid_area&quot;: null, &quot;grid_template_columns&quot;: null, &quot;flex&quot;: null, &quot;_model_name&quot;: &quot;LayoutModel&quot;, &quot;justify_items&quot;: null, &quot;grid_row&quot;: null, &quot;max_height&quot;: null, &quot;align_content&quot;: null, &quot;visibility&quot;: null, &quot;align_self&quot;: null, &quot;height&quot;: null, &quot;min_height&quot;: null, &quot;padding&quot;: null, &quot;grid_auto_rows&quot;: null, &quot;grid_gap&quot;: null, &quot;max_width&quot;: null, &quot;order&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;grid_template_areas&quot;: null, &quot;object_position&quot;: null, &quot;object_fit&quot;: null, &quot;grid_auto_columns&quot;: null, &quot;margin&quot;: null, &quot;display&quot;: null, &quot;left&quot;: null}}, &quot;a79d01067f3a4b3f9fb8b9c4bddd8d7a&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;IntProgressModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;ProgressView&quot;, &quot;style&quot;: &quot;IPY_MODEL_ebe8874127c6474d901a7ae1396a7468&quot;, &quot;_dom_classes&quot;: [], &quot;description&quot;: &quot;Downloading: 100%&quot;, &quot;_model_name&quot;: &quot;IntProgressModel&quot;, &quot;bar_style&quot;: &quot;success&quot;, &quot;max&quot;: 440473133, &quot;_view_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;value&quot;: 440473133, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.5.0&quot;, &quot;orientation&quot;: &quot;horizontal&quot;, &quot;min&quot;: 0, &quot;description_tooltip&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;layout&quot;: &quot;IPY_MODEL_107a6f3afd6c4733937c5648b44e098b&quot;}}, &quot;7e91f60fb1c94ca2b661629b38ebbce2&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;HTMLModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;HTMLView&quot;, &quot;style&quot;: &quot;IPY_MODEL_a9487cb255524bb5bf7923a236128320&quot;, &quot;_dom_classes&quot;: [], &quot;description&quot;: &quot;&quot;, &quot;_model_name&quot;: &quot;HTMLModel&quot;, &quot;placeholder&quot;: &quot;\u200b&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;value&quot;: &quot; 440M/440M [00:45&amp;lt;00:00, 9.62MB/s]&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.5.0&quot;, &quot;description_tooltip&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;layout&quot;: &quot;IPY_MODEL_6db3980b85da4f81b0198cabd277dc6a&quot;}}, &quot;ebe8874127c6474d901a7ae1396a7468&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;ProgressStyleModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;StyleView&quot;, &quot;_model_name&quot;: &quot;ProgressStyleModel&quot;, &quot;description_width&quot;: &quot;initial&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;bar_color&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;}}, &quot;107a6f3afd6c4733937c5648b44e098b&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;model_name&quot;: &quot;LayoutModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;LayoutView&quot;, &quot;grid_template_rows&quot;: null, &quot;right&quot;: null, &quot;justify_content&quot;: null, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;overflow&quot;: null, &quot;_model_module_version&quot;: &quot;1.2.0&quot;, &quot;_view_count&quot;: null, &quot;flex_flow&quot;: null, &quot;width&quot;: null, &quot;min_width&quot;: null, &quot;border&quot;: null, &quot;align_items&quot;: null, &quot;bottom&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;top&quot;: null, &quot;grid_column&quot;: null, &quot;overflow_y&quot;: null, &quot;overflow_x&quot;: null, &quot;grid_auto_flow&quot;: null, &quot;grid_area&quot;: null, &quot;grid_template_columns&quot;: null, &quot;flex&quot;: null, &quot;_model_name&quot;: &quot;LayoutModel&quot;, &quot;justify_items&quot;: null, &quot;grid_row&quot;: null, &quot;max_height&quot;: null, &quot;align_content&quot;: null, &quot;visibility&quot;: null, &quot;align_self&quot;: null, &quot;height&quot;: null, &quot;min_height&quot;: null, &quot;padding&quot;: null, &quot;grid_auto_rows&quot;: null, &quot;grid_gap&quot;: null, &quot;max_width&quot;: null, &quot;order&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;grid_template_areas&quot;: null, &quot;object_position&quot;: null, &quot;object_fit&quot;: null, &quot;grid_auto_columns&quot;: null, &quot;margin&quot;: null, &quot;display&quot;: null, &quot;left&quot;: null}}, &quot;a9487cb255524bb5bf7923a236128320&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/controls&quot;, &quot;model_name&quot;: &quot;DescriptionStyleModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;StyleView&quot;, &quot;_model_name&quot;: &quot;DescriptionStyleModel&quot;, &quot;description_width&quot;: &quot;&quot;, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;_model_module_version&quot;: &quot;1.5.0&quot;, &quot;_view_count&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;_model_module&quot;: &quot;@jupyter-widgets/controls&quot;}}, &quot;6db3980b85da4f81b0198cabd277dc6a&quot;: {&quot;model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;model_name&quot;: &quot;LayoutModel&quot;, &quot;state&quot;: {&quot;_view_name&quot;: &quot;LayoutView&quot;, &quot;grid_template_rows&quot;: null, &quot;right&quot;: null, &quot;justify_content&quot;: null, &quot;_view_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;overflow&quot;: null, &quot;_model_module_version&quot;: &quot;1.2.0&quot;, &quot;_view_count&quot;: null, &quot;flex_flow&quot;: null, &quot;width&quot;: null, &quot;min_width&quot;: null, &quot;border&quot;: null, &quot;align_items&quot;: null, &quot;bottom&quot;: null, &quot;_model_module&quot;: &quot;@jupyter-widgets/base&quot;, &quot;top&quot;: null, &quot;grid_column&quot;: null, &quot;overflow_y&quot;: null, &quot;overflow_x&quot;: null, &quot;grid_auto_flow&quot;: null, &quot;grid_area&quot;: null, &quot;grid_template_columns&quot;: null, &quot;flex&quot;: null, &quot;_model_name&quot;: &quot;LayoutModel&quot;, &quot;justify_items&quot;: null, &quot;grid_row&quot;: null, &quot;max_height&quot;: null, &quot;align_content&quot;: null, &quot;visibility&quot;: null, &quot;align_self&quot;: null, &quot;height&quot;: null, &quot;min_height&quot;: null, &quot;padding&quot;: null, &quot;grid_auto_rows&quot;: null, &quot;grid_gap&quot;: null, &quot;max_width&quot;: null, &quot;order&quot;: null, &quot;_view_module_version&quot;: &quot;1.2.0&quot;, &quot;grid_template_areas&quot;: null, &quot;object_position&quot;: null, &quot;object_fit&quot;: null, &quot;grid_auto_columns&quot;: null, &quot;margin&quot;: null, &quot;display&quot;: null, &quot;left&quot;: null}}}
&lt;/script&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">BERT Fine-Tuning Sentence Classification</title><link href="https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning-Sentence-Classification.html" rel="alternate" type="text/html" title="BERT Fine-Tuning Sentence Classification" /><published>2020-04-19T00:00:00-05:00</published><updated>2020-04-19T00:00:00-05:00</updated><id>https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning-Sentence-Classification</id><content type="html" xml:base="https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning-Sentence-Classification.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-19-BERT_Fine-Tuning-Sentence-Classification.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;1.-Setup&quot;&gt;1. Setup&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-Setup&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;1.1.-Using-Colab-GPU-for-Training&quot;&gt;1.1. Using Colab GPU for Training&lt;a class=&quot;anchor-link&quot; href=&quot;#1.1.-Using-Colab-GPU-for-Training&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Get the GPU device name.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;device_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gpu_device_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The device name should look like the following:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;/device:GPU:0&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Found GPU at: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;ne&quot;&gt;SystemError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;GPU device not found&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Found GPU at: /device:GPU:0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# If there&amp;#39;s a GPU available,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Tell PyTorch to user the GPU.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;cuda&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;There are &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; GPU(s) available.&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;We will user the GPU:&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_device_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# If not,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;No GPU available, using the CPU instead.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;cpu&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;There are 1 GPU(s) available.
We will user the GPU: Tesla T4
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.2.-Installing-the-Hugging-Face-Library&quot;&gt;1.2. Installing the Hugging Face Library&lt;a class=&quot;anchor-link&quot; href=&quot;#1.2.-Installing-the-Hugging-Face-Library&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; pip install transformers -q
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; pip install wget -q
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;wget&lt;/span&gt; 
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Downloading dataset...&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The URL for the dataset zip file.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;https://nyu-mll.github.io/CoLA/cola_public_1.1.zip&amp;#39;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Download the file (if we haven&amp;#39;t already)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;./cola_public_1.1.zip&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;wget&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;./cola_public_1.1.zip&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Downloading dataset...
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Unzip the dataset (if we haven&amp;#39;t already)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;./cola_public/&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; unzip cola_public_1.1.zip
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; head cola_public/*/*
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;==&amp;gt; cola_public/raw/in_domain_dev.tsv &amp;lt;==
gj04	1		The sailors rode the breeze clear of the rocks.
gj04	1		The weights made the rope stretch over the pulley.
gj04	1		The mechanical doll wriggled itself loose.
cj99	1		If you had eaten more, you would want less.
cj99	0	*	As you eat the most, you want the least.
cj99	0	*	The more you would want, the less you would eat.
cj99	0	*	I demand that the more John eat, the more he pays.
cj99	1		Mary listens to the Grateful Dead, she gets depressed.
cj99	1		The angrier Mary got, the more she looked at pictures.
cj99	1		The higher the stakes, the lower his expectations are.

==&amp;gt; cola_public/raw/in_domain_train.tsv &amp;lt;==
gj04	1		Our friends won&amp;#39;t buy this analysis, let alone the next one we propose.
gj04	1		One more pseudo generalization and I&amp;#39;m giving up.
gj04	1		One more pseudo generalization or I&amp;#39;m giving up.
gj04	1		The more we study verbs, the crazier they get.
gj04	1		Day by day the facts are getting murkier.
gj04	1		I&amp;#39;ll fix you a drink.
gj04	1		Fred watered the plants flat.
gj04	1		Bill coughed his way out of the restaurant.
gj04	1		We&amp;#39;re dancing the night away.
gj04	1		Herman hammered the metal flat.

==&amp;gt; cola_public/raw/out_of_domain_dev.tsv &amp;lt;==
clc95	1		Somebody just left - guess who.
clc95	1		They claimed they had settled on something, but it wasn&amp;#39;t clear what they had settled on.
clc95	1		If Sam was going, Sally would know where.
clc95	1		They&amp;#39;re going to serve the guests something, but it&amp;#39;s unclear what.
clc95	1		She&amp;#39;s reading. I can&amp;#39;t imagine what.
clc95	1		John said Joan saw someone from her graduating class.
clc95	0	*	John ate dinner but I don&amp;#39;t know who.
clc95	0	*	She mailed John a letter, but I don&amp;#39;t know to whom.
clc95	1		I served leek soup to my guests.
clc95	1		I served my guests.

==&amp;gt; cola_public/tokenized/in_domain_dev.tsv &amp;lt;==
gj04	1		the sailors rode the breeze clear of the rocks .
gj04	1		the weights made the rope stretch over the pulley .
gj04	1		the mechanical doll wriggled itself loose .
cj99	1		if you had eaten more , you would want less .
cj99	0	*	as you eat the most , you want the least .
cj99	0	*	the more you would want , the less you would eat .
cj99	0	*	i demand that the more john eat , the more he pays .
cj99	1		mary listens to the grateful dead , she gets depressed .
cj99	1		the angrier mary got , the more she looked at pictures .
cj99	1		the higher the stakes , the lower his expectations are .

==&amp;gt; cola_public/tokenized/in_domain_train.tsv &amp;lt;==
gj04	1		our friends wo n&amp;#39;t buy this analysis , let alone the next one we propose .
gj04	1		one more pseudo generalization and i &amp;#39;m giving up .
gj04	1		one more pseudo generalization or i &amp;#39;m giving up .
gj04	1		the more we study verbs , the crazier they get .
gj04	1		day by day the facts are getting murkier .
gj04	1		i &amp;#39;ll fix you a drink .
gj04	1		fred watered the plants flat .
gj04	1		bill coughed his way out of the restaurant .
gj04	1		we &amp;#39;re dancing the night away .
gj04	1		herman hammered the metal flat .

==&amp;gt; cola_public/tokenized/out_of_domain_dev.tsv &amp;lt;==
clc95	1		somebody just left - guess who .
clc95	1		they claimed they had settled on something , but it was n&amp;#39;t clear what they had settled on .
clc95	1		if sam was going , sally would know where .
clc95	1		they &amp;#39;re going to serve the guests something , but it &amp;#39;s unclear what .
clc95	1		she &amp;#39;s reading . i ca n&amp;#39;t imagine what .
clc95	1		john said joan saw someone from her graduating class .
clc95	0	*	john ate dinner but i do n&amp;#39;t know who .
clc95	0	*	she mailed john a letter , but i do n&amp;#39;t know to whom .
clc95	1		i served leek soup to my guests .
clc95	1		i served my guests .
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.2.-Parse&quot;&gt;2.2. Parse&lt;a class=&quot;anchor-link&quot; href=&quot;#2.2.-Parse&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Load the dataset into a pandas dataframe.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;./cola_public/raw/in_domain_train.tsv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delimiter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;sentence_source&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;label&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;label_notes&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;sentence&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Report the number of sentences.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Number of training sentences: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Display 10 random rows from the data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Number of training sentences: 8,551

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sentence_source&lt;/th&gt;
      &lt;th&gt;label&lt;/th&gt;
      &lt;th&gt;label_notes&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1643&lt;/th&gt;
      &lt;td&gt;r-67&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;I brought a razor with which to shave myself.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;558&lt;/th&gt;
      &lt;td&gt;bc01&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;A bear inhabits the cave.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7840&lt;/th&gt;
      &lt;td&gt;ad03&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;The therapist's analysis of Lucy&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4713&lt;/th&gt;
      &lt;td&gt;ks08&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;This noise cannot be put up with.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5501&lt;/th&gt;
      &lt;td&gt;b_73&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Such a scholar as you were speaking of just no...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;568&lt;/th&gt;
      &lt;td&gt;bc01&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;The rolling stone avoided the river.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1920&lt;/th&gt;
      &lt;td&gt;r-67&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;I would prefer it for there to be no talking.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2492&lt;/th&gt;
      &lt;td&gt;l-93&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;He worked his way through the book.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5364&lt;/th&gt;
      &lt;td&gt;b_73&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Enough is going on to keep them confused.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5344&lt;/th&gt;
      &lt;td&gt;b_73&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Mary is taller than six feet.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;sentence&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;label&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
      &lt;th&gt;label&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;444&lt;/th&gt;
      &lt;td&gt;Mickey slips all the time up.&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;187&lt;/th&gt;
      &lt;td&gt;The more books I ask to whom he will give, the...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4866&lt;/th&gt;
      &lt;td&gt;This needs investigating the problem.&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7780&lt;/th&gt;
      &lt;td&gt;We believed to be omnipotent.&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1267&lt;/th&gt;
      &lt;td&gt;What table will he put the chair between and s...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Get the lists of sentences and their labels.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;3.-Tokenization-&amp;amp;-Input-Formatting&quot;&gt;3. Tokenization &amp;amp; Input Formatting&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-Tokenization-&amp;amp;-Input-Formatting&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;3.1.-BERT-Tokenizer&quot;&gt;3.1. BERT Tokenizer&lt;a class=&quot;anchor-link&quot; href=&quot;#3.1.-BERT-Tokenizer&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Load the BERT tokenizer.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Loading BERT tokenizer...&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;bert-base-multilingual-uncased&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;do_lower_case&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Loading BERT tokenizer...
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Print the original sentence.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39; Original: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print the sentence split into tokens.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Tokenized: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print the sentence mapped to token ids.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Token IDs: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_tokens_to_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt; Original:  Our friends won&amp;#39;t buy this analysis, let alone the next one we propose.
Tokenized:  [&amp;#39;our&amp;#39;, &amp;#39;friends&amp;#39;, &amp;#39;won&amp;#39;, &amp;#34;&amp;#39;&amp;#34;, &amp;#39;t&amp;#39;, &amp;#39;buy&amp;#39;, &amp;#39;this&amp;#39;, &amp;#39;analysis&amp;#39;, &amp;#39;,&amp;#39;, &amp;#39;let&amp;#39;, &amp;#39;alone&amp;#39;, &amp;#39;the&amp;#39;, &amp;#39;next&amp;#39;, &amp;#39;one&amp;#39;, &amp;#39;we&amp;#39;, &amp;#39;propose&amp;#39;, &amp;#39;.&amp;#39;]
Token IDs:  [14008, 16119, 11441, 112, 162, 35172, 10372, 15559, 117, 12421, 19145, 10103, 12878, 10399, 11312, 25690, 119]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;korean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;안녕하세요. 반갑습니다. 너는 이름이 뭐니? 오늘 날씨가 맑고 좋구나.&amp;quot;&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Print the original sentence.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39; Original: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;korean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print the sentence split into tokens.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Tokenized: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;korean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print the sentence mapped to token ids.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Token IDs: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_tokens_to_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;korean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt; Original:  안녕하세요. 반갑습니다. 너는 이름이 뭐니? 오늘 날씨가 맑고 좋구나.
Tokenized:  [&amp;#39;ᄋ&amp;#39;, &amp;#39;##ᅡᆫ&amp;#39;, &amp;#39;##녀&amp;#39;, &amp;#39;##ᆼ&amp;#39;, &amp;#39;##하&amp;#39;, &amp;#39;##세&amp;#39;, &amp;#39;##요&amp;#39;, &amp;#39;.&amp;#39;, &amp;#39;ᄇ&amp;#39;, &amp;#39;##ᅡᆫ&amp;#39;, &amp;#39;##가&amp;#39;, &amp;#39;##ᆸ&amp;#39;, &amp;#39;##스&amp;#39;, &amp;#39;##ᆸ니다&amp;#39;, &amp;#39;.&amp;#39;, &amp;#39;ᄂ&amp;#39;, &amp;#39;##ᅥ&amp;#39;, &amp;#39;##는&amp;#39;, &amp;#39;이&amp;#39;, &amp;#39;##름이&amp;#39;, &amp;#39;ᄆ&amp;#39;, &amp;#39;##ᅯ&amp;#39;, &amp;#39;##니&amp;#39;, &amp;#39;?&amp;#39;, &amp;#39;ᄋ&amp;#39;, &amp;#39;##ᅩ&amp;#39;, &amp;#39;##ᄂ&amp;#39;, &amp;#39;##ᅳᆯ&amp;#39;, &amp;#39;날&amp;#39;, &amp;#39;##씨&amp;#39;, &amp;#39;##가&amp;#39;, &amp;#39;ᄆ&amp;#39;, &amp;#39;##ᅡ&amp;#39;, &amp;#39;##ᆰ&amp;#39;, &amp;#39;##고&amp;#39;, &amp;#39;ᄌ&amp;#39;, &amp;#39;##ᅩ&amp;#39;, &amp;#39;##ᇂ&amp;#39;, &amp;#39;##구&amp;#39;, &amp;#39;##나&amp;#39;, &amp;#39;.&amp;#39;]
Token IDs:  [1174, 26646, 49345, 13045, 35132, 25169, 47024, 119, 1170, 26646, 11376, 17360, 13212, 79427, 119, 1165, 33645, 11192, 12398, 89420, 1169, 97090, 25536, 136, 1174, 29347, 97071, 63277, 76818, 47928, 11376, 1169, 25539, 97098, 12300, 1175, 29347, 97109, 16336, 16801, 119]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When we actually convert all of our sentences, we'll use the &lt;code&gt;tokenize.encode&lt;/code&gt; functio to handle both steps, rather than calling &lt;code&gt;tokenize&lt;/code&gt; and &lt;code&gt;convert_tokens_to_ids&lt;/code&gt; seperately.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.2.-Required-Formatting&quot;&gt;3.2. Required Formatting&lt;a class=&quot;anchor-link&quot; href=&quot;#3.2.-Required-Formatting&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.3.-Tokenize-Dataset&quot;&gt;3.3. Tokenize Dataset&lt;a class=&quot;anchor-link&quot; href=&quot;#3.3.-Tokenize-Dataset&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For every sentence,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sent&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Tokenize the text and add `[CLS]` and `[SEP]` tokens.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;add_special_tokens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Update the maximum sentence langth.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Max sentence length: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Max sentence length:  48
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Tokenize all of the sentences and map the tokens to their word IDs.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For every sentence,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sent&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# `encode_plus` will:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (1) Tokenize the sentence.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (2) Prepend the `[CLS]` token to the start.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (3) Append the `[SEP]` token to the end.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (4) Map tokens to their IDs.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (5) Pad or truncate the sentence to `max_length`&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (6) Create attention masks for [PAD] tokens.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;encoded_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode_plus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                                       &lt;span class=&quot;c1&quot;&gt;# Sentence to encode.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;add_special_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# Add &amp;#39;[CLS]&amp;#39; and &amp;#39;[SEP]&amp;#39;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                           &lt;span class=&quot;c1&quot;&gt;# Pad &amp;amp; truncate all sentences.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pad_to_max_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;return_attention_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Construct attn. masks.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;return_Tensors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;pt&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                  &lt;span class=&quot;c1&quot;&gt;# Return pytorch tensors.&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Add the encoded sentence to the list.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;input_ids&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# And its attention mask (simply differentiates padding from non-padding.)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;attention_mask&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Convert the lists into tensors.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LongTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FloatTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print sentence 0, now as a list of IDs.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Original:  &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Token IDs: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Original:   Our friends won&amp;#39;t buy this analysis, let alone the next one we propose.
Token IDs:  tensor([  101, 14008, 16119, 11441,   112,   162, 35172, 10372, 15559,   117,
        12421, 19145, 10103, 12878, 10399, 11312, 25690,   119,   102,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.4.-Training-&amp;amp;-Validation-Split&quot;&gt;3.4. Training &amp;amp; Validation Split&lt;a class=&quot;anchor-link&quot; href=&quot;#3.4.-Training-&amp;amp;-Validation-Split&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_split&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Combine the training inputs into a TensorDataset.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create a 90-10 train-validation split.&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Calculate the number of samples to include in each set.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;val_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Devide the dataset by randomly selecting samples.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;5}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; training samples&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;5}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; validation samples&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt; 7695 training samples
  856 validation samples
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SequentialSampler&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The DataLoader needs to know our batch size for training, so we specify it here.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create the DataLoaders for our training and validation sets.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# We&amp;#39;ll take training samples in random order.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# The training samples.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Select batches randomly&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Trains with this batch size.&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For validation the order doesn&amp;#39;t matter, so we&amp;#39;ll just read them sequentially.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validation_dataloader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# The validation samples.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SequentialSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Pull out batches sequentially.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Evaluate with this batch size.&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 7695/32 = 240.46875&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;32&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;4.-Train-Our-Classification-Model&quot;&gt;4. Train Our Classification Model&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-Train-Our-Classification-Model&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;4.1.-BertForSequenceClassification&quot;&gt;4.1. BertForSequenceClassification&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.-BertForSequenceClassification&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;BertModel&lt;/li&gt;
&lt;li&gt;BertForPreTraining&lt;/li&gt;
&lt;li&gt;BertForMaskedLM&lt;/li&gt;
&lt;li&gt;BertForNextSentencePredicion&lt;/li&gt;
&lt;li&gt;BertForSequenceClassification&lt;/li&gt;
&lt;li&gt;BertForTokenClassification&lt;/li&gt;
&lt;li&gt;BertForQuestionAnswering&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;device(type=&amp;#39;cuda&amp;#39;)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForSequenceClassification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdamW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertConfig&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Load BertForSequenceClassification, the pretrained BERT model with &lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# a single linear classification layer on top.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForSequenceClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;bert-base-uncased&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Use the 12-layer BERT model, with an uncased vocab.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# The number of output labels--2 for binary classification.&lt;/span&gt;
                                &lt;span class=&quot;c1&quot;&gt;# You can increase this for multi-class tasks.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output_attentions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Whether the model returns attentions weights.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output_hidden_states&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Whether the model returns all hidden-states.&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Tell pytorch to run this model on the GPU.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# if device == &amp;#39;cuda&amp;#39;:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Get all of the model&amp;#39;s parameters as a list of tuples.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;named_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;The BERT model has &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; different named parameters.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;==== Embedding Layer ====&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;lt;55}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;12}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))))&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;==== First Transformer ====&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;lt;55}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;12}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))))&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;==== Output Layer ====&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;lt;55}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;12}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;The BERT model has 201 different named parameters.

==== Embedding Layer ====

bert.embeddings.word_embeddings.weight                  (30522, 768)
bert.embeddings.position_embeddings.weight                (512, 768)
bert.embeddings.token_type_embeddings.weight                (2, 768)
bert.embeddings.LayerNorm.weight                              (768,)
bert.embeddings.LayerNorm.bias                                (768,)

==== First Transformer ====

bert.encoder.layer.0.attention.self.query.weight          (768, 768)
bert.encoder.layer.0.attention.self.query.bias                (768,)
bert.encoder.layer.0.attention.self.key.weight            (768, 768)
bert.encoder.layer.0.attention.self.key.bias                  (768,)
bert.encoder.layer.0.attention.self.value.weight          (768, 768)
bert.encoder.layer.0.attention.self.value.bias                (768,)
bert.encoder.layer.0.attention.output.dense.weight        (768, 768)
bert.encoder.layer.0.attention.output.dense.bias              (768,)
bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)
bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)
bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)
bert.encoder.layer.0.intermediate.dense.bias                 (3072,)
bert.encoder.layer.0.output.dense.weight                 (768, 3072)
bert.encoder.layer.0.output.dense.bias                        (768,)
bert.encoder.layer.0.output.LayerNorm.weight                  (768,)
bert.encoder.layer.0.output.LayerNorm.bias                    (768,)

==== Output Layer ====

bert.pooler.dense.weight                                  (768, 768)
bert.pooler.dense.bias                                        (768,)
classifier.weight                                           (2, 768)
classifier.bias                                                 (2,)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.2.-Optimizer-&amp;amp;-Learning-Rate-Scheduler&quot;&gt;4.2. Optimizer &amp;amp; Learning Rate Scheduler&lt;a class=&quot;anchor-link&quot; href=&quot;#4.2.-Optimizer-&amp;amp;-Learning-Rate-Scheduler&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Note: AdamW is a class from the huggingface library (as opposed to pytorch)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# I believe the &amp;#39;W&amp;#39; stands for &amp;#39;Weight Decay fix&amp;#39;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdamW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2e-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# args.learning_rate - default is 5e-5, our notebook had 2e-5&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-8&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# args.dam_epsilon  - default is 1e-8&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_linear_schedule_with_warmup&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Number of training epochs. The BERT authors recommend between 2 and 4.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# We chose to run for 4, but we&amp;#39;ll see later that this my be over-fitting the training data.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Total number of training steps is [number of batches] x [number of epochs].&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# (Note that this is not the same as the number of training samples).&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;total_steps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create the learning rate scheduler.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_linear_schedule_with_warmup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                            &lt;span class=&quot;n&quot;&gt;num_warmup_steps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Default value in run_glue.py&lt;/span&gt;
                                            &lt;span class=&quot;n&quot;&gt;num_training_steps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.3.-Training-Loop&quot;&gt;4.3. Training Loop&lt;a class=&quot;anchor-link&quot; href=&quot;#4.3.-Training-Loop&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unpack our data inputs and labels&lt;/li&gt;
&lt;li&gt;Load data onto the GPU for acceleration&lt;/li&gt;
&lt;li&gt;Clear out the gradients calculated in the previous pass.&lt;ul&gt;
&lt;li&gt;In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Forward pass (feed input data through the network)&lt;/li&gt;
&lt;li&gt;Backward pass (backpropagation)&lt;/li&gt;
&lt;li&gt;Tell the network to update parameters with optimizer.step()&lt;/li&gt;
&lt;li&gt;Track variables for monitoring progress&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Evaluation:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unpack our data inputs and labels&lt;/li&gt;
&lt;li&gt;Load data onto the GPU for acceleration&lt;/li&gt;
&lt;li&gt;Forward pass (feed input data through the network)&lt;/li&gt;
&lt;li&gt;Compute loss on our validation data and track variables for monitoring progress&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Function to calculate the accuracy of our predictions vs labels&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;flat_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pred_flat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;labels_flat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_flat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels_flat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels_flat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;format_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Takes a time in seconds and returns a string hh:mm:ss&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Round to the nearest second.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;elapsed_rounded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Format as hh:mm:ss&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;    &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elapsed_rounded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elapsed_rounded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elapsed_rounded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# This training code is based on the `run_glue.py` script here:&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Set the seed value all over the place to make this reproducible.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;seed_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;manual_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;manual_seed_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# We&amp;#39;ll store a number of quantities such as training and validation loss,&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# validation accuracy, and timings.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_stats&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Measure the total training time for the whole run.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;total_t0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For each epoch,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch_i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# ========================================&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#                                 Training &lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ========================================&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Perform one full pass over the training set.&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;======== Epoch &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; / &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; ========&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch_i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Training...&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Measure how long the training epoch takes.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Reset the total loss for this epoch.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total_train_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Put the model into training mode. Don&amp;#39;t be mislead--the call to&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# `train` just changes the *mode*, it doesn&amp;#39;t *perform* the training.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# `dropout` and `batchnorm` layers behave differently during training vs test&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# For each batch of training data,&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# # of step : 241&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# len of batch : 32&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# size of batch[0] : 64&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Progress update every 40 batches.&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            
            &lt;span class=&quot;c1&quot;&gt;# Calcuate elapsed time in minutes.&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;format_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# Report progress.&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;  Batch &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;5,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; of &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;5,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;.    Elapsed: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elapsed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Unpack this training batch from our dataloader&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# As we unpack the batch, we&amp;#39;ll also copy each tensor to the GPU using `to` method.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# `batch` contains three pytorch tensors:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#     [0]: input ids&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#     [1] : attention masks&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#     [2] : labels&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b_input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b_input_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Always clear any previously calculated gradients before performing backward pass.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# PyTorch doesn&amp;#39;t do this automatically because accumulating the gradients is &lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# &amp;quot;convenient while training RNNs&amp;quot;. (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Perform a forwad pass (evaluate the model on this training batch).&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# The documentation for this `model` function is here:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# It returns different numbers of parameters depending on what arguments&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# are given and what flags are set. For our usage here, it returns the loss (because we provided labels)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# and the &amp;quot;logits&amp;quot;--the model outputs prior to activation.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;token_type_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_input_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Accumulate the training loss over all of the batches so that we can calculate the average loss at the end.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# `loss` is a Tensor containing a single value; the `.item()` function just returns the Python value from the tensor.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;total_train_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Perform a backward pass to calculate the gradients.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Clip the norm of the gradients to 1.0.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# This is to help prevent the &amp;quot;exploding gradients&amp;quot; problem.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clip_grad_norm_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Update parameters and take a step using the computed gradient.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# The optimizer dictates the &amp;quot;update rule&amp;quot;--how the parameters are modified &lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# based on their gradients, the learning rate, etc.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Update the learning rate.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Calculate the average loss over all of the batches.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;avg_train_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_train_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Measure how long this epoch took.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;training_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;format_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;  Average training loss: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{0:.2f}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_train_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;  Training epoch took: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# ========================================&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#                                 Validation &lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ========================================&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# After the completion of each training epoch, measure our performance on our validation set.&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Running Validation...&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Put the model in evaluation mode--the dropout layers behave differently during evaluation.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Tracking variables&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total_eval_accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total_eval_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nb_eval_steps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Evaluate data for one epoch&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Unpack this training batch from our dataloader.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# As we unpack the batch, we&amp;#39;ll also copy each tensor to the GPU using the `to` method.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# `batch` contains three pytorch tensors:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#   [0]: input ids&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#   [1]: attention masks&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#   [2]: labels&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b_input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b_input_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Tell pytorch not to bother with constructing the compute graph during the forward pass,&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# since this is only needed for backprop (training).&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# Forward pass, calculate logit predictions.&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# token_type_ids is the same as the &amp;quot;segment ids&amp;quot;, which differentiates sentence 1 and 2 in 2-sentence tasks.&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# The documentation for this `model` function is here:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Get the &amp;quot;logits&amp;quot; output byt the model. The &amp;quot;logits&amp;quot; are the output&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# values prior to applying an activation function like the softmax.&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;token_type_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;attention_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_input_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            
        &lt;span class=&quot;c1&quot;&gt;# Accumulate the validation loss.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;total_eval_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Move logits and labels to CPU&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;label_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;cpu&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Calculate the accuracy for this batch of test sentence,&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# and accumulate it over all batches.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;total_eval_accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flat_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Report the final accuracy for this validation run.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;avg_val_accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_eval_accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;  Accuracy: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{0:.2f}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_val_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Calculate the average loss over all of the batches.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;avg_val_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_eval_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Measure how long the validation run took.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;validation_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;format_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;  Validation Loss: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{0:.2f}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_val_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;  Validation took: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Record all statistics from this epoch.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;training_stats&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s1&quot;&gt;&amp;#39;epoch&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s1&quot;&gt;&amp;#39;Training Loss&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_train_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s1&quot;&gt;&amp;#39;Valid. Loss&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_val_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s1&quot;&gt;&amp;#39;Valid. Accur.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_val_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s1&quot;&gt;&amp;#39;Training Time&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s1&quot;&gt;&amp;#39;Validation Time&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_time&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Training complete!&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Total training took &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; (h:mm:ss)&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_t0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;
======== Epoch 1 / 4 ========
Training...
&amp;lt;class &amp;#39;torch.Tensor&amp;#39;&amp;gt;
tensor([[  101, 11312, 10320,  ...,     0,     0,     0],
        [  101, 13667, 10564,  ...,     0,     0,     0],
        [  101, 10103, 23222,  ...,     0,     0,     0],
        ...,
        [  101, 21506, 10935,  ...,     0,     0,     0],
        [  101, 69042, 10127,  ...,     0,     0,     0],
        [  101, 12941, 50134,  ...,     0,     0,     0]])
&amp;lt;class &amp;#39;torch.Tensor&amp;#39;&amp;gt;
tensor([[  101, 11312, 10320,  ...,     0,     0,     0],
        [  101, 13667, 10564,  ...,     0,     0,     0],
        [  101, 10103, 23222,  ...,     0,     0,     0],
        ...,
        [  101, 21506, 10935,  ...,     0,     0,     0],
        [  101, 69042, 10127,  ...,     0,     0,     0],
        [  101, 12941, 50134,  ...,     0,     0,     0]], device=&amp;#39;cuda:0&amp;#39;)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_text output_error&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;ansi-red-fg&quot;&gt;---------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;ansi-red-fg&quot;&gt;RuntimeError&lt;/span&gt;                              Traceback (most recent call last)
&lt;span class=&quot;ansi-green-fg&quot;&gt;&amp;lt;ipython-input-26-443d4943574d&amp;gt;&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;&amp;lt;module&amp;gt;&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;     90&lt;/span&gt;                                         token_type_ids&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;ansi-green-fg&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;     91&lt;/span&gt;                                         attention_mask&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;b_input_mask&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;---&amp;gt; 92&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;                                         labels=b_labels)
&lt;/span&gt;&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;     93&lt;/span&gt; 
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;     94&lt;/span&gt;         &lt;span class=&quot;ansi-red-fg&quot;&gt;# Accumulate the training loss over all of the batches so that we can calculate the average loss at the end.&lt;/span&gt;

&lt;span class=&quot;ansi-green-fg&quot;&gt;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;__call__&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;(self, *input, **kwargs)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    487&lt;/span&gt;             result &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;_slow_forward&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;*&lt;/span&gt;input&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ansi-blue-fg&quot;&gt;**&lt;/span&gt;kwargs&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    488&lt;/span&gt;         &lt;span class=&quot;ansi-green-fg&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;--&amp;gt; 489&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;             &lt;/span&gt;result &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;forward&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;*&lt;/span&gt;input&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ansi-blue-fg&quot;&gt;**&lt;/span&gt;kwargs&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    490&lt;/span&gt;         &lt;span class=&quot;ansi-green-fg&quot;&gt;for&lt;/span&gt; hook &lt;span class=&quot;ansi-green-fg&quot;&gt;in&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;_forward_hooks&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;values&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    491&lt;/span&gt;             hook_result &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; hook&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; input&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; result&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;ansi-green-fg&quot;&gt;/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;   1174&lt;/span&gt;             position_ids&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;position_ids&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;   1175&lt;/span&gt;             head_mask&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;head_mask&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;-&amp;gt; 1176&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;             &lt;/span&gt;inputs_embeds&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;inputs_embeds&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;   1177&lt;/span&gt;         )
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;   1178&lt;/span&gt; 

&lt;span class=&quot;ansi-green-fg&quot;&gt;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;__call__&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;(self, *input, **kwargs)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    487&lt;/span&gt;             result &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;_slow_forward&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;*&lt;/span&gt;input&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ansi-blue-fg&quot;&gt;**&lt;/span&gt;kwargs&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    488&lt;/span&gt;         &lt;span class=&quot;ansi-green-fg&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;--&amp;gt; 489&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;             &lt;/span&gt;result &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;forward&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;*&lt;/span&gt;input&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ansi-blue-fg&quot;&gt;**&lt;/span&gt;kwargs&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    490&lt;/span&gt;         &lt;span class=&quot;ansi-green-fg&quot;&gt;for&lt;/span&gt; hook &lt;span class=&quot;ansi-green-fg&quot;&gt;in&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;_forward_hooks&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;values&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    491&lt;/span&gt;             hook_result &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; hook&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; input&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; result&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;ansi-green-fg&quot;&gt;/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    781&lt;/span&gt; 
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    782&lt;/span&gt;         embedding_output = self.embeddings(
&lt;span class=&quot;ansi-green-fg&quot;&gt;--&amp;gt; 783&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;             &lt;/span&gt;input_ids&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;input_ids&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; position_ids&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;position_ids&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; token_type_ids&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;token_type_ids&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; inputs_embeds&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;inputs_embeds
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    784&lt;/span&gt;         )
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    785&lt;/span&gt;         encoder_outputs = self.encoder(

&lt;span class=&quot;ansi-green-fg&quot;&gt;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;__call__&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;(self, *input, **kwargs)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    487&lt;/span&gt;             result &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;_slow_forward&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;*&lt;/span&gt;input&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ansi-blue-fg&quot;&gt;**&lt;/span&gt;kwargs&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    488&lt;/span&gt;         &lt;span class=&quot;ansi-green-fg&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;--&amp;gt; 489&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;             &lt;/span&gt;result &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;forward&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;*&lt;/span&gt;input&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ansi-blue-fg&quot;&gt;**&lt;/span&gt;kwargs&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    490&lt;/span&gt;         &lt;span class=&quot;ansi-green-fg&quot;&gt;for&lt;/span&gt; hook &lt;span class=&quot;ansi-green-fg&quot;&gt;in&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;_forward_hooks&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;values&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    491&lt;/span&gt;             hook_result &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; hook&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; input&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; result&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;ansi-green-fg&quot;&gt;/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;(self, input_ids, token_type_ids, position_ids, inputs_embeds)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    177&lt;/span&gt;         embeddings &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; inputs_embeds &lt;span class=&quot;ansi-blue-fg&quot;&gt;+&lt;/span&gt; position_embeddings &lt;span class=&quot;ansi-blue-fg&quot;&gt;+&lt;/span&gt; token_type_embeddings
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    178&lt;/span&gt;         embeddings &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;LayerNorm&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;embeddings&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;--&amp;gt; 179&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;         &lt;/span&gt;embeddings &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;dropout&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;embeddings&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    180&lt;/span&gt;         &lt;span class=&quot;ansi-green-fg&quot;&gt;return&lt;/span&gt; embeddings
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    181&lt;/span&gt; 

&lt;span class=&quot;ansi-green-fg&quot;&gt;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;__call__&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;(self, *input, **kwargs)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    487&lt;/span&gt;             result &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;_slow_forward&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;*&lt;/span&gt;input&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ansi-blue-fg&quot;&gt;**&lt;/span&gt;kwargs&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    488&lt;/span&gt;         &lt;span class=&quot;ansi-green-fg&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;--&amp;gt; 489&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;             &lt;/span&gt;result &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;forward&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;*&lt;/span&gt;input&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ansi-blue-fg&quot;&gt;**&lt;/span&gt;kwargs&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    490&lt;/span&gt;         &lt;span class=&quot;ansi-green-fg&quot;&gt;for&lt;/span&gt; hook &lt;span class=&quot;ansi-green-fg&quot;&gt;in&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;_forward_hooks&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;values&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    491&lt;/span&gt;             hook_result &lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt; hook&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; input&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; result&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;ansi-green-fg&quot;&gt;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/dropout.py&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;(self, input)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;     56&lt;/span&gt;     &lt;span class=&quot;ansi-blue-fg&quot;&gt;@&lt;/span&gt;weak_script_method
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;     57&lt;/span&gt;     &lt;span class=&quot;ansi-green-fg&quot;&gt;def&lt;/span&gt; forward&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; input&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;---&amp;gt; 58&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;         &lt;/span&gt;&lt;span class=&quot;ansi-green-fg&quot;&gt;return&lt;/span&gt; F&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;dropout&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;input&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;p&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;training&lt;span class=&quot;ansi-blue-fg&quot;&gt;,&lt;/span&gt; self&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;inplace&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;     59&lt;/span&gt; 
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;     60&lt;/span&gt; 

&lt;span class=&quot;ansi-green-fg&quot;&gt;/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;(input, p, training, inplace)&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    747&lt;/span&gt;     return (_VF.dropout_(input, p, training)
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    748&lt;/span&gt;             &lt;span class=&quot;ansi-green-fg&quot;&gt;if&lt;/span&gt; inplace
&lt;span class=&quot;ansi-green-fg&quot;&gt;--&amp;gt; 749&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt;             else _VF.dropout(input, p, training))
&lt;/span&gt;&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    750&lt;/span&gt; 
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;    751&lt;/span&gt; 

&lt;span class=&quot;ansi-red-fg&quot;&gt;RuntimeError&lt;/span&gt;: Creating MTGP constants failed. at /pytorch/aten/src/THC/THCTensorRandom.cu:35&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">BERT Fine-Tuning Sentence Classification</title><link href="https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification.html" rel="alternate" type="text/html" title="BERT Fine-Tuning Sentence Classification" /><published>2020-04-19T00:00:00-05:00</published><updated>2020-04-19T00:00:00-05:00</updated><id>https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification</id><content type="html" xml:base="https://sparklingness.github.io/blog/bert/jupyter/2020/04/19/BERT_Fine-Tuning_Sentence_Classification.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-19-BERT_Fine-Tuning_Sentence_Classification.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;1.-Setup&quot;&gt;1. Setup&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-Setup&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;1.1.-Using-Colab-GPU-for-Training&quot;&gt;1.1. Using Colab GPU for Training&lt;a class=&quot;anchor-link&quot; href=&quot;#1.1.-Using-Colab-GPU-for-Training&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Get the GPU device name.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;device_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gpu_device_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The device name should look like the following:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;/device:GPU:0&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Found GPU at: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;ne&quot;&gt;SystemError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;GPU device not found&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Found GPU at: /device:GPU:0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# If there&amp;#39;s a GPU available,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Tell PyTorch to user the GPU.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;cuda&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;There are &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; GPU(s) available.&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;We will user the GPU:&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_device_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# If not,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;No GPU available, using the CPU instead.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;cpu&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;There are 1 GPU(s) available.
We will user the GPU: Tesla K80
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.2.-Installing-the-Hugging-Face-Library&quot;&gt;1.2. Installing the Hugging Face Library&lt;a class=&quot;anchor-link&quot; href=&quot;#1.2.-Installing-the-Hugging-Face-Library&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; pip install transformers -q
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; pip install wget -q
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;wget&lt;/span&gt; 
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Downloading dataset...&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The URL for the dataset zip file.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;https://nyu-mll.github.io/CoLA/cola_public_1.1.zip&amp;#39;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Download the file (if we haven&amp;#39;t already)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;./cola_public_1.1.zip&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;wget&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;./cola_public_1.1.zip&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Downloading dataset...
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Unzip the dataset (if we haven&amp;#39;t already)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;./cola_public/&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; unzip cola_public_1.1.zip
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; head cola_public/*/*
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;==&amp;gt; cola_public/raw/in_domain_dev.tsv &amp;lt;==
gj04	1		The sailors rode the breeze clear of the rocks.
gj04	1		The weights made the rope stretch over the pulley.
gj04	1		The mechanical doll wriggled itself loose.
cj99	1		If you had eaten more, you would want less.
cj99	0	*	As you eat the most, you want the least.
cj99	0	*	The more you would want, the less you would eat.
cj99	0	*	I demand that the more John eat, the more he pays.
cj99	1		Mary listens to the Grateful Dead, she gets depressed.
cj99	1		The angrier Mary got, the more she looked at pictures.
cj99	1		The higher the stakes, the lower his expectations are.

==&amp;gt; cola_public/raw/in_domain_train.tsv &amp;lt;==
gj04	1		Our friends won&amp;#39;t buy this analysis, let alone the next one we propose.
gj04	1		One more pseudo generalization and I&amp;#39;m giving up.
gj04	1		One more pseudo generalization or I&amp;#39;m giving up.
gj04	1		The more we study verbs, the crazier they get.
gj04	1		Day by day the facts are getting murkier.
gj04	1		I&amp;#39;ll fix you a drink.
gj04	1		Fred watered the plants flat.
gj04	1		Bill coughed his way out of the restaurant.
gj04	1		We&amp;#39;re dancing the night away.
gj04	1		Herman hammered the metal flat.

==&amp;gt; cola_public/raw/out_of_domain_dev.tsv &amp;lt;==
clc95	1		Somebody just left - guess who.
clc95	1		They claimed they had settled on something, but it wasn&amp;#39;t clear what they had settled on.
clc95	1		If Sam was going, Sally would know where.
clc95	1		They&amp;#39;re going to serve the guests something, but it&amp;#39;s unclear what.
clc95	1		She&amp;#39;s reading. I can&amp;#39;t imagine what.
clc95	1		John said Joan saw someone from her graduating class.
clc95	0	*	John ate dinner but I don&amp;#39;t know who.
clc95	0	*	She mailed John a letter, but I don&amp;#39;t know to whom.
clc95	1		I served leek soup to my guests.
clc95	1		I served my guests.

==&amp;gt; cola_public/tokenized/in_domain_dev.tsv &amp;lt;==
gj04	1		the sailors rode the breeze clear of the rocks .
gj04	1		the weights made the rope stretch over the pulley .
gj04	1		the mechanical doll wriggled itself loose .
cj99	1		if you had eaten more , you would want less .
cj99	0	*	as you eat the most , you want the least .
cj99	0	*	the more you would want , the less you would eat .
cj99	0	*	i demand that the more john eat , the more he pays .
cj99	1		mary listens to the grateful dead , she gets depressed .
cj99	1		the angrier mary got , the more she looked at pictures .
cj99	1		the higher the stakes , the lower his expectations are .

==&amp;gt; cola_public/tokenized/in_domain_train.tsv &amp;lt;==
gj04	1		our friends wo n&amp;#39;t buy this analysis , let alone the next one we propose .
gj04	1		one more pseudo generalization and i &amp;#39;m giving up .
gj04	1		one more pseudo generalization or i &amp;#39;m giving up .
gj04	1		the more we study verbs , the crazier they get .
gj04	1		day by day the facts are getting murkier .
gj04	1		i &amp;#39;ll fix you a drink .
gj04	1		fred watered the plants flat .
gj04	1		bill coughed his way out of the restaurant .
gj04	1		we &amp;#39;re dancing the night away .
gj04	1		herman hammered the metal flat .

==&amp;gt; cola_public/tokenized/out_of_domain_dev.tsv &amp;lt;==
clc95	1		somebody just left - guess who .
clc95	1		they claimed they had settled on something , but it was n&amp;#39;t clear what they had settled on .
clc95	1		if sam was going , sally would know where .
clc95	1		they &amp;#39;re going to serve the guests something , but it &amp;#39;s unclear what .
clc95	1		she &amp;#39;s reading . i ca n&amp;#39;t imagine what .
clc95	1		john said joan saw someone from her graduating class .
clc95	0	*	john ate dinner but i do n&amp;#39;t know who .
clc95	0	*	she mailed john a letter , but i do n&amp;#39;t know to whom .
clc95	1		i served leek soup to my guests .
clc95	1		i served my guests .
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.2.-Parse&quot;&gt;2.2. Parse&lt;a class=&quot;anchor-link&quot; href=&quot;#2.2.-Parse&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Load the dataset into a pandas dataframe.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;./cola_public/raw/in_domain_train.tsv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delimiter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;sentence_source&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;label&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;label_notes&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;sentence&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Report the number of sentences.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Number of training sentences: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Display 10 random rows from the data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Number of training sentences: 8,551

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sentence_source&lt;/th&gt;
      &lt;th&gt;label&lt;/th&gt;
      &lt;th&gt;label_notes&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1337&lt;/th&gt;
      &lt;td&gt;r-67&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;I want to peruse that contract before filing a...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1670&lt;/th&gt;
      &lt;td&gt;r-67&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;This hat Tom said Al thought you wanted me to ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6952&lt;/th&gt;
      &lt;td&gt;m_02&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Jane visits Emma.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;449&lt;/th&gt;
      &lt;td&gt;bc01&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;??&lt;/td&gt;
      &lt;td&gt;Who is he reading a book that criticizes?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4988&lt;/th&gt;
      &lt;td&gt;ks08&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;The fact that scientists have now established ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1830&lt;/th&gt;
      &lt;td&gt;r-67&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;*&lt;/td&gt;
      &lt;td&gt;That informers they never use is claimed by th...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6508&lt;/th&gt;
      &lt;td&gt;d_98&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;*&lt;/td&gt;
      &lt;td&gt;Every cat doesn't like mice, but Felix doesn't.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8329&lt;/th&gt;
      &lt;td&gt;ad03&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;I asked did Medea poison Jason.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1358&lt;/th&gt;
      &lt;td&gt;r-67&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;*&lt;/td&gt;
      &lt;td&gt;They will give me a hat that I won't like whic...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5503&lt;/th&gt;
      &lt;td&gt;b_73&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Her mother wants Mary to be such an eminent wo...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;sentence&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;label&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
      &lt;th&gt;label&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;6544&lt;/th&gt;
      &lt;td&gt;The table, I put Kim on which supported the book.&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5993&lt;/th&gt;
      &lt;td&gt;Has Calvin a bowl?&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;379&lt;/th&gt;
      &lt;td&gt;How do you wonder who could solve this problem.&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;588&lt;/th&gt;
      &lt;td&gt;the branch dropped bare of its apple.&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7194&lt;/th&gt;
      &lt;td&gt;Your desk before, this girl in the red coat wi...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Get the lists of sentences and their labels.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;3.-Tokenization-&amp;amp;-Input-Formatting&quot;&gt;3. Tokenization &amp;amp; Input Formatting&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-Tokenization-&amp;amp;-Input-Formatting&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;3.1.-BERT-Tokenizer&quot;&gt;3.1. BERT Tokenizer&lt;a class=&quot;anchor-link&quot; href=&quot;#3.1.-BERT-Tokenizer&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Load the BERT tokenizer.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Loading BERT tokenizer...&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;bert-base-multilingual-uncased&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;do_lower_case&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Loading BERT tokenizer...
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Print the original sentence.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39; Original: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print the sentence split into tokens.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Tokenized: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print the sentence mapped to token ids.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Token IDs: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_tokens_to_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt; Original:  Our friends won&amp;#39;t buy this analysis, let alone the next one we propose.
Tokenized:  [&amp;#39;our&amp;#39;, &amp;#39;friends&amp;#39;, &amp;#39;won&amp;#39;, &amp;#34;&amp;#39;&amp;#34;, &amp;#39;t&amp;#39;, &amp;#39;buy&amp;#39;, &amp;#39;this&amp;#39;, &amp;#39;analysis&amp;#39;, &amp;#39;,&amp;#39;, &amp;#39;let&amp;#39;, &amp;#39;alone&amp;#39;, &amp;#39;the&amp;#39;, &amp;#39;next&amp;#39;, &amp;#39;one&amp;#39;, &amp;#39;we&amp;#39;, &amp;#39;propose&amp;#39;, &amp;#39;.&amp;#39;]
Token IDs:  [14008, 16119, 11441, 112, 162, 35172, 10372, 15559, 117, 12421, 19145, 10103, 12878, 10399, 11312, 25690, 119]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;korean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;안녕하세요. 반갑습니다. 너는 이름이 뭐니? 오늘 날씨가 맑고 좋구나.&amp;quot;&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Print the original sentence.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39; Original: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;korean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print the sentence split into tokens.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Tokenized: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;korean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print the sentence mapped to token ids.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Token IDs: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_tokens_to_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;korean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt; Original:  안녕하세요. 반갑습니다. 너는 이름이 뭐니? 오늘 날씨가 맑고 좋구나.
Tokenized:  [&amp;#39;ᄋ&amp;#39;, &amp;#39;##ᅡᆫ&amp;#39;, &amp;#39;##녀&amp;#39;, &amp;#39;##ᆼ&amp;#39;, &amp;#39;##하&amp;#39;, &amp;#39;##세&amp;#39;, &amp;#39;##요&amp;#39;, &amp;#39;.&amp;#39;, &amp;#39;ᄇ&amp;#39;, &amp;#39;##ᅡᆫ&amp;#39;, &amp;#39;##가&amp;#39;, &amp;#39;##ᆸ&amp;#39;, &amp;#39;##스&amp;#39;, &amp;#39;##ᆸ니다&amp;#39;, &amp;#39;.&amp;#39;, &amp;#39;ᄂ&amp;#39;, &amp;#39;##ᅥ&amp;#39;, &amp;#39;##는&amp;#39;, &amp;#39;이&amp;#39;, &amp;#39;##름이&amp;#39;, &amp;#39;ᄆ&amp;#39;, &amp;#39;##ᅯ&amp;#39;, &amp;#39;##니&amp;#39;, &amp;#39;?&amp;#39;, &amp;#39;ᄋ&amp;#39;, &amp;#39;##ᅩ&amp;#39;, &amp;#39;##ᄂ&amp;#39;, &amp;#39;##ᅳᆯ&amp;#39;, &amp;#39;날&amp;#39;, &amp;#39;##씨&amp;#39;, &amp;#39;##가&amp;#39;, &amp;#39;ᄆ&amp;#39;, &amp;#39;##ᅡ&amp;#39;, &amp;#39;##ᆰ&amp;#39;, &amp;#39;##고&amp;#39;, &amp;#39;ᄌ&amp;#39;, &amp;#39;##ᅩ&amp;#39;, &amp;#39;##ᇂ&amp;#39;, &amp;#39;##구&amp;#39;, &amp;#39;##나&amp;#39;, &amp;#39;.&amp;#39;]
Token IDs:  [1174, 26646, 49345, 13045, 35132, 25169, 47024, 119, 1170, 26646, 11376, 17360, 13212, 79427, 119, 1165, 33645, 11192, 12398, 89420, 1169, 97090, 25536, 136, 1174, 29347, 97071, 63277, 76818, 47928, 11376, 1169, 25539, 97098, 12300, 1175, 29347, 97109, 16336, 16801, 119]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When we actually convert all of our sentences, we'll use the &lt;code&gt;tokenize.encode&lt;/code&gt; functio to handle both steps, rather than calling &lt;code&gt;tokenize&lt;/code&gt; and &lt;code&gt;convert_tokens_to_ids&lt;/code&gt; seperately.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.2.-Required-Formatting&quot;&gt;3.2. Required Formatting&lt;a class=&quot;anchor-link&quot; href=&quot;#3.2.-Required-Formatting&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.3.-Tokenize-Dataset&quot;&gt;3.3. Tokenize Dataset&lt;a class=&quot;anchor-link&quot; href=&quot;#3.3.-Tokenize-Dataset&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For every sentence,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sent&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Tokenize the text and add `[CLS]` and `[SEP]` tokens.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;add_special_tokens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Update the maximum sentence langth.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Max sentence length: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Max sentence length:  48
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Tokenize all of the sentences and map the tokens to their word IDs.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For every sentence,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sent&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# `encode_plus` will:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (1) Tokenize the sentence.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (2) Prepend the `[CLS]` token to the start.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (3) Append the `[SEP]` token to the end.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (4) Map tokens to their IDs.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (5) Pad or truncate the sentence to `max_length`&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# (6) Create attention masks for [PAD] tokens.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;encoded_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode_plus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                                       &lt;span class=&quot;c1&quot;&gt;# Sentence to encode.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;add_special_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# Add &amp;#39;[CLS]&amp;#39; and &amp;#39;[SEP]&amp;#39;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                           &lt;span class=&quot;c1&quot;&gt;# Pad &amp;amp; truncate all sentences.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pad_to_max_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;return_attention_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Construct attn. masks.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;return_Tensors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;pt&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                  &lt;span class=&quot;c1&quot;&gt;# Return pytorch tensors.&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Add the encoded sentence to the list.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;input_ids&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# And its attention mask (simply differentiates padding from non-padding.)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;attention_mask&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Convert the lists into tensors.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FloatTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FloatTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print sentence 0, now as a list of IDs.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Original:  &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Token IDs: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Original:   Our friends won&amp;#39;t buy this analysis, let alone the next one we propose.
Token IDs:  tensor([  101., 14008., 16119., 11441.,   112.,   162., 35172., 10372., 15559.,
          117., 12421., 19145., 10103., 12878., 10399., 11312., 25690.,   119.,
          102.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,
            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,
            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,
            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,
            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,
            0.])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.4.-Training-&amp;amp;-Validation-Split&quot;&gt;3.4. Training &amp;amp; Validation Split&lt;a class=&quot;anchor-link&quot; href=&quot;#3.4.-Training-&amp;amp;-Validation-Split&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_split&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Combine the training inputs into a TensorDataset.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attention_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create a 90-10 train-validation split.&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Calculate the number of samples to include in each set.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;val_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Devide the dataset by randomly selecting samples.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;5}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; training samples&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;5}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; validation samples&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt; 7695 training samples
  856 validation samples
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SequentialSampler&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The DataLoader needs to know our batch size for training, so we specify it here.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create the DataLoaders for our training and validation sets.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# We&amp;#39;ll take training samples in random order.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# The training samples.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Select batches randomly&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Trains with this batch size.&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For validation the order doesn&amp;#39;t matter, so we&amp;#39;ll just read them sequentially.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validation_dataloader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# The validation samples.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SequentialSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Pull out batches sequentially.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Evaluate with this batch size.&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Inspect Bert Vocabulary</title><link href="https://sparklingness.github.io/blog/bert/jupyter/2020/04/18/Inspect-Bert-Vocabulary.html" rel="alternate" type="text/html" title="Inspect Bert Vocabulary" /><published>2020-04-18T00:00:00-05:00</published><updated>2020-04-18T00:00:00-05:00</updated><id>https://sparklingness.github.io/blog/bert/jupyter/2020/04/18/Inspect-Bert-Vocabulary</id><content type="html" xml:base="https://sparklingness.github.io/blog/bert/jupyter/2020/04/18/Inspect-Bert-Vocabulary.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-18-Inspect-Bert-Vocabulary.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Load-the-model&quot;&gt;Load the model&lt;a class=&quot;anchor-link&quot; href=&quot;#Load-the-model&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Install the huggingface implementation.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install pytorch-pretrained-bert -q
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pytorch_pretrained_bert&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Load pre-trained model tokenizer (vocabulary)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;bert-base-multilingual-uncased&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, &lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# bert-base-multilingual-uncased, bert-base-multilingual-cased, &lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# bert-base-chinese&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Inspect-BERT-Vocabulary&quot;&gt;Inspect BERT Vocabulary&lt;a class=&quot;anchor-link&quot; href=&quot;#Inspect-BERT-Vocabulary&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Vocab-Dump&quot;&gt;Vocab Dump&lt;a class=&quot;anchor-link&quot; href=&quot;#Vocab-Dump&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Retrieve the entire list of &quot;tokens&quot; and write these out to text files so we can peruse them.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;vocabulary.txt&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;utf-8&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Single-Characters&quot;&gt;Single Characters&lt;a class=&quot;anchor-link&quot; href=&quot;#Single-Characters&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_chars&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;one_chars_hashes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For each token in the vocabulary,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Record any single-character tokens.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;one_chars&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;c1&quot;&gt;# Record single-character tokens preceded by the two hashes.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;##&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;one_chars_hashes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Number of single character tokens:&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_chars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print all of the single characters, 40 per row.&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For every batch of 40 tokens,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_chars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Limit the end index so we don&amp;#39;t go past the end of the list.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_chars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Print out the tokens, seperated by a space.&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_chars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Number of single character tokens: 9995 

! &amp;#34; # $ % &amp;amp; &amp;#39; ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; &amp;lt; = &amp;gt; ? @ [ \ ] ^ _ a b c
d e f g h i j k l m n o p q r s t u v w x y z { | } ~ ¡ ¢ £ ¤ ¥ ¦ § ¨ © ª « ¬ ®
¯ ° ± ² ³ µ ¶ · ¹ º » ¼ ½ ¾ ¿ × ß æ ð ÷ ø þ đ ħ ı ĳ ł ŉ ŋ œ ſ ƒ ǁ ɐ ɑ ɒ ɓ ɔ ɕ ə
ɛ ɟ ɡ ɣ ɤ ɦ ɨ ɪ ɫ ɬ ɯ ɲ ɴ ɸ ɹ ɾ ʀ ʁ ʂ ʃ ʉ ʊ ʋ ʌ ʎ ʐ ʑ ʒ ʔ ʕ ʙ ʝ ʤ ʦ ʧ ʰ ʲ ʳ ʷ ʸ
ʻ ʼ ʾ ʿ ˁ ˈ ˋ ˌ ː ˙ ˚ ˡ ˢ ˣ ΄ α β γ δ ε ζ η θ ι κ λ μ ν ξ ο π ρ ς σ τ υ φ χ ψ ω
ϕ а б в г д е ж з и к л м н о п р с т у ф х ц ч ш щ ъ ы ь э ю я ђ є ѕ і ј љ њ ћ
џ ѣ ѧ ѫ ѳ ґ ғ җ ҙ қ ҡ ң ҫ ү ұ ҳ ҷ ҹ һ ӊ ӏ ә ө ՚ ՛ ՜ ՝ ՞ ա բ գ դ ե զ է ը թ ժ ի լ
խ ծ կ հ ձ ղ ճ մ յ ն շ ո չ պ ջ ռ ս վ տ ր ց ւ փ ք օ ֆ և ։ ֊ ־ ׃ א ב ג ד ה ו ז ח ט
י ך כ ל ם מ ן נ ס ע ף פ ץ צ ק ר ש ת ׳ ״ ، ؍ ؛ ؟ ء ا ب ة ت ث ج ح خ د ذ ر ز س ش ص
ض ط ظ ع غ ـ ف ق ك ل م ن ه و ى ي ٠ ١ ٢ ٣ ٤ ٥ ٦ ٧ ٨ ٩ ٪ ٫ ٬ ٭ ٴ ٹ پ چ ڈ ڑ ژ ڠ ڤ ک
ڭ گ ں ھ ہ ۃ ۆ ۇ ۋ ی ێ ے ۔ ە ۰ ۱ ۲ ۳ ۴ ۵ ۶ ۷ ۸ ۹ ݨ ः अ आ इ ई उ ऊ ऋ ऍ ऎ ए ऐ ऑ ऒ ओ
औ क ख ग घ ङ च छ ज झ ञ ट ठ ड ढ ण त थ द ध न प फ ब भ म य र ल ळ व श ष स ह ऽ ा ि ी ॉ
ॊ ो ौ ॐ ॠ । ॥ ० १ २ ३ ४ ५ ६ ७ ८ ९ ॰ ॲ ং ঃ অ আ ই ঈ উ ঊ ঋ এ ঐ ও ঔ ক খ গ ঘ ঙ চ ছ জ
ঝ ঞ ট ঠ ড ঢ ণ ত থ দ ধ ন প ফ ব ভ ম য র ল শ ষ স হ া ি ী ে ৈ ৎ ৗ ০ ১ ২ ৩ ৪ ৫ ৬ ৭ ৮
৯ ৰ ৱ ৷ ਅ ਆ ਇ ਈ ਉ ਊ ਏ ਐ ਓ ਔ ਕ ਖ ਗ ਘ ਚ ਛ ਜ ਝ ਟ ਠ ਡ ਢ ਣ ਤ ਥ ਦ ਧ ਨ ਪ ਫ ਬ ਭ ਮ ਯ ਰ ਲ
ਵ ਸ ਹ ਾ ਿ ੀ ੜ ੦ ੧ ੨ ੩ ੪ ੫ ੬ ੭ ੮ ੯ ੲ ੳ ઃ અ આ ઇ ઈ ઉ ઊ ઋ ઍ એ ઐ ઑ ઓ ઔ ક ખ ગ ઘ ચ છ જ
ઝ ઞ ટ ઠ ડ ઢ ણ ત થ દ ધ ન પ ફ બ ભ મ ય ર લ ળ વ શ ષ સ હ ા િ ી ૉ ો ૌ ૦ ૧ ૨ ૩ ૪ ૫ ૬ ૭
૮ ૯ ஃ அ ஆ இ ஈ உ ஊ எ ஏ ஐ ஒ ஓ க ங ச ஜ ஞ ட ண த ந ன ப ம ய ர ற ல ள ழ வ ஷ ஸ ஹ ா ி ு ூ
ெ ே ை ௗ ఁ ం ః అ ఆ ఇ ఈ ఉ ఊ ఋ ఎ ఏ ఐ ఒ ఓ ఔ క ఖ గ ఘ ఙ చ ఛ జ ఝ ఞ ట ఠ డ ఢ ణ త థ ద ధ న
ప ఫ బ భ మ య ర ఱ ల ళ వ శ ష స హ ు ూ ృ ಂ ಃ ಅ ಆ ಇ ಈ ಉ ಊ ಋ ಎ ಏ ಐ ಒ ಓ ಔ ಕ ಖ ಗ ಘ ಚ ಛ ಜ
ಝ ಞ ಟ ಠ ಡ ಢ ಣ ತ ಥ ದ ಧ ನ ಪ ಫ ಬ ಭ ಮ ಯ ರ ಲ ಳ ವ ಶ ಷ ಸ ಹ ಾ ು ೂ ೃ ೕ ೖ ೦ ೧ ೨ ೩ ೪ ೫ ೬ ೭
೮ ೯ ം ഃ അ ആ ഇ ഈ ഉ ഊ ഋ എ ഏ ഐ ഒ ഓ ഔ ക ഖ ഗ ഘ ങ ച ഛ ജ ഝ ഞ ട ഠ ഡ ഢ ണ ത ഥ ദ ധ ന പ ഫ ബ
ഭ മ യ ര റ ല ള ഴ വ ശ ഷ സ ഹ ാ ി ീ െ േ ൈ ൗ ൧ ൨ ൺ ൻ ർ ൽ ൾ ൿ ක ද ප ම ය ර ල ශ ා ෙ ก ข
ค ง จ ด ต ท น บ ป ภ ม ย ร ล ว ส ห อ า ำ เ แ ไ ་ ། ཀ ཁ ག ང ཆ ད ན པ ཕ བ མ ཚ ཟ འ ར
ལ ཤ ས က ခ ဂ ဃ င စ ဆ ဇ ဈ ဉ ည ဋ ဌ ဍ ဏ တ ထ ဒ ဓ န ပ ဖ ဗ ဘ မ ယ ရ လ ဝ သ ဟ ဠ အ ဤ ဥ ဧ ဩ
ါ ာ ေ း ျ ြ ဿ ၀ ၁ ၂ ၃ ၄ ၅ ၆ ၇ ၈ ၉ ၊ ။ ၌ ၍ ၎ ၏ ა ბ გ დ ე ვ ზ თ ი კ ლ მ ნ ო პ ჟ რ
ს ტ უ ფ ქ ღ ყ შ ჩ ც ძ წ ჭ ხ ჯ ჰ ჲ ᄀ ᄁ ᄂ ᄃ ᄄ ᄅ ᄆ ᄇ ᄈ ᄉ ᄊ ᄋ ᄌ ᄍ ᄎ ᄏ ᄐ ᄑ ᄒ ᅡ ᅢ ᅣ ᅤ
ᅥ ᅦ ᅧ ᅨ ᅩ ᅪ ᅫ ᅬ ᅭ ᅮ ᅯ ᅰ ᅱ ᅲ ᅳ ᅴ ᅵ ᆨ ᆩ ᆪ ᆫ ᆬ ᆭ ᆮ ᆯ ᆰ ᆱ ᆲ ᆵ ᆶ ᆷ ᆸ ᆹ ᆺ ᆻ ᆼ ᆽ ᆾ ᆿ ᇀ
ᇁ ᇂ ᛫ ᛬ ᠠ ᠨ ᠬ ᠭ ᠰ ᡳ ᴬ ᴮ ᴰ ᴵ ᴶ ᴷ ᴺ ᴼ ᴾ ᴿ ᵀ ᵂ ᵃ ᵇ ᵈ ᵉ ᵍ ᵏ ᵐ ᵒ ᵖ ᵗ ᵘ ᵛ ᵢ ᵣ ᵤ ᵥ ᶜ ᶠ
ᶻ ᾽ ‖ ‚ ‛ „ ‟ † ‡ • ․ ‥ ‧ ‰ ′ ″ ‹ › ※ ‼ ‿ ⁄ ⁰ ⁱ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ⁺ ⁻ ⁾ ⁿ ₀ ₁ ₂ ₃ ₄ ₅
₆ ₇ ₈ ₉ ₊ ₍ ₎ ₐ ₑ ₒ ₓ ₕ ₖ ₗ ₘ ₙ ₚ ₛ ₜ ₤ ₩ ₪ € ₱ ₴ ₹ ℂ ℃ ℓ ℕ № ℚ ℝ ™ ℤ ⅓ ⅔ ⅛ ⅰ ⅱ
ⅲ ⅳ ⅴ ⅵ ⅶ ← ↑ → ↓ ↔ ↗ ↦ ↺ ⇄ ⇌ ⇒ ⇔ ⇧ ∀ ∂ ∃ ∅ ∆ ∇ ∈ ∑ − ∕ ∗ ∘ ∙ √ ∞ ∥ ∧ ∨ ∩ ∪ ∫ ∴
∼ ≅ ≈ ≒ ≡ ≤ ≥ ≪ ≫ ⊂ ⊃ ⊆ ⊕ ⊗ ⊙ ⊞ ⊥ ⋅ ⋯ ① ② ③ ④ ⑤ ⑥ ⑦ ⑧ ⓐ ─ ━ │ ┃ ┏ ┓ └ ┗ ┛ ├ ┣ ┫
┬ ┳ ║ ╱ █ ■ □ ▪ ▬ ▭ ▲ △ ▶ ► ▼ ▽ ◆ ◇ ◊ ○ ◌ ◎ ● ◡ ◦ ◯ ★ ☆ ☉ ♀ ♂ ♕ ♖ ♗ ♘ ♙ ♠ ♡ ♣ ♥
♦ ♪ ♭ ♯ ⚭ ✝ ⟨ ⟩ ⟪ ⟫ ⟶ ⱼ ⵏ ⺩ ⽥ 、 。 〃 々 〆 〇 〈 〉 《 》 「 」 『 』 【 】 〒 〔 〕 〖 〗 〜 〝 〞 〟
ぁ あ ぃ い ぅ う ぇ え ぉ お か き く け こ さ し す せ そ た ち っ つ て と な に ぬ ね の は ひ ふ へ ほ ま み む め
も ゃ や ゅ ゆ ょ よ ら り る れ ろ わ ゐ ゑ を ん ゝ ァ ア ィ イ ゥ ウ ェ エ ォ オ カ キ ク ケ コ サ シ ス セ ソ タ チ
ッ ツ テ ト ナ ニ ヌ ネ ノ ハ ヒ フ ヘ ホ マ ミ ム メ モ ャ ヤ ュ ユ ョ ヨ ラ リ ル レ ロ ワ ヰ ヱ ヲ ン ヵ ヶ ・ ー ㄚ
ㄧ ㄨ ㄱ ㄴ ㄷ ㄹ ㅁ ㅂ ㅅ ㅇ ㅈ ㅡ ㅣ ㆍ ㈱ ㎒ ㎞ ㎡ 㭴 㹴 䜣 䲁 䲗 䲢 一 丁 七 万 丈 三 上 下 不 与 丐 丑 专 且 丕 世
丘 丙 业 丛 东 丝 丞 丟 両 丢 两 严 並 丧 个 丫 中 丰 串 临 丶 丸 丹 为 主 丼 丽 举 乂 乃 久 么 义 之 乌 乍 乎 乏 乐 乒
乓 乔 乖 乗 乘 乙 乜 九 乞 也 习 乡 书 乩 乭 买 乱 乳 乾 亀 亂 了 予 争 事 二 于 亏 云 互 五 井 亘 亙 亚 些 亜 亞 亟 亡
亢 交 亥 亦 产 亨 亩 享 京 亭 亮 亰 亲 亳 亵 亶 人 亻 亿 什 仁 仃 仄 仅 仆 仇 今 介 仍 从 仏 仑 仓 仔 仕 他 仗 付 仙 仝
仞 仟 仡 代 令 以 仪 们 仮 仰 仲 仵 件 价 任 份 仿 企 伉 伊 伋 伍 伎 伏 伐 休 伕 众 优 伙 会 伝 伞 伟 传 伤 伦 伪 伫 伯
估 伴 伶 伷 伸 伺 似 伽 佃 但 佇 佈 佉 位 低 住 佐 佑 体 佔 何 佗 佘 余 佚 佛 作 佞 佟 你 佢 佣 佤 佥 佩 佬 佯 佰 佳 併
佶 佺 佻 佼 佾 使 侂 侃 侄 來 侈 例 侍 侏 侑 侖 侗 侘 供 依 侠 価 侣 侥 侦 侧 侨 侪 侬 侭 侮 侯 侵 侶 侷 便 俁 係 促 俄
俅 俊 俎 俏 俐 俑 俗 俘 俚 保 俞 俟 俠 信 俣 俥 俦 俨 俩 俪 俬 俭 修 俯 俱 俳 俵 俶 俸 俺 俾 倂 倆 倉 個 倍 們 倒 倓 倔
倖 倘 候 倚 倜 借 倡 倣 値 倦 倧 倩 倪 倫 倬 倭 倮 倶 倹 债 倻 值 倾 偁 偃 假 偈 偉 偏 偕 做 停 健 偰 偲 側 偵 偶 偷 偽
偿 傀 傅 傈 傉 傍 傑 傕 傘 備 傚 傢 傣 储 催 傭 傲 傳 債 傷 傻 傾 僅 僉 僊 働 像 僑 僕 僖 僚 僞 僥 僧 僭 僮 僱 僳 僵 價
僻 儀 儁 儂 億 儆 儉 儋 儒 儔 儕 儘 儚 償 儡 優 儲 儷 儸 儺 儼 儿 兀 允 元 兄 充 兆 兇 先 光 克 兌 免 兎 児 兑 兒 兔 兖
兗 党 兜 兢 入 內 全 兩 兪 八 公 六 兮 兰 共 关 兴 兵 其 具 典 兹 养 兼 兽 冀 内 円 冇 冈 冉 冊 册 再 冏 冑 冒 冕 冗 写
军 农 冠 冢 冤 冥 冨 冪 冬 冯 冰 冲 决 冴 况 冶 冷 冻 冼 冽 净 凄 准 凈 凉 凊 凋 凌 凍 减 凑 凖 凛 凜 凝 几 凡 凤 処 凧
凪 凫 凭 凯 凰 凱 凳 凶 凸 凹 出 击 凼 函 凿 刀 刁 刃 分 切 刈 刊 刍 刎 刑 划 列 刘 则 刚 创 初 删 判 別 刨 利 刪 别 刮
到 刳 制 刷 券 刹 刺 刻 刽 剁 剂 剃 則 削 剋 剌 前 剎 剑 剔 剖 剛 剝 剡 剣 剤 剥 剧 剩 剪 副 剰 剱 割 創 剷 剽 剿 劃 劇
劈 劉 劊 劍 劑 劔 力 劝 办 功 加 务 劢 劣 动 助 努 劫 劭 励 劲 劳 労 劵 効 劻 劾 势 勁 勃 勅 勇 勉 勋 勐 勒 勔 動 勖 勗
勘 務 勛 勝 勞 募 勢 勣 勤 勧 勰 勱 勲 勳 勵 勸 勺 勻 勾 勿 匀 匁 匂 包 匆 匈 匍 匏 匐 匕 化 北 匙 匝 匠 匡 匣 匪 匮 匯
匱 匹 区 医 匾 匿 區 十 千 卅 升 午 卉 半 卍 华 协 卐 卑 卒 卓 協 单 卖 南 単 博 卜 卞 卟 占 卡 卢 卤 卦 卧 卫 卬 卯 印
危 即 却 卵 卷 卸 卹 卻 卽 卿 厂 厄 厅 历 厉 压 厌 厓 厔 厕 厘 厚 厝 原 厢 厥 厦 厨 厩 厭 厮 厲 厳 去 县 叁 参 參 叄 又
叉 及 友 双 反 収 发 叔 取 受 变 叙 叛 叟 叠 叡 叢 口 古 句 另 叨 叩 只 叫 召 叭 叮 可 台 叱 史 右 叶 号 司 叹 叻 叼 吁
吃 各 合 吉 吊 吋 同 名 后 吏 吐 向 吒 吓 吕 吗 君 吝 吞 吟 吠 吡 否 吧 吨 吩 含 听 吮 启 吱 吲 吳 吴 吵 吶 吸 吹 吻 吼
吽 吾 呀 呂 呃 呆 呈 呉 告 呋 呎 呐 呑 呕 呗 员 呛 呜 呟 呢 呤 呦 周 呪 呱 味 呵 呷 呻 呼 命 咀 咁 咄 咆 咋 和 咎 咏 咐
咒 咔 咕 咖 咗 咙 咚 咤 咥 咧 咨 咩 咪 咫 咬 咭 咯 咱 咲 咳 咸 咽 咿 哀 品 哂 哄 哆 哇 哈 哉 哋 哌 响 哎 哏 哑 哒 哔 哗
哙 哚 哟 員 哥 哦 哨 哩 哪 哭 哮 哲 哺 哼 哽 唁 唄 唆 唇 唎 唐 唑 唔 唖 唛 唢 唤 唧 唬 唭 售 唯 唱 唵 唷 唸 唾 啃 啄 商
啉 啊 問 啓 啖 啜 啞 啟 啡 啤 啥 啦 啪 啬 啮 啰 啲 啶 啸 啼 啾 喀 喂 喃 善 喆 喇 喉 喊 喋 喔 喘 喙 喚 喜 喝 喧 喩 喪 喫
喬 單 喰 喱 喲 喵 営 喷 喹 喺 喻 嗅 嗆 嗇 嗎 嗓 嗔 嗚 嗜 嗝 嗟 嗡 嗣 嗤 嗨 嗩 嗪 嗯 嗲 嗶 嗽 嘅 嘆 嘈 嘉 嘌 嘎 嘏 嘔 嘗
嘘 嘛 嘜 嘟 嘢 嘧 嘩 嘯 嘱 嘲 嘴 嘶 嘹 嘻 嘿 噁 噂 噌 噏 噓 噗 噛 噜 噠 噢 噤 器 噩 噪 噫 噬 噱 噲 噴 噶 噸 噹 噺 噻 嚆
嚇 嚈 嚎 嚏 嚐 嚕 嚙 嚟 嚢 嚣 嚥 嚨 嚮 嚴 嚷 嚼 囁 囂 囃 囉 囊 囍 囑 囓 囗 囚 四 囝 回 因 囡 团 団 囤 囧 囪 园 囮 困 囱
囲 図 围 固 国 图 囿 圀 圃 圆 圈 圉 國 圍 圏 園 圓 圖 團 圜 土 圣 圧 在 圩 圪 圭 圮 圯 地 圳 圹 场 圻 圾 址 坂 均 坊 坌
坍 坎 坏 坐 坑 块 坚 坛 坜 坝 坞 坟 坠 坡 坤 坦 坨 坩 坪 坬 坭 坯 坳 坵 坷 坻 垂 垃 垄 垅 垈 型 垌 垒 垓 垕 垚 垛 垟 垠
垡 垢 垣 垦 垩 垫 垭 垮 垴 垵 垸 埂 埃 埈 埋 城 埏 埒 埔 埕 埗 埙 埜 埝 域 埠 埡 埤 埭 埴 埵 執 埸 培 基 埼 堀 堂 堃 堅
堆 堇 堉 堊 堑 堕 堝 堞 堡 堤 堪 堯 堰 報 場 堵 堺 塀 塁 塊 塋 塌 塑 塔 塗 塘 塙 塚 塞 塡 塢 塩 填 塬 塭 塱 塵 塹 塽 塾
墀 境 墅 墉 墊 墓 墕 増 墘 墙 墜 增 墟 墨 墩 墬 墮 墳 墺 墻 墾 壁 壅 壆 壇 壊 壌 壑 壓 壕 壘 壙 壞 壟 壠 壢 壤 壩 士 壬
壮 壯 声 壱 売 壳 壶 壷 壹 壺 壽 处 备 変 复 夏 夔 夕 外 夙 多 夜 够 夠 夢 夤 夥 大 天 太 夫 夭 央 夯 失 头 夷 夸 夹 夺
夼 夾 奂 奄 奇 奈 奉 奋 奎 奏 奐 契 奔 奕 奖 套 奘 奚 奠 奢 奥 奧 奨 奪 奬 奭 奮 女 奴 奶 奸 她 好 如 妃 妄 妆 妇 妈 妊
妍 妒 妓 妖 妙 妝 妞 妣 妤 妥 妨 妫 妬 妮 妲 妳 妹 妻 妾 姆 姉 姊 始 姍 姐 姑 姒 姓 委 姗 姚 姜 姝 姣 姥 姦 姨 姪 姫 姬
姵 姶 姻 姿 威 娃 娄 娅 娆 娇 娉 娑 娓 娘 娛 娜 娟 娠 娣 娥 娩 娯 娱 娲 娴 娶 娼 婁 婆 婉 婕 婚 婢 婦 婧 婪 婬 婭 婴 婵
婶 婷 婺 婿 媒 媚 媛 媞 媧 媯 媲 媳 媽 媾 嫁 嫂 嫄 嫉 嫌 嫔 嫖 嫗 嫚 嫡 嫣 嫦 嫩 嫻 嬅 嬈 嬉 嬋 嬌 嬖 嬗 嬛 嬢 嬤 嬪 嬬
嬰 嬲 嬴 嬷 嬸 嬿 孀 孃 子 孑 孔 孕 孖 字 存 孙 孚 孛 孜 孝 孟 孢 季 孤 学 孩 孪 孫 孰 孱 孳 孵 學 孺 孽 孿 宀 宁 它 宅
宇 守 安 宋 完 宍 宏 宓 宕 宗 官 宙 定 宛 宜 宝 实 実 宠 审 客 宣 室 宥 宦 宪 宫 宮 宰 害 宴 宵 家 宸 容 宽 宾 宿 寂 寄
寅 密 寇 富 寐 寒 寓 寔 寘 寛 寝 寞 察 寡 寢 寥 實 寧 寨 審 寫 寬 寮 寯 寰 寳 寵 寶 寸 对 寺 寻 导 対 寿 封 専 射 将 將
專 尉 尊 尋 對 導 小 少 尔 尕 尖 尘 尙 尚 尝 尤 尧 尪 尬 尭 就 尴 尷 尸 尹 尺 尻 尼 尽 尾 尿 局 屁 层 居 屆 屈 届 屋 屌
屍 屎 屏 屐 屑 屓 展 属 屠 屡 屢 層 履 屬 屯 山 屹 屿 岀 岁 岂 岌 岐 岑 岔 岖 岗 岘 岙 岚 岛 岡 岢 岩 岫 岬 岭 岱 岳 岷
岸 峁 峄 峇 峋 峒 峙 峠 峡 峤 峥 峦 峨 峩 峪 峭 峯 峰 峴 島 峻 峽 崁 崂 崆 崇 崋 崍 崎 崐 崑 崔 崖 崗 崙 崚 崛 崞 崢 崤
崧 崩 崭 崮 崱 崴 崽 嵇 嵊 嵋 嵌 嵐 嵘 嵙 嵜 嵩 嵬 嵯 嵴 嶂 嶄 嶇 嶋 嶌 嶗 嶙 嶝 嶠 嶧 嶴 嶷 嶸 嶺 嶼 嶽 巂 巅 巌 巍 巒
巔 巖 川 州 巡 巢 巣 工 左 巧 巨 巩 巫 差 己 已 巳 巴 巷 巻 巽 巾 巿 币 市 布 帅 帆 师 希 帐 帑 帕 帖 帘 帙 帚 帛 帜 帝
帥 带 帧 師 席 帮 帯 帰 帳 帶 帷 常 帼 帽 幀 幂 幄 幅 幇 幌 幔 幕 幗 幟 幡 幢 幣 幪 幫 干 平 年 并 幷 幸 幹 幺 幻 幼 幽
幾 广 庁 広 庄 庆 庇 床 序 庐 庑 库 应 底 庖 店 庙 庚 府 庞 废 庠 庥 度 座 庫 庭 庵 庶 康 庸 庹 庾 廁 廂 廃 廄 廆 廈 廉
廊 廍 廓 廖 廙 廚 廝 廞 廟 廠 廡 廢 廣 廨 廩 廪 廬 廳 延 廷 廸 建 廻 廼 廿 开 弁 异 弃 弄 弇 弈 弉 弊 弋 弌 式 弐 弑 弒
弓 弔 引 弖 弗 弘 弛 弟 张 弢 弥 弦 弧 弩 弭 弯 弱 張 強 弹 强 弼 弾 彀 彅 彈 彊 彌 彎 归 当 录 彗 彘 彙 彝 彡 形 彤 彥
彦 彧 彩 彪 彫 彬 彭 彰 影 彳 彷 役 彻 彼 彿 往 征 徂 径 待 徇 很 徊 律 後 徐 徑 徒 従 徕 得 徘 徙 從 徠 御 徨 復 循 徬
徭 微 徳 徴 徵 德 徹 徽 心 忄 必 忆 忉 忌 忍 忏 忒 志 忘 忙 応 忞 忠 忡 忤 忧 快 忱 念 忻 忽 忿 怀 态 怂 怎 怒 怕 怖 怙
怛 怜 思 怠 怡 急 怦 性 怨 怪 怯 总 怿 恂 恃 恆 恋 恍 恐 恒 恕 恙 恚 恢 恣 恤 恥 恨 恩 恪 恫 恬 恭 息 恰 恳 恵 恶 恸 恺
恼 恽 恿 悄 悅 悉 悌 悍 悔 悖 悚 悛 悝 悟 悠 患 悦 悧 您 悩 悪 悫 悬 悯 悰 悲 悳 悴 悶 悸 悼 悽 情 惇 惊 惋 惑 惕 惘 惚
惛 惜 惟 惠 惡 惣 惧 惨 惩 惫 惭 惮 惯 惰 惱 惲 想 惶 惹 惺 愁 愃 愆 愈 愉 愍 愎 意 愔 愕 愚 愛 感 愤 愧 愨 愫 愴 愷 愼
愾 愿 慄 慇 慈 態 慌 慎 慑 慕 慘 慚 慜 慟 慢 慣 慤 慧 慨 慫 慮 慰 慳 慵 慶 慷 慾 憂 憊 憍 憎 憐 憑 憔 憙 憚 憤 憧 憨 憩
憫 憬 憲 憶 憺 憾 懂 懃 懇 懈 應 懊 懋 懌 懐 懒 懦 懲 懵 懶 懷 懸 懺 懼 懾 懿 戀 戈 戊 戌 戍 戎 戏 成 我 戒 戔 戕 或 战
戚 戛 戟 戡 戢 戦 戩 截 戮 戯 戰 戱 戲 戳 戴 戶 户 戸 戻 戾 房 所 扁 扆 扇 扈 扉 手 扌 才 扎 扑 扒 打 扔 払 托 扛 扣 扦
执 扩 扫 扬 扭 扮 扯 扰 扱 扳 扶 批 扼 找 承 技 抄 抉 把 抑 抒 抓 投 抖 抗 折 抚 抛 抜 択 抟 抡 抢 护 报 抨 披 抬 抱 抵
抹 押 抽 拂 担 拆 拇 拈 拉 拋 拌 拍 拏 拐 拒 拓 拔 拖 拗 拘 拙 拚 招 拜 拝 拟 拠 拡 拢 拣 拥 拦 拨 择 括 拭 拮 拯 拱 拳
拴 拵 拶 拷 拼 拽 拾 拿 持 挂 指 挈 按 挑 挖 挙 挚 挛 挝 挞 挟 挠 挡 挣 挤 挥 挨 挪 挫 振 挹 挺 挽 挾 挿 捅 捆 捉 捌 捍
捎 捏 捐 捒 捕 捗 捜 捞 损 捡 换 捣 捧 捨 捩 据 捱 捲 捶 捷 捺 捻 掀 掃 授 掉 掌 掏 掐 排 掖 掘 掙 掛 掟 掠 採 探 掣 接
控 推 掩 措 掬 掰 掲 掳 掴 掷 掸 掺 掻 掾 揀 揃 揄 揆 揉 揍 描 提 插 揖 揚 換 握 揣 揪 揭 揮 援 揶 揺 揽 揾 搁 搅 損 搏
搐 搓 搔 搖 搗 搜 搞 搦 搪 搬 搭 搵 搶 携 搾 摂 摄 摆 摇 摊 摑 摒 摔 摘 摠 摧 摩 摭 摯 摶 摸 摹 摺 摻 撃 撇 撈 撐 撑 撒
撓 撕 撚 撞 撣 撤 撥 撩 撫 撬 播 撮 撰 撲 撷 撹 撻 撼 撿 擁 擂 擄 擅 擇 擊 擋 操 擎 擒 擔 擘 據 擠 擢 擦 擧 擬 擱 擲 擴
擷 擺 擾 攀 攏 攒 攔 攘 攜 攝 攢 攣 攤 攪 攫 攬 支 收 攷 攸 改 攻 放 政 故 效 敌 敍 敎 敏 救 敕 敖 敗 敘 教 敛 敝 敞 敢
散 敦 敬 数 敲 整 敵 敷 數 斂 斃 文 斉 斋 斌 斎 斐 斑 斗 料 斛 斜 斟 斡 斤 斥 斧 斩 斫 斬 断 斯 新 斷 方 於 施 旁 旃 旅
旋 旌 族 旒 旗 旛 无 既 旣 日 旦 旧 旨 早 旬 旭 旱 时 旷 旸 旺 旻 旼 昀 昂 昆 昇 昉 昊 昌 明 昏 昐 易 昔 昕 昙 昝 昞 星
映 春 昧 昨 昪 昭 是 昰 昱 昴 昵 昶 昺 昼 显 晁 時 晃 晄 晉 晋 晏 晒 晓 晔 晕 晖 晗 晙 晚 晝 晞 晟 晤 晦 晧 晨 晩 普 景
晰 晳 晴 晶 晷 晸 智 晾 暁 暂 暄 暇 暈 暉 暌 暎 暐 暑 暖 暗 暘 暝 暠 暢 暦 暧 暨 暫 暮 暱 暲 暴 暹 暻 暾 曄 曆 曇 曉 曖
曙 曜 曝 曠 曦 曩 曬 曰 曲 曳 更 曷 書 曹 曺 曼 曽 曾 替 最 會 月 有 朋 服 朐 朓 朔 朕 朗 望 朝 期 朦 朧 木 未 末 本 札
朮 术 朱 朴 朵 朶 机 朽 杀 杂 权 杆 杉 杌 李 杏 材 村 杓 杖 杙 杜 杞 束 杠 条 杢 杣 来 杨 杭 杮 杯 杰 東 杲 杳 杵 杷 杼
松 板 极 构 枇 枉 枋 析 枕 林 枚 果 枝 枞 枠 枡 枢 枣 枪 枫 枭 枯 枳 架 枷 枸 枹 柁 柃 柄 柊 柏 某 柑 柒 染 柔 柘 柚 柜
柝 柞 柠 柢 查 柩 柬 柯 柱 柳 柴 柵 査 柽 柾 柿 栂 栃 栄 栅 标 栈 栉 栋 栎 栏 树 栒 栓 栖 栗 栞 校 栢 栩 株 栱 栲 栴 样
核 根 栻 格 栽 栾 桀 桁 桂 桃 桅 框 案 桉 桌 桎 桐 桑 桓 桔 桜 桝 桟 桡 桢 档 桥 桦 桧 桨 桩 桫 桶 桷 桿 梁 梃 梅 梆 梓
梔 梗 條 梟 梠 梢 梦 梧 梨 梭 梯 械 梱 梳 梵 梶 检 棂 棄 棉 棋 棍 棒 棕 棗 棘 棚 棟 棠 棣 棧 棨 棫 森 棱 棲 棵 棹 棺 棻
椀 椁 椅 椋 植 椎 椏 椒 椙 椚 椛 検 椭 椰 椴 椹 椽 椿 楂 楊 楓 楔 楕 楙 楚 楝 楞 楠 楡 楢 楣 楨 楫 業 楮 楯 楳 極 楷 楸
楹 楼 楽 概 榄 榆 榈 榉 榊 榎 榑 榔 榕 榖 榘 榛 榜 榧 榨 榫 榭 榮 榴 榷 榻 槁 槃 槇 槊 構 槌 槍 槎 槐 槓 様 槙 槛 槟 槤
槨 槪 槭 槲 槳 槺 槻 槽 槿 樁 樂 樅 樊 樋 樑 樓 樗 標 樞 樟 模 樣 樨 権 横 樫 樱 樵 樸 樹 樺 樽 樾 橄 橇 橈 橋 橐 橘 橙
機 橡 橢 橫 橱 橹 橿 檀 檄 檉 檎 檐 檔 檗 檜 檢 檣 檬 檯 檳 檸 檻 櫂 櫃 櫓 櫚 櫛 櫟 櫥 櫨 櫸 櫻 欄 欅 欉 權 欒 欖 欞 欠
次 欢 欣 欤 欧 欲 欺 欽 款 歆 歇 歉 歌 歎 歐 歓 歙 歟 歡 止 正 此 步 武 歧 歩 歪 歯 歲 歳 歴 歷 歸 歹 死 歼 歿 殁 殃 殆
殇 殉 殊 残 殒 殓 殖 殘 殞 殡 殤 殭 殮 殯 殲 殴 段 殷 殺 殻 殼 殿 毀 毁 毂 毅 毆 毋 毌 母 毎 每 毒 毓 比 毕 毖 毗 毘 毙
毛 毡 毫 毬 毯 毽 氂 氈 氏 氐 民 氓 气 氖 気 氘 氙 氚 氛 氟 氡 氢 氣 氦 氧 氨 氩 氪 氫 氬 氮 氯 氰 水 氵 氷 永 氹 氾 汀
汁 求 汇 汉 汊 汎 汐 汕 汗 汙 汚 汛 汜 汝 汞 江 池 污 汤 汧 汨 汪 汭 汰 汲 汴 汶 汹 決 汽 汾 沁 沂 沃 沄 沅 沆 沈 沉 沌
沐 沒 沓 沔 沖 沙 沛 沟 没 沢 沣 沥 沦 沧 沪 沫 沭 沮 沱 河 沸 油 治 沼 沽 沾 沿 況 泄 泉 泊 泌 泓 法 泗 泚 泛 泞 泠 泡
波 泣 泥 注 泪 泫 泮 泯 泰 泱 泳 泵 泷 泸 泻 泼 泽 泾 洁 洄 洋 洎 洒 洗 洙 洛 洞 津 洧 洩 洪 洮 洱 洲 洵 洶 洸 洹 洺 活
洼 洽 派 流 浄 浅 浆 浇 浊 测 济 浏 浑 浒 浓 浔 浙 浚 浜 浞 浠 浣 浤 浦 浩 浪 浬 浮 浯 浴 海 浸 涂 涅 涇 消 涉 涌 涎 涓
涔 涕 涙 涛 涜 涝 涞 涟 涡 涣 涤 润 涧 涨 涩 涪 涮 涯 液 涵 涸 涼 涿 淀 淄 淅 淆 淇 淋 淌 淑 淒 淖 淘 淙 淚 淝 淞 淡 淤
淦 淨 淩 淪 淫 淬 淮 淯 深 淳 淵 淶 混 淸 淹 淺 添 淼 清 渇 済 渉 渊 渋 渍 渎 渐 渓 渔 渕 渗 渙 渚 減 渝 渟 渠 渡 渣 渤
渥 渦 温 渫 測 渭 港 渲 渴 游 渺 渾 湃 湄 湊 湍 湖 湘 湛 湜 湟 湣 湧 湫 湮 湯 湳 湾 湿 満 溃 溅 溆 溉 溏 源 準 溜 溝 溟
溢 溥 溧 溪 溫 溯 溱 溲 溴 溶 溺 溼 滁 滂 滄 滅 滇 滉 滋 滌 滎 滏 滑 滓 滔 滕 滘 滙 滚 滝 滞 满 滢 滤 滥 滦 滨 滩 滬 滯
滲 滴 滷 滸 滹 滾 滿 漁 漂 漆 漉 漏 漑 漓 演 漕 漠 漢 漣 漩 漪 漫 漬 漯 漱 漲 漳 漵 漸 漾 漿 潁 潇 潍 潑 潔 潘 潛 潜 潞
潟 潢 潤 潦 潭 潮 潯 潰 潴 潺 潼 潾 澀 澁 澂 澄 澆 澇 澈 澍 澎 澗 澜 澠 澡 澤 澧 澪 澮 澱 澳 澶 澹 激 濁 濂 濃 濉 濊 濑
濒 濕 濘 濛 濞 濟 濠 濡 濤 濫 濬 濮 濯 濰 濱 濵 濺 濾 瀅 瀆 瀉 瀋 瀏 瀑 瀕 瀘 瀚 瀛 瀝 瀞 瀟 瀧 瀨 瀬 瀰 瀾 灃 灌 灏 灑
灕 灘 灝 灞 灣 灤 火 灭 灯 灰 灵 灶 灸 灼 災 灾 灿 炀 炁 炅 炆 炉 炊 炎 炒 炔 炕 炖 炘 炙 炜 炤 炫 炬 炭 炮 炯 炱 炳 炸
点 為 炼 炽 烁 烂 烃 烈 烏 烘 烙 烛 烜 烝 烟 烤 烦 烧 烨 烩 烫 烬 热 烯 烴 烷 烹 烺 烽 焉 焊 焓 焔 焕 焗 焘 焙 焚 焜 無
焦 焯 焰 焱 然 焼 煇 煉 煊 煌 煎 煒 煕 煖 煙 煚 煜 煞 煤 煥 煦 照 煨 煩 煬 煮 煲 煽 熄 熈 熊 熏 熒 熔 熙 熟 熠 熨 熬 熱
熲 熵 熹 熾 燁 燃 燄 燈 燉 燊 燎 燐 燒 燔 燕 燗 燙 營 燥 燦 燧 燬 燭 燮 燴 燹 燻 燼 燾 燿 爀 爆 爍 爐 爛 爨 爪 爬 爭 爰
爱 爲 爵 父 爷 爸 爹 爺 爻 爽 爾 牂 牆 片 版 牌 牍 牒 牘 牙 牛 牝 牟 牠 牡 牢 牧 物 牯 牲 牴 牵 特 牺 牻 牽 犀 犁 犂 犄
犊 犍 犒 犛 犠 犢 犧 犬 犯 犰 状 犷 犸 犹 犽 狀 狂 狄 狈 狍 狎 狐 狒 狗 狙 狛 狠 狡 狨 狩 独 狭 狮 狱 狳 狷 狸 狹 狼 狽
猁 猊 猎 猕 猖 猗 猛 猜 猝 猞 猟 猥 猩 猪 猫 猬 献 猴 猶 猷 猾 猿 獁 獄 獅 獎 獏 獐 獒 獗 獠 獣 獨 獬 獭 獰 獲 獴 獵 獷
獸 獺 獻 獼 獾 玄 玆 率 玉 王 玑 玕 玖 玘 玛 玟 玠 玢 玥 玩 玫 玮 环 现 玲 玳 玷 玹 玺 玻 珀 珂 珅 珈 珉 珊 珍 珏 珐 珑
珙 珝 珞 珠 珣 珥 珦 珩 珪 班 珮 珲 珺 珽 現 球 琅 理 琇 琉 琊 琍 琏 琐 琚 琛 琢 琤 琥 琦 琨 琪 琬 琮 琯 琰 琲 琳 琴 琵
琶 琺 琼 琿 瑀 瑁 瑄 瑈 瑊 瑋 瑕 瑗 瑙 瑚 瑛 瑜 瑞 瑟 瑠 瑢 瑣 瑤 瑩 瑪 瑭 瑯 瑰 瑱 瑳 瑶 瑷 瑾 璀 璁 璃 璆 璇 璈 璉 璋
璐 璘 璜 璞 璟 璠 璣 璥 璦 璧 璨 璩 璪 環 璲 璹 璽 璿 瓊 瓌 瓏 瓒 瓔 瓘 瓚 瓛 瓜 瓠 瓢 瓣 瓦 瓩 瓮 瓯 瓶 瓷 甄 甌 甑 甕
甘 甚 甜 生 甡 產 産 甥 甦 用 甩 甫 甬 甯 田 由 甲 申 电 男 甸 町 画 甾 畀 畅 畈 畋 界 畏 畑 畔 留 畜 畝 畠 畢 畤 略 畦
番 畫 畬 畯 異 畲 畳 畴 畵 當 畷 畸 畹 畿 疃 疆 疇 疊 疋 疍 疎 疏 疑 疒 疗 疙 疚 疝 疟 疡 疣 疤 疥 疫 疮 疯 疱 疲 疵 疸
疹 疼 疽 疾 痂 病 症 痉 痊 痍 痒 痔 痕 痘 痙 痛 痞 痢 痣 痤 痩 痪 痫 痰 痲 痴 痹 痺 瘀 瘁 瘋 瘍 瘓 瘙 瘟 瘠 瘡 瘢 瘤 瘦
瘧 瘩 瘫 瘴 瘾 療 癇 癌 癒 癖 癡 癢 癣 癥 癩 癪 癫 癬 癮 癱 癲 癸 発 登 發 白 百 皂 的 皆 皇 皈 皋 皎 皐 皓 皖 皙 皝 皮
皰 皱 皺 皿 盂 盃 盅 盆 盈 益 盎 盏 盐 监 盒 盔 盖 盗 盘 盛 盜 盞 盟 盡 監 盤 盥 盧 盩 盪 目 盯 盱 盲 直 相 盼 盾 省 眈
眉 看 県 眙 眞 真 眠 眨 眩 眭 眶 眷 眸 眺 眼 眾 着 睁 睇 睐 睑 睛 睜 睞 睡 睢 督 睦 睨 睪 睫 睬 睹 睺 睽 睾 睿 瞄 瞋 瞌
瞎 瞑 瞒 瞞 瞥 瞧 瞩 瞪 瞬 瞭 瞰 瞳 瞻 瞼 瞽 瞿 矇 矍 矗 矚 矛 矜 矢 矣 知 矧 矩 矫 短 矮 矯 石 矶 矽 矾 矿 砀 码 砂 砌
砍 砒 研 砕 砖 砚 砜 砥 砦 砧 砬 砭 砰 砲 破 砵 砷 砸 砺 砾 砿 础 硃 硅 硏 硐 硒 硕 硖 硝 硤 硫 硬 确 硯 硼 碁 碇 碉 碌
碍 碎 碑 碓 碕 碗 碘 碚 碛 碟 碣 碧 碩 碭 碰 碱 碲 碳 碴 碶 碸 確 碼 碾 磁 磅 磊 磋 磐 磔 磕 磘 磚 磡 磧 磨 磬 磯 磴 磷
磺 磻 磾 礁 礎 礒 礙 礦 礪 礫 礬 礴 示 礼 礽 社 祀 祁 祂 祆 祇 祈 祉 祎 祏 祐 祓 祔 祕 祖 祗 祚 祛 祜 祝 神 祟 祠 祢 祥
票 祭 祯 祷 祸 祺 祿 禀 禁 禄 禅 禊 禍 禎 福 禑 禔 禕 禛 禦 禧 禩 禪 禮 禰 禱 禹 禺 离 禽 禾 禿 秀 私 秃 秆 秉 秋 种 科
秒 秘 租 秣 秤 秦 秧 秩 秭 积 称 秸 移 秽 稀 稃 稅 稈 程 稍 税 稔 稗 稙 稚 稜 稟 稠 稣 種 稱 稲 稳 稷 稹 稻 稼 稽 稿 穀
穂 穆 穌 積 穎 穏 穐 穗 穢 穣 穩 穫 穰 穴 究 穷 穹 空 穿 突 窃 窄 窈 窍 窑 窒 窓 窕 窖 窗 窘 窜 窝 窟 窠 窣 窥 窦 窩 窪
窮 窯 窺 窿 竄 竅 竇 竈 竊 立 竑 竖 站 竜 竝 竞 竟 章 竣 童 竦 竪 竭 端 競 竹 竺 竿 笃 笄 笆 笈 笋 笏 笑 笔 笕 笙 笛 笞
笠 笥 符 笨 笪 第 笮 笳 笹 笺 笼 筅 筆 筈 等 筊 筋 筌 筍 筏 筐 筑 筒 答 策 筛 筝 筠 筥 筧 筮 筰 筱 筲 筵 筷 筹 筺 签 简
箇 箋 箍 箏 箐 箒 箓 箔 箕 算 箚 箝 管 箪 箫 箬 箭 箱 箴 箸 節 篁 範 篆 篇 築 篋 篙 篝 篠 篡 篤 篦 篩 篪 篭 篮 篱 篷 篾
簀 簃 簇 簋 簑 簒 簕 簗 簡 簧 簪 簫 簷 簸 簽 簾 簿 籀 籁 籃 籌 籍 籐 籓 籔 籙 籟 籠 籤 籬 籲 米 类 籽 籾 粁 粂 粄 粉 粋
粍 粑 粒 粕 粗 粘 粛 粟 粤 粥 粧 粪 粮 粱 粲 粳 粵 粹 粽 精 粿 糀 糅 糊 糍 糎 糕 糖 糗 糙 糜 糞 糟 糠 糧 糬 糯 糰 糸 糹
糺 系 糾 紀 紂 約 紅 紆 紇 紉 紊 紋 納 紐 紓 純 紗 紘 紙 級 紛 紜 素 紡 索 紧 紫 紬 紮 累 細 紳 紹 紺 終 絃 組 絅 絆 経
結 絕 絜 絞 絡 絢 絣 給 絨 絮 統 絲 絳 絵 絶 絹 綁 綃 綉 綏 綑 經 継 続 綜 綝 綠 綢 綦 綫 綬 維 綰 綱 網 綴 綵 綸 綺 綻
綽 綾 綿 緊 緋 総 緑 緒 緖 緘 線 緝 緞 締 緡 緣 編 緩 緬 緯 緲 練 緹 緻 縁 縄 縈 縉 縊 縛 縝 縞 縣 縦 縫 縮 縯 縱 縷 縹
縻 總 績 繁 繃 繆 繇 繊 繋 繍 繒 織 繕 繙 繚 繞 繡 繩 繪 繫 繭 繰 繳 繹 繼 繽 纂 纈 續 纍 纏 纓 纖 纘 纛 纜 纠 红 纣 纤
纥 约 级 纪 纫 纬 纭 纮 纯 纱 纲 纳 纵 纶 纷 纸 纹 纺 纽 纾 线 绀 绂 练 组 绅 细 织 终 绊 绍 绎 经 绑 绒 结 绕 绘 给 绚
绛 络 绝 绞 统 绡 绢 绣 绥 继 绩 绪 绫 续 绮 绯 绰 绳 维 绵 绶 绷 绸 综 绽 绾 绿 缀 缄 缅 缆 缇 缉 缎 缓 缔 缕 编 缘 缙
缚 缜 缝 缟 缠 缢 缤 缨 缩 缪 缬 缭 缮 缯 缴 缵 缶 缸 缺 缽 罂 罄 罅 罈 罌 罐 网 罔 罕 罗 罘 罚 罟 罠 罡 罢 罩 罪 置 罰
署 罵 罷 罹 罽 羁 羅 羆 羈 羊 羋 羌 美 羔 羚 羞 羟 羡 羣 群 羥 羧 羨 義 羯 羰 羲 羸 羹 羽 羿 翀 翁 翅 翊 翌 翎 習 翔 翕
翘 翟 翠 翡 翥 翦 翩 翫 翮 翰 翱 翳 翹 翻 翼 耀 老 考 者 耆 而 耍 耐 耒 耕 耗 耘 耙 耜 耦 耨 耳 耶 耸 耻 耽 耿 聂 聃 聆
聊 聋 职 联 聖 聘 聚 聞 聡 聪 聯 聰 聲 聳 聴 聶 職 聽 聾 聿 肃 肄 肅 肆 肇 肉 肋 肌 肓 肖 肘 肚 肛 肜 肝 肟 肠 股 肢 肤
肥 肩 肪 肮 肯 肱 育 肴 肺 肼 肽 肾 肿 胀 胁 胃 胄 胆 背 胍 胎 胖 胚 胛 胜 胝 胞 胡 胤 胥 胧 胪 胫 胭 胯 胰 胱 胳 胴 胶
胸 胺 胼 能 脂 脅 脆 脇 脈 脉 脊 脍 脏 脐 脑 脓 脖 脚 脛 脣 脩 脫 脯 脱 脲 脳 脷 脸 脹 脾 腆 腈 腊 腋 腌 腎 腐 腑 腓 腔
腕 腥 腦 腩 腫 腭 腮 腰 腱 腳 腴 腸 腹 腺 腻 腾 腿 膀 膂 膈 膊 膏 膑 膚 膛 膜 膝 膠 膣 膦 膨 膩 膳 膵 膺 膽 膾 膿 臀 臂
臆 臈 臉 臍 臏 臓 臘 臚 臟 臣 臥 臧 臨 自 臬 臭 至 致 臺 臻 臼 臾 舁 舂 舅 舆 與 興 舉 舊 舌 舍 舎 舐 舒 舔 舖 舗 舘 舛
舜 舞 舟 舢 舩 航 舫 般 舯 舰 舱 舳 舵 舶 舷 舸 船 舺 舾 艀 艇 艉 艋 艏 艘 艙 艛 艤 艦 艮 良 艰 艱 色 艳 艶 艷 艸 艹 艺
艾 节 芃 芈 芊 芋 芍 芎 芒 芗 芙 芜 芝 芡 芥 芦 芨 芩 芪 芫 芬 芭 芮 芯 花 芳 芷 芸 芹 芻 芽 芾 苄 苅 苇 苋 苌 苍 苎 苏
苑 苓 苔 苕 苗 苛 苜 苞 苟 苡 苣 若 苦 苧 苫 苯 英 苳 苴 苷 苹 苺 苻 苾 茁 茂 范 茄 茅 茉 茌 茎 茔 茗 茛 茜 茧 茨 茫 茯
茱 茲 茴 茵 茶 茸 茹 荀 荃 荆 草 荊 荏 荐 荒 荔 荖 荘 荚 荞 荟 荠 荡 荣 荤 荥 荧 荨 荩 荪 荫 药 荳 荷 荸 荻 荼 荽 莅 莆
莉 莊 莎 莒 莓 莖 莘 莞 莠 莢 莧 莨 莪 莫 莱 莲 莳 莴 获 莹 莺 莽 莿 菀 菁 菅 菇 菈 菊 菌 菏 菓 菖 菘 菜 菝 菟 菠 菡 菩
菫 華 菰 菱 菲 菴 菸 菽 萁 萃 萄 萇 萊 萌 萍 萎 萘 萜 萝 萠 萣 萤 营 萦 萧 萨 萩 萬 萱 萵 萸 萼 落 葆 葉 葎 著 葛 葜 葡
董 葦 葩 葫 葬 葭 葯 葱 葳 葵 葶 葷 葺 蒂 蒋 蒐 蒔 蒙 蒜 蒞 蒟 蒡 蒨 蒯 蒲 蒴 蒸 蒺 蒻 蒼 蒽 蒾 蒿 蓀 蓁 蓄 蓆 蓉 蓋 蓍
蓑 蓓 蓖 蓝 蓟 蓣 蓬 蓮 蓼 蓿 蔀 蔑 蔓 蔔 蔗 蔘 蔚 蔡 蔣 蔥 蔦 蔬 蔭 蔴 蔵 蔷 蔺 蔻 蔼 蔽 蕁 蕃 蕈 蕉 蕊 蕎 蕗 蕙 蕤 蕨
蕩 蕪 蕭 蕲 蕴 蕷 蕾 薀 薄 薇 薈 薊 薌 薏 薑 薔 薗 薙 薛 薜 薦 薨 薩 薪 薫 薬 薮 薯 薰 薷 薹 薺 藁 藉 藍 藎 藏 藐 藓 藔
藕 藜 藝 藤 藥 藨 藩 藪 藷 藹 藺 藻 藿 蘂 蘄 蘅 蘆 蘇 蘊 蘋 蘑 蘚 蘭 蘸 蘿 虎 虏 虐 虑 虓 虔 處 虚 虛 虜 虞 號 虢 虧 虫
虬 虯 虱 虹 虻 虽 虾 蚀 蚁 蚂 蚊 蚋 蚌 蚓 蚕 蚜 蚝 蚣 蚤 蚩 蚪 蚬 蚯 蚱 蚵 蚶 蚺 蛀 蛄 蛆 蛇 蛉 蛊 蛋 蛍 蛎 蛏 蛔 蛙 蛛
蛞 蛟 蛤 蛭 蛮 蛯 蛰 蛱 蛳 蛸 蛹 蛺 蛻 蛾 蜀 蜂 蜃 蜆 蜈 蜉 蜊 蜍 蜑 蜒 蜓 蜕 蜗 蜘 蜚 蜜 蜡 蜢 蜥 蜱 蜴 蜷 蜻 蜿 蝇 蝉
蝋 蝌 蝎 蝓 蝕 蝗 蝙 蝟 蝠 蝣 蝦 蝨 蝮 蝰 蝴 蝶 蝸 蝽 蝾 蝿 螂 螃 螄 螈 融 螞 螟 螢 螨 螫 螭 螯 螳 螺 螽 蟀 蟄 蟆 蟇 蟋
蟎 蟑 蟒 蟜 蟠 蟬 蟲 蟷 蟹 蟻 蟾 蠅 蠊 蠍 蠑 蠔 蠕 蠟 蠡 蠢 蠣 蠱 蠲 蠵 蠶 蠹 蠻 血 衅 衆 衊 行 衍 衒 術 衔 衕 街 衙 衚
衛 衝 衞 衡 衢 衣 补 表 衫 衬 衮 衰 衷 衹 衽 衾 衿 袁 袂 袄 袈 袋 袍 袒 袖 袛 袜 袞 袢 袤 被 袭 袱 袴 裁 裂 装 裏 裒 裔
裕 裘 裙 補 裝 裟 裡 裤 裨 裱 裳 裴 裵 裸 裹 製 裾 褂 複 褌 褐 褒 褓 褔 褚 褥 褪 褫 褲 褶 褻 襁 襄 襖 襞 襟 襦 襪 襯 襲
襷 西 要 覃 覆 覇 見 規 覓 視 覗 覚 覦 覧 親 覬 覲 観 覺 覽 觀 见 观 规 觅 视 览 觉 觊 觎 觐 角 觚 觞 解 触 觴 觸 言 訁
訂 訃 訇 計 訊 訌 討 訓 訕 訖 託 記 訛 訝 訟 訢 訣 訥 訪 設 許 訳 訴 訶 診 註 証 訾 詁 詆 詈 詐 詒 詔 評 詛 詞 詠 詡 詢
詣 試 詧 詩 詫 詬 詭 詮 詰 話 該 詳 詵 詹 詼 誅 誇 誉 誊 誌 認 誑 誓 誕 誘 語 誠 誡 誣 誤 誥 誦 誨 說 説 読 誰 課 誹 誼
誾 調 諂 諄 談 請 諌 諍 諏 諒 論 諛 諜 諡 諤 諦 諧 諫 諭 諮 諱 諲 諳 諴 諶 諷 諸 諺 諾 謀 謁 謂 謄 謇 謊 謎 謐 謔 謖 謗
謙 謚 講 謝 謠 謡 謨 謫 謬 謳 謹 謾 譁 證 譏 識 譙 譚 譜 警 譬 譯 議 譲 譴 護 譽 讀 讃 變 讎 讐 讒 讓 讖 讚 讞 计 订 讣
认 讥 讦 讧 讨 让 讪 讫 训 议 讯 记 讲 讳 讴 讶 讷 许 讹 论 讼 讽 设 访 诀 证 诂 诃 评 诅 识 诈 诉 诊 诋 词 诏 译 诒 试
诗 诘 诙 诚 诛 诜 话 诞 诟 诠 诡 询 诣 诤 该 详 诧 诩 诫 诬 语 误 诰 诱 诲 说 诵 请 诸 诹 诺 读 诽 课 谀 谁 调 谄 谅 谈
谊 谋 谌 谍 谎 谏 谐 谑 谒 谓 谔 谕 谗 谘 谙 谚 谛 谜 谟 谢 谣 谤 谥 谦 谧 谨 谩 谪 谬 谭 谯 谱 谳 谴 谶 谷 谿 豁 豆 豈
豉 豊 豌 豎 豐 豔 豕 豚 象 豢 豨 豪 豫 豬 豳 豸 豹 豺 貂 貉 貊 貌 貓 貘 貝 貞 負 財 貢 貧 貨 販 貪 貫 責 貯 貰 貳 貴 貶
買 貸 費 貼 貽 貿 賀 賁 賂 賃 賄 資 賈 賊 賎 賑 賓 賚 賛 賜 賞 賠 賡 賢 賣 賤 賦 質 賬 賭 賴 賺 購 賽 賾 贄 贅 贇 贈 贊
贋 贍 贏 贓 贔 贖 贛 贝 贞 负 贡 财 责 贤 败 账 货 质 贩 贪 贫 贬 购 贮 贯 贰 贱 贲 贴 贵 贷 贸 费 贺 贻 贼 贽 贾 贿 赀
赁 赂 赃 资 赈 赉 赋 赌 赍 赎 赏 赐 赓 赔 赖 赘 赚 赛 赝 赞 赟 赠 赡 赢 赣 赤 赦 赧 赫 赭 走 赳 赴 赵 赶 起 趁 超 越 趋
趕 趙 趟 趣 趨 足 趴 趸 趺 趾 跃 跆 跋 跌 跏 跑 跖 跗 跛 距 跟 跡 跤 跨 跪 路 跳 践 跷 跸 跻 踊 踏 踐 踝 踞 踢 踩 踪 踰
踴 踵 踹 蹂 蹄 蹇 蹈 蹉 蹊 蹋 蹕 蹙 蹟 蹠 蹤 蹦 蹬 蹲 蹴 蹶 蹺 蹼 躁 躄 躅 躇 躉 躊 躋 躍 躏 躑 躓 躙 躪 身 躬 躯 躰 躲
躺 躾 軀 車 軋 軌 軍 軒 軔 軛 軟 転 軫 軸 軻 軼 軽 軾 較 輅 載 輋 輌 輒 輓 輔 輕 輛 輜 輝 輟 輥 輦 輩 輪 輯 輳 輸 輻 輾
輿 轂 轄 轅 轆 轉 轍 轎 轟 轡 轢 轤 车 轧 轨 轩 轫 转 轭 轮 软 轰 轲 轴 轶 轸 轻 轼 载 轿 辂 较 辄 辅 辆 辇 辈 辉 辊 辍
辎 辐 辑 输 辕 辖 辗 辘 辙 辛 辜 辞 辟 辣 辦 辨 辩 辫 辭 辮 辯 辰 辱 農 辶 边 辺 辻 込 辽 达 辿 迁 迂 迄 迅 过 迈 迎 运
近 返 还 这 进 远 违 连 迟 迢 迤 迥 迦 迨 迩 迪 迫 迭 述 迳 迴 迷 迸 迹 迺 追 退 送 适 逃 逄 逅 逆 选 逊 逋 逍 透 逐 逑
递 逓 途 逕 逖 逗 這 通 逛 逝 逞 速 造 逡 逢 連 逮 逯 週 進 逵 逸 逹 逻 逼 逾 遁 遂 遅 遇 遊 運 遍 過 遏 遐 遑 遒 道 達
違 遗 遘 遙 遜 遞 遠 遡 遣 遥 遨 適 遭 遮 遲 遴 遵 遶 遷 選 遹 遺 遼 遽 避 邀 邁 邂 邃 還 邇 邈 邉 邊 邏 邑 邓 邕 邗 邙
邛 邝 邠 邡 邢 那 邦 邨 邪 邬 邮 邯 邰 邱 邳 邴 邵 邸 邹 邺 邻 邽 邾 郁 郃 郅 郇 郊 郎 郑 郓 郕 郗 郛 郜 郝 郞 郡 郢 郤
郦 郧 部 郫 郭 郯 郴 郵 郷 郸 都 郾 郿 鄂 鄄 鄆 鄉 鄒 鄔 鄕 鄖 鄙 鄚 鄜 鄞 鄠 鄢 鄣 鄧 鄭 鄯 鄰 鄱 鄲 鄴 鄺 酆 酈 酉 酊
酋 酌 配 酎 酐 酒 酔 酗 酚 酝 酞 酢 酣 酥 酩 酪 酬 酮 酯 酰 酱 酵 酶 酷 酸 酿 醃 醇 醉 醋 醌 醍 醐 醒 醗 醚 醛 醜 醞 醣
醤 醪 醫 醬 醮 醯 醴 醸 醺 釀 釁 采 釈 釉 释 釋 里 重 野 量 釐 金 釔 釗 釘 釜 針 釣 釤 釧 釩 釵 釷 鈀 鈉 鈍 鈎 鈐 鈑 鈔
鈕 鈞 鈣 鈦 鈮 鈰 鈴 鈷 鈸 鈹 鈺 鈽 鈾 鈿 鉀 鉄 鉅 鉈 鉉 鉋 鉍 鉏 鉑 鉗 鉚 鉛 鉞 鉢 鉤 鉦 鉬 鉭 鉱 鉴 鉸 鉻 鉾 銀 銃 銅
銑 銓 銕 銖 銘 銚 銛 銜 銣 銥 銦 銨 銫 銬 銭 銮 銳 銷 銹 銻 銼 鋁 鋅 鋆 鋇 鋌 鋏 鋐 鋒 鋤 鋪 鋭 鋯 鋰 鋲 鋳 鋸 鋹 鋼 錄
錆 錐 錒 錕 錘 錚 錠 錡 錢 錦 錨 錫 錬 錮 錯 録 錳 錶 錸 鍇 鍊 鍋 鍍 鍔 鍛 鍬 鍮 鍰 鍱 鍵 鍶 鍺 鍼 鍾 鎂 鎊 鎌 鎏 鎔 鎖
鎗 鎘 鎚 鎝 鎢 鎧 鎬 鎭 鎮 鎰 鎳 鎵 鏃 鏈 鏊 鏐 鏑 鏖 鏗 鏘 鏜 鏞 鏟 鏡 鏢 鏤 鏵 鏽 鐐 鐘 鐙 鐡 鐫 鐮 鐲 鐳 鐵 鐸 鐺 鑄
鑅 鑑 鑒 鑓 鑛 鑠 鑣 鑫 鑭 鑰 鑲 鑼 鑽 鑾 鑿 钇 针 钉 钊 钍 钏 钒 钓 钗 钙 钚 钛 钜 钝 钞 钟 钠 钡 钢 钤 钥 钦 钧 钨 钩
钫 钮 钯 钰 钱 钳 钴 钵 钹 钺 钻 钼 钽 钾 钿 铀 铁 铂 铃 铅 铆 铈 铉 铊 铋 铌 铍 铎 铐 铛 铜 铝 铟 铠 铢 铣 铤 铧 铨 铫
铬 铭 铮 铯 铰 铱 铲 铳 铵 银 铷 铸 铺 铼 链 铿 销 锁 锂 锄 锅 锆 锈 锉 锋 锌 锐 锑 锕 锗 错 锚 锜 锝 锟 锡 锢 锣 锤 锥
锦 锭 键 锯 锰 锴 锵 锶 锷 锹 锺 锻 锽 锾 镀 镁 镂 镇 镉 镌 镍 镐 镑 镒 镓 镕 镖 镗 镛 镜 镝 镠 镧 镫 镭 镯 镰 镳 镶 長
长 門 閂 閃 閆 閉 開 閎 閏 閑 閒 間 閔 閘 閡 関 閣 閤 閥 閨 閩 閬 閭 閱 閲 閹 閻 閼 閾 闆 闇 闈 闊 闋 闌 闍 闐 闓 闔 闕
闖 闘 關 闞 闡 闢 闥 门 闩 闪 闫 闭 问 闯 闰 闱 闲 闳 间 闵 闷 闸 闹 闺 闻 闼 闽 闾 闿 阀 阁 阂 阅 阆 阇 阈 阉 阎 阏 阐
阑 阔 阕 阖 阗 阙 阚 阜 阝 队 阡 阪 阮 阯 阱 防 阳 阴 阵 阶 阻 阿 陀 陂 附 际 陆 陇 陈 陉 陋 陌 降 限 陔 陕 陘 陛 陜 陝
陞 陟 陡 院 陣 除 陥 陨 险 陪 陰 陲 陳 陵 陶 陷 陸 険 陽 隅 隆 隈 隊 隋 隍 階 随 隐 隔 隕 隗 隘 隙 際 障 隠 隣 隧 隨 險
隰 隱 隴 隶 隷 隸 隹 隻 隼 隽 难 雀 雁 雄 雅 集 雇 雉 雋 雌 雍 雎 雏 雑 雒 雕 雖 雙 雛 雜 雞 雠 離 難 雨 雩 雪 雫 雯 雰
雲 雳 零 雷 雹 電 雾 需 霁 霄 霆 震 霈 霉 霊 霍 霎 霏 霑 霓 霖 霜 霞 霧 霭 霰 露 霸 霹 霽 霾 靂 靄 靈 靑 青 靓 靖 静 靚
靛 靜 非 靠 靡 面 革 靭 靱 靳 靴 靶 靺 靼 鞄 鞅 鞆 鞋 鞍 鞏 鞑 鞘 鞞 鞠 鞣 鞦 鞨 鞫 鞬 鞭 鞮 鞴 韃 韆 韋 韌 韓 韜 韞 韦
韧 韩 韫 韬 韭 韮 音 韵 韶 韻 響 頁 頂 頃 項 順 須 頊 頌 預 頑 頒 頓 頗 領 頚 頜 頠 頡 頤 頦 頫 頬 頭 頰 頴 頵 頷 頸 頹
頻 頼 頽 顆 題 額 顎 顏 顒 顓 顔 顕 顗 願 顛 類 顥 顧 顫 顯 顰 顱 顳 页 顶 顷 项 顺 须 顼 顽 顾 顿 颁 颂 预 颅 领 颇 颈
颉 颊 颌 颍 颏 颐 频 颓 颔 颖 颗 题 颙 颚 颛 颜 额 颞 颠 颢 颤 風 颯 颱 颳 颶 飄 飆 飈 风 飒 飓 飘 飙 飚 飛 飜 飞 食 飡
飢 飨 飩 飪 飫 飭 飮 飯 飲 飴 飼 飽 飾 餃 餅 餉 養 餌 餐 餓 餘 餚 餞 餡 館 餮 餵 餾 饅 饉 饋 饌 饑 饒 饕 饗 饥 饪 饬 饭
饮 饯 饰 饱 饲 饵 饶 饷 饺 饼 饿 馀 馅 馆 馈 馏 馑 馒 馔 首 馗 馘 香 馥 馨 馬 馭 馮 馱 馳 馴 駁 駄 駅 駆 駈 駐 駒 駕 駙
駛 駝 駟 駢 駭 駱 駸 駿 騁 騎 騏 騒 験 騙 騨 騫 騭 騮 騰 騶 騷 騾 驃 驅 驊 驍 驒 驕 驗 驚 驛 驟 驢 驤 驥 驩 驪 马 驭 驮
驯 驰 驱 驳 驴 驶 驷 驸 驹 驺 驻 驼 驾 驿 骁 骂 骄 骅 骆 骇 骈 骊 骋 验 骏 骐 骑 骗 骘 骚 骝 骞 骠 骡 骤 骥 骧 骨 骯 骰
骶 骷 骸 骼 髀 髂 髄 髅 髋 髏 髑 髒 髓 體 髖 高 髙 髡 髣 髦 髪 髭 髮 髯 髴 髷 髹 髻 鬃 鬆 鬍 鬓 鬘 鬚 鬟 鬢 鬣 鬥 鬧 鬨
鬪 鬬 鬯 鬱 鬲 鬻 鬼 魁 魂 魃 魄 魅 魇 魉 魋 魍 魎 魏 魑 魔 魘 魚 魟 魣 魨 魮 魯 魴 魷 鮃 鮄 鮈 鮋 鮎 鮑 鮒 鮗 鮟 鮠 鮡
鮨 鮪 鮫 鮭 鮮 鯉 鯊 鯒 鯓 鯔 鯖 鯙 鯛 鯡 鯤 鯧 鯨 鯪 鯰 鯱 鯵 鯷 鯽 鰂 鰈 鰍 鰐 鰓 鰕 鰧 鰨 鰩 鰭 鰯 鰲 鰹 鰺 鰻 鰾 鱂
鱇 鱈 鱉 鱒 鱔 鱗 鱘 鱚 鱟 鱥 鱧 鱨 鱲 鱵 鱷 鱸 鱺 鱼 鱿 鲀 鲁 鲂 鲆 鲇 鲈 鲉 鲍 鲎 鲑 鲔 鲗 鲛 鲜 鲟 鲡 鲣 鲤 鲨 鲫 鲬
鲭 鲮 鲱 鲲 鲳 鲶 鲷 鲸 鲹 鲻 鲽 鲿 鳀 鳃 鳄 鳅 鳉 鳌 鳍 鳎 鳐 鳔 鳕 鳖 鳗 鳚 鳝 鳞 鳟 鳢 鳥 鳧 鳩 鳳 鳴 鳶 鴇 鴈 鴉 鴎
鴒 鴕 鴛 鴝 鴞 鴟 鴣 鴦 鴨 鴫 鴻 鴿 鵄 鵑 鵙 鵜 鵝 鵞 鵟 鵠 鵡 鵬 鵯 鵰 鵲 鵺 鶇 鶉 鶏 鶘 鶚 鶥 鶯 鶲 鶴 鶺 鶻 鶿 鷂 鷄
鷓 鷗 鷯 鷲 鷸 鷹 鷺 鸕 鸚 鸛 鸞 鸟 鸠 鸡 鸢 鸣 鸥 鸦 鸨 鸪 鸫 鸬 鸭 鸮 鸯 鸰 鸲 鸳 鸵 鸽 鸾 鸿 鹀 鹃 鹄 鹅 鹈 鹉 鹊 鹌
鹎 鹏 鹑 鹕 鹗 鹘 鹚 鹛 鹞 鹟 鹡 鹤 鹦 鹧 鹩 鹫 鹬 鹭 鹮 鹰 鹳 鹵 鹸 鹹 鹼 鹽 鹿 麁 麂 麋 麒 麓 麗 麝 麟 麤 麥 麦 麩 麴
麵 麸 麹 麺 麻 麼 麽 麾 麿 黃 黄 黌 黍 黎 黏 黐 黑 黒 黔 默 黙 黛 黜 黝 點 黟 黠 黥 黨 黯 黴 黻 黼 鼈 鼎 鼐 鼓 鼠 鼩 鼬
鼯 鼱 鼴 鼹 鼻 齊 齋 齎 齐 齒 齟 齡 齢 齣 齦 齧 齬 齶 齿 龄 龈 龍 龐 龑 龔 龕 龙 龚 龛 龜 龟 龢 龸 ꧉ 﨑 ﭖ ﭘ ﮏ ﮐ ﮟ
ﮧ ﮨ ﮭ ﮯ ﯽ ﯾ ﯿ ﴾ ﴿ ﷲ ﷺ ︰ ﹐ ﹑ ﹒ ﹔ ﹕ ﹙ ﹚ ﹝ ﹞ ﹣ ﺍ ﺎ ﺑ ﺗ ﺟ ﺭ ﺮ ﺱ ﺳ ﻣ ﻦ ﻧ ﻭ ﻮ ！ ＂ ＃ ％
＆ （ ） ＊ ＋ ， － ． ／ ０ １ ２ ３ ４ ５ ６ ７ ８ ９ ： ； ＜ ＝ ＞ ？ ＠ ［ ＼ ］ ＿ ａ ｂ ｃ ｄ ｅ ｆ ｇ ｈ ｉ ｊ
ｋ ｌ ｍ ｎ ｏ ｐ ｒ ｓ ｔ ｖ ｗ ｘ ｚ ｛ ｜ ｝ ～ ｡ ｢ ｣ ､ ･ ｰ ￥ ￼ 𩽾 𩾌 𪨶 𫖮 𫚉 𫚒 𬀩 𬘭 𬞟 𬶋
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Number of single character tokens with hashes:&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_chars_hashes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print all of the single characters, 40 per row.&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Strip the hash marks, since they just clutter the display.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;##&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one_chars_hashes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For every batch of 40 tokens...&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Limit the end index so we don&amp;#39;t go past the end of the list.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Print out the tokens, seperated by a space.&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Number of single character tokens with hashes: 9995 

s e a n i t r o u m y l d k и а е у h g м c н z ی ы і о p ا т х ة я ه b j v f ה
к ي ו ت ς л с р י ת в ю w x ن の م د ı ر ी 2 か 1 д ा 3 ل ь 0 ם ల п न ը г 4 ł ب س
ן ो و র ר ն 5 ш 6 त ի 7 を ल ہ ে に ל ს న 9 8 र q は ə ز з य ა と ش ದ ス ন ు ᆫ ι ع ک
ف て ನ ம ے क υ б য स ق ס க ى ° ד ב া ц ರ ч క ப ᆯ ン ש ј ק ð є ك ం త ು ף ర ಯ ح ન म
ט గ ಕ ᆨ α ء ি ג ν ன ф ח ج ड ਾ ա ল ж ә ル ट ᆼ ர ि ை ல ᵉ ক ত ം ト య ο ੀ ਰ ਨ ց ע ట ী
ᆷ ത ग ი ں ી も த ң ਲ ா ط द ા ಲ ി ತ က ൽ ಗ ಾ қ స व ಸ ज თ ण ਕ א ー し ম ி ો ク ך স ർ ム
ү వ い ր च ז կ డ ய ય మ ద ص ք ض ս ই ર ß ث ო η ൻ ਤ ラ ச ਦ り տ ವ º യ گ ਸ प ٹ フ დ લ す
ह خ ई ট き る ე ץ দ ಡ ಮ ব غ ક ત ਣ イ λ ら ब ಟ コ श ਆ τ ლ ਜ ೂ ਮ ਟ ᆸ ೕ ക く ட ۰ ρ చ ナ ª
み タ സ સ ০ ० ာ ア た ए پ カ ओ մ ნ ん ಳ κ મ ᆺ ո တ မ ণ ு ড थ း ۲ ও щ ஸ э ε მ ణ գ ટ ₂ ന
গ ۳ ڈ դ జ ே ۵ ۹ ৫ צ ध শ ள ৭ ಣ ৬ ۱ န ۴ ৩ ロ ۶ ² რ ৮ ۷ ћ ਗ ং फ ҡ ө ե ५ ৪ や ણ լ ख ડ
ਹ چ ळ リ ৯ ১ ২ ۸ թ ూ ప ८ ø ६ ც ३ २ વ な െ マ μ হ စ ရ င छ ര ω १ ष জ け ४ פ ਡ ေ প ७ つ
う こ থ ९ ష ウ દ ગ ാ ठ ਵ ಪ お ذ ಷ જ ઓ ھ シ ᅡ േ ਈ သ ᅢ π വ ظ ണ ま ಂ キ չ வ ծ ഡ શ ண ノ လ վ
ტ ः オ բ れ ਪ ъ ڑ ಜ レ ₁ ᅩ ၀ ਬ ち և պ כ ൺ ढ ツ ₃ ハ せ ઈ ջ チ ੜ ষ ਚ ᅬ է ળ サ బ ҙ भ ᅵ િ ၅
զ უ ပ ယ ვ ³ ဝ ઇ ᅥ પ ચ इ φ ೦ ধ മ ಚ מ ヒ ၂ ધ ਖ ည კ え ദ ΄ ട ၈ ಥ ၁ め ᅲ ല ற љ ਧ ၄ ၆ ろ
ధ ミ ₄ એ ళ ಶ চ ᅱ ಹ ၃ × ζ બ ਥ ঠ റ ၇ ਫ ᅦ հ æ ழ ヤ ഷ ½ խ હ ീ ၉ ೖ థ ခ খ ಧ హ ಬ ျ テ ौ へ
נ ネ გ ホ ঃ さ ૦ ҳ భ ഥ ৎ घ ఫ ᅭ ケ ⁺ ᅧ ᅮ よ ೫ շ ռ ൾ ժ ছ ィ ニ ါ ਿ ʻ ᅳ エ ژ ഫ ங થ ワ ਏ ফ ひ
ヌ δ ბ శ モ ബ ξ ヶ セ θ յ χ ₀ わ ভ ಫ ജ ഴ ூ ൈ ખ β ೨ პ ղ ထ പ ェ ങ ᅨ փ ね ほ ソ ಠ ճ ւ ೧ σ ಃ
ള ശ ೮ ձ ₙ ೬ ೭ ─ झ ऊ メ ೪ ზ ャ ફ ფ そ ೯ ઠ ఖ ᆾ ァ ೩ ङ ხ ᵢ ঙ ஜ ષ ʳ ふ ֆ џ ਠ む ૧ ૭ ュ ˢ ഗ
đ ⁿ ഖ ૫ ဒ ஷ ᅫ ქ ြ ᵐ ᅣ ʰ ઝ ಭ ૪ ۃ ぬ њ ಖ উ ૨ ૩ ঘ ! &amp;#34; # $ % &amp;amp; &amp;#39; ( ) * + , - . / : ;
&amp;lt; = &amp;gt; ? @ [ \ ] ^ _ { | } ~ ¡ ¢ £ ¤ ¥ ¦ § ¨ © « ¬ ® ¯ ± µ ¶ · ¹ » ¼ ¾ ¿ ÷ þ ħ ĳ
ŉ ŋ œ ſ ƒ ǁ ɐ ɑ ɒ ɓ ɔ ɕ ɛ ɟ ɡ ɣ ɤ ɦ ɨ ɪ ɫ ɬ ɯ ɲ ɴ ɸ ɹ ɾ ʀ ʁ ʂ ʃ ʉ ʊ ʋ ʌ ʎ ʐ ʑ ʒ
ʔ ʕ ʙ ʝ ʤ ʦ ʧ ʲ ʷ ʸ ʼ ʾ ʿ ˁ ˈ ˋ ˌ ː ˙ ˚ ˡ ˣ γ ψ ϕ ђ ѕ ѣ ѧ ѫ ѳ ґ ғ җ ҫ ұ ҷ ҹ һ ӊ
ӏ ՚ ՛ ՜ ՝ ՞ օ ։ ֊ ־ ׃ ׳ ״ ، ؍ ؛ ؟ ـ ٠ ١ ٢ ٣ ٤ ٥ ٦ ٧ ٨ ٩ ٪ ٫ ٬ ٭ ٴ ڠ ڤ ڭ ۆ ۇ ۋ ێ
۔ ە ݨ अ आ उ ऋ ऍ ऎ ऐ ऑ ऒ औ ञ ऽ ॉ ॊ ॐ ॠ । ॥ ॰ ॲ অ আ ঈ ঊ ঋ এ ঐ ঔ ঝ ঞ ঢ ৈ ৗ ৰ ৱ ৷ ਅ
ਇ ਉ ਊ ਐ ਓ ਔ ਘ ਛ ਝ ਢ ਭ ਯ ੦ ੧ ੨ ੩ ੪ ੫ ੬ ੭ ੮ ੯ ੲ ੳ ઃ અ આ ઉ ઊ ઋ ઍ ઐ ઑ ઔ ઘ છ ઞ ઢ ભ ૉ
ૌ ૬ ૮ ૯ ஃ அ ஆ இ ஈ உ ஊ எ ஏ ஐ ஒ ஓ ஞ ந ஹ ெ ௗ ఁ ః అ ఆ ఇ ఈ ఉ ఊ ఋ ఎ ఏ ఐ ఒ ఓ ఔ ఘ ఙ ఛ ఝ
ఞ ఠ ఢ ఱ ృ ಅ ಆ ಇ ಈ ಉ ಊ ಋ ಎ ಏ ಐ ಒ ಓ ಔ ಘ ಛ ಝ ಞ ಢ ೃ ഃ അ ആ ഇ ഈ ഉ ഊ ഋ എ ഏ ഐ ഒ ഓ ഔ ഘ ച
ഛ ഝ ഞ ഠ ഢ ധ ഭ ഹ ൗ ൧ ൨ ൿ ක ද ප ම ය ර ල ශ ා ෙ ก ข ค ง จ ด ต ท น บ ป ภ ม ย ร ล ว ส
ห อ า ำ เ แ ไ ་ ། ཀ ཁ ག ང ཆ ད ན པ ཕ བ མ ཚ ཟ འ ར ལ ཤ ས ဂ ဃ ဆ ဇ ဈ ဉ ဋ ဌ ဍ ဏ ဓ ဖ ဗ
ဘ ဟ ဠ အ ဤ ဥ ဧ ဩ ဿ ၊ ။ ၌ ၍ ၎ ၏ ჟ ღ ყ შ ჩ ძ წ ჭ ჯ ჰ ჲ ᄀ ᄁ ᄂ ᄃ ᄄ ᄅ ᄆ ᄇ ᄈ ᄉ ᄊ ᄋ ᄌ ᄍ
ᄎ ᄏ ᄐ ᄑ ᄒ ᅤ ᅪ ᅯ ᅰ ᅴ ᆩ ᆪ ᆬ ᆭ ᆮ ᆰ ᆱ ᆲ ᆵ ᆶ ᆹ ᆻ ᆽ ᆿ ᇀ ᇁ ᇂ ᛫ ᛬ ᠠ ᠨ ᠬ ᠭ ᠰ ᡳ ᴬ ᴮ ᴰ ᴵ ᴶ
ᴷ ᴺ ᴼ ᴾ ᴿ ᵀ ᵂ ᵃ ᵇ ᵈ ᵍ ᵏ ᵒ ᵖ ᵗ ᵘ ᵛ ᵣ ᵤ ᵥ ᶜ ᶠ ᶻ ᾽ ‖ ‚ ‛ „ ‟ † ‡ • ․ ‥ ‧ ‰ ′ ″ ‹ ›
※ ‼ ‿ ⁄ ⁰ ⁱ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ⁻ ⁾ ₅ ₆ ₇ ₈ ₉ ₊ ₍ ₎ ₐ ₑ ₒ ₓ ₕ ₖ ₗ ₘ ₚ ₛ ₜ ₤ ₩ ₪ € ₱ ₴ ₹
ℂ ℃ ℓ ℕ № ℚ ℝ ™ ℤ ⅓ ⅔ ⅛ ⅰ ⅱ ⅲ ⅳ ⅴ ⅵ ⅶ ← ↑ → ↓ ↔ ↗ ↦ ↺ ⇄ ⇌ ⇒ ⇔ ⇧ ∀ ∂ ∃ ∅ ∆ ∇ ∈ ∑
− ∕ ∗ ∘ ∙ √ ∞ ∥ ∧ ∨ ∩ ∪ ∫ ∴ ∼ ≅ ≈ ≒ ≡ ≤ ≥ ≪ ≫ ⊂ ⊃ ⊆ ⊕ ⊗ ⊙ ⊞ ⊥ ⋅ ⋯ ① ② ③ ④ ⑤ ⑥ ⑦
⑧ ⓐ ━ │ ┃ ┏ ┓ └ ┗ ┛ ├ ┣ ┫ ┬ ┳ ║ ╱ █ ■ □ ▪ ▬ ▭ ▲ △ ▶ ► ▼ ▽ ◆ ◇ ◊ ○ ◌ ◎ ● ◡ ◦ ◯ ★
☆ ☉ ♀ ♂ ♕ ♖ ♗ ♘ ♙ ♠ ♡ ♣ ♥ ♦ ♪ ♭ ♯ ⚭ ✝ ⟨ ⟩ ⟪ ⟫ ⟶ ⱼ ⵏ ⺩ ⽥ 、 。 〃 々 〆 〇 〈 〉 《 》 「 」
『 』 【 】 〒 〔 〕 〖 〗 〜 〝 〞 〟 ぁ あ ぃ ぅ ぇ ぉ っ ゃ ゅ ゆ ょ ゐ ゑ ゝ ゥ ォ ッ ヘ ユ ョ ヨ ヰ ヱ ヲ ヵ ・ ㄚ
ㄧ ㄨ ㄱ ㄴ ㄷ ㄹ ㅁ ㅂ ㅅ ㅇ ㅈ ㅡ ㅣ ㆍ ㈱ ㎒ ㎞ ㎡ 㭴 㹴 䜣 䲁 䲗 䲢 一 丁 七 万 丈 三 上 下 不 与 丐 丑 专 且 丕 世
丘 丙 业 丛 东 丝 丞 丟 両 丢 两 严 並 丧 个 丫 中 丰 串 临 丶 丸 丹 为 主 丼 丽 举 乂 乃 久 么 义 之 乌 乍 乎 乏 乐 乒
乓 乔 乖 乗 乘 乙 乜 九 乞 也 习 乡 书 乩 乭 买 乱 乳 乾 亀 亂 了 予 争 事 二 于 亏 云 互 五 井 亘 亙 亚 些 亜 亞 亟 亡
亢 交 亥 亦 产 亨 亩 享 京 亭 亮 亰 亲 亳 亵 亶 人 亻 亿 什 仁 仃 仄 仅 仆 仇 今 介 仍 从 仏 仑 仓 仔 仕 他 仗 付 仙 仝
仞 仟 仡 代 令 以 仪 们 仮 仰 仲 仵 件 价 任 份 仿 企 伉 伊 伋 伍 伎 伏 伐 休 伕 众 优 伙 会 伝 伞 伟 传 伤 伦 伪 伫 伯
估 伴 伶 伷 伸 伺 似 伽 佃 但 佇 佈 佉 位 低 住 佐 佑 体 佔 何 佗 佘 余 佚 佛 作 佞 佟 你 佢 佣 佤 佥 佩 佬 佯 佰 佳 併
佶 佺 佻 佼 佾 使 侂 侃 侄 來 侈 例 侍 侏 侑 侖 侗 侘 供 依 侠 価 侣 侥 侦 侧 侨 侪 侬 侭 侮 侯 侵 侶 侷 便 俁 係 促 俄
俅 俊 俎 俏 俐 俑 俗 俘 俚 保 俞 俟 俠 信 俣 俥 俦 俨 俩 俪 俬 俭 修 俯 俱 俳 俵 俶 俸 俺 俾 倂 倆 倉 個 倍 們 倒 倓 倔
倖 倘 候 倚 倜 借 倡 倣 値 倦 倧 倩 倪 倫 倬 倭 倮 倶 倹 债 倻 值 倾 偁 偃 假 偈 偉 偏 偕 做 停 健 偰 偲 側 偵 偶 偷 偽
偿 傀 傅 傈 傉 傍 傑 傕 傘 備 傚 傢 傣 储 催 傭 傲 傳 債 傷 傻 傾 僅 僉 僊 働 像 僑 僕 僖 僚 僞 僥 僧 僭 僮 僱 僳 僵 價
僻 儀 儁 儂 億 儆 儉 儋 儒 儔 儕 儘 儚 償 儡 優 儲 儷 儸 儺 儼 儿 兀 允 元 兄 充 兆 兇 先 光 克 兌 免 兎 児 兑 兒 兔 兖
兗 党 兜 兢 入 內 全 兩 兪 八 公 六 兮 兰 共 关 兴 兵 其 具 典 兹 养 兼 兽 冀 内 円 冇 冈 冉 冊 册 再 冏 冑 冒 冕 冗 写
军 农 冠 冢 冤 冥 冨 冪 冬 冯 冰 冲 决 冴 况 冶 冷 冻 冼 冽 净 凄 准 凈 凉 凊 凋 凌 凍 减 凑 凖 凛 凜 凝 几 凡 凤 処 凧
凪 凫 凭 凯 凰 凱 凳 凶 凸 凹 出 击 凼 函 凿 刀 刁 刃 分 切 刈 刊 刍 刎 刑 划 列 刘 则 刚 创 初 删 判 別 刨 利 刪 别 刮
到 刳 制 刷 券 刹 刺 刻 刽 剁 剂 剃 則 削 剋 剌 前 剎 剑 剔 剖 剛 剝 剡 剣 剤 剥 剧 剩 剪 副 剰 剱 割 創 剷 剽 剿 劃 劇
劈 劉 劊 劍 劑 劔 力 劝 办 功 加 务 劢 劣 动 助 努 劫 劭 励 劲 劳 労 劵 効 劻 劾 势 勁 勃 勅 勇 勉 勋 勐 勒 勔 動 勖 勗
勘 務 勛 勝 勞 募 勢 勣 勤 勧 勰 勱 勲 勳 勵 勸 勺 勻 勾 勿 匀 匁 匂 包 匆 匈 匍 匏 匐 匕 化 北 匙 匝 匠 匡 匣 匪 匮 匯
匱 匹 区 医 匾 匿 區 十 千 卅 升 午 卉 半 卍 华 协 卐 卑 卒 卓 協 单 卖 南 単 博 卜 卞 卟 占 卡 卢 卤 卦 卧 卫 卬 卯 印
危 即 却 卵 卷 卸 卹 卻 卽 卿 厂 厄 厅 历 厉 压 厌 厓 厔 厕 厘 厚 厝 原 厢 厥 厦 厨 厩 厭 厮 厲 厳 去 县 叁 参 參 叄 又
叉 及 友 双 反 収 发 叔 取 受 变 叙 叛 叟 叠 叡 叢 口 古 句 另 叨 叩 只 叫 召 叭 叮 可 台 叱 史 右 叶 号 司 叹 叻 叼 吁
吃 各 合 吉 吊 吋 同 名 后 吏 吐 向 吒 吓 吕 吗 君 吝 吞 吟 吠 吡 否 吧 吨 吩 含 听 吮 启 吱 吲 吳 吴 吵 吶 吸 吹 吻 吼
吽 吾 呀 呂 呃 呆 呈 呉 告 呋 呎 呐 呑 呕 呗 员 呛 呜 呟 呢 呤 呦 周 呪 呱 味 呵 呷 呻 呼 命 咀 咁 咄 咆 咋 和 咎 咏 咐
咒 咔 咕 咖 咗 咙 咚 咤 咥 咧 咨 咩 咪 咫 咬 咭 咯 咱 咲 咳 咸 咽 咿 哀 品 哂 哄 哆 哇 哈 哉 哋 哌 响 哎 哏 哑 哒 哔 哗
哙 哚 哟 員 哥 哦 哨 哩 哪 哭 哮 哲 哺 哼 哽 唁 唄 唆 唇 唎 唐 唑 唔 唖 唛 唢 唤 唧 唬 唭 售 唯 唱 唵 唷 唸 唾 啃 啄 商
啉 啊 問 啓 啖 啜 啞 啟 啡 啤 啥 啦 啪 啬 啮 啰 啲 啶 啸 啼 啾 喀 喂 喃 善 喆 喇 喉 喊 喋 喔 喘 喙 喚 喜 喝 喧 喩 喪 喫
喬 單 喰 喱 喲 喵 営 喷 喹 喺 喻 嗅 嗆 嗇 嗎 嗓 嗔 嗚 嗜 嗝 嗟 嗡 嗣 嗤 嗨 嗩 嗪 嗯 嗲 嗶 嗽 嘅 嘆 嘈 嘉 嘌 嘎 嘏 嘔 嘗
嘘 嘛 嘜 嘟 嘢 嘧 嘩 嘯 嘱 嘲 嘴 嘶 嘹 嘻 嘿 噁 噂 噌 噏 噓 噗 噛 噜 噠 噢 噤 器 噩 噪 噫 噬 噱 噲 噴 噶 噸 噹 噺 噻 嚆
嚇 嚈 嚎 嚏 嚐 嚕 嚙 嚟 嚢 嚣 嚥 嚨 嚮 嚴 嚷 嚼 囁 囂 囃 囉 囊 囍 囑 囓 囗 囚 四 囝 回 因 囡 团 団 囤 囧 囪 园 囮 困 囱
囲 図 围 固 国 图 囿 圀 圃 圆 圈 圉 國 圍 圏 園 圓 圖 團 圜 土 圣 圧 在 圩 圪 圭 圮 圯 地 圳 圹 场 圻 圾 址 坂 均 坊 坌
坍 坎 坏 坐 坑 块 坚 坛 坜 坝 坞 坟 坠 坡 坤 坦 坨 坩 坪 坬 坭 坯 坳 坵 坷 坻 垂 垃 垄 垅 垈 型 垌 垒 垓 垕 垚 垛 垟 垠
垡 垢 垣 垦 垩 垫 垭 垮 垴 垵 垸 埂 埃 埈 埋 城 埏 埒 埔 埕 埗 埙 埜 埝 域 埠 埡 埤 埭 埴 埵 執 埸 培 基 埼 堀 堂 堃 堅
堆 堇 堉 堊 堑 堕 堝 堞 堡 堤 堪 堯 堰 報 場 堵 堺 塀 塁 塊 塋 塌 塑 塔 塗 塘 塙 塚 塞 塡 塢 塩 填 塬 塭 塱 塵 塹 塽 塾
墀 境 墅 墉 墊 墓 墕 増 墘 墙 墜 增 墟 墨 墩 墬 墮 墳 墺 墻 墾 壁 壅 壆 壇 壊 壌 壑 壓 壕 壘 壙 壞 壟 壠 壢 壤 壩 士 壬
壮 壯 声 壱 売 壳 壶 壷 壹 壺 壽 处 备 変 复 夏 夔 夕 外 夙 多 夜 够 夠 夢 夤 夥 大 天 太 夫 夭 央 夯 失 头 夷 夸 夹 夺
夼 夾 奂 奄 奇 奈 奉 奋 奎 奏 奐 契 奔 奕 奖 套 奘 奚 奠 奢 奥 奧 奨 奪 奬 奭 奮 女 奴 奶 奸 她 好 如 妃 妄 妆 妇 妈 妊
妍 妒 妓 妖 妙 妝 妞 妣 妤 妥 妨 妫 妬 妮 妲 妳 妹 妻 妾 姆 姉 姊 始 姍 姐 姑 姒 姓 委 姗 姚 姜 姝 姣 姥 姦 姨 姪 姫 姬
姵 姶 姻 姿 威 娃 娄 娅 娆 娇 娉 娑 娓 娘 娛 娜 娟 娠 娣 娥 娩 娯 娱 娲 娴 娶 娼 婁 婆 婉 婕 婚 婢 婦 婧 婪 婬 婭 婴 婵
婶 婷 婺 婿 媒 媚 媛 媞 媧 媯 媲 媳 媽 媾 嫁 嫂 嫄 嫉 嫌 嫔 嫖 嫗 嫚 嫡 嫣 嫦 嫩 嫻 嬅 嬈 嬉 嬋 嬌 嬖 嬗 嬛 嬢 嬤 嬪 嬬
嬰 嬲 嬴 嬷 嬸 嬿 孀 孃 子 孑 孔 孕 孖 字 存 孙 孚 孛 孜 孝 孟 孢 季 孤 学 孩 孪 孫 孰 孱 孳 孵 學 孺 孽 孿 宀 宁 它 宅
宇 守 安 宋 完 宍 宏 宓 宕 宗 官 宙 定 宛 宜 宝 实 実 宠 审 客 宣 室 宥 宦 宪 宫 宮 宰 害 宴 宵 家 宸 容 宽 宾 宿 寂 寄
寅 密 寇 富 寐 寒 寓 寔 寘 寛 寝 寞 察 寡 寢 寥 實 寧 寨 審 寫 寬 寮 寯 寰 寳 寵 寶 寸 对 寺 寻 导 対 寿 封 専 射 将 將
專 尉 尊 尋 對 導 小 少 尔 尕 尖 尘 尙 尚 尝 尤 尧 尪 尬 尭 就 尴 尷 尸 尹 尺 尻 尼 尽 尾 尿 局 屁 层 居 屆 屈 届 屋 屌
屍 屎 屏 屐 屑 屓 展 属 屠 屡 屢 層 履 屬 屯 山 屹 屿 岀 岁 岂 岌 岐 岑 岔 岖 岗 岘 岙 岚 岛 岡 岢 岩 岫 岬 岭 岱 岳 岷
岸 峁 峄 峇 峋 峒 峙 峠 峡 峤 峥 峦 峨 峩 峪 峭 峯 峰 峴 島 峻 峽 崁 崂 崆 崇 崋 崍 崎 崐 崑 崔 崖 崗 崙 崚 崛 崞 崢 崤
崧 崩 崭 崮 崱 崴 崽 嵇 嵊 嵋 嵌 嵐 嵘 嵙 嵜 嵩 嵬 嵯 嵴 嶂 嶄 嶇 嶋 嶌 嶗 嶙 嶝 嶠 嶧 嶴 嶷 嶸 嶺 嶼 嶽 巂 巅 巌 巍 巒
巔 巖 川 州 巡 巢 巣 工 左 巧 巨 巩 巫 差 己 已 巳 巴 巷 巻 巽 巾 巿 币 市 布 帅 帆 师 希 帐 帑 帕 帖 帘 帙 帚 帛 帜 帝
帥 带 帧 師 席 帮 帯 帰 帳 帶 帷 常 帼 帽 幀 幂 幄 幅 幇 幌 幔 幕 幗 幟 幡 幢 幣 幪 幫 干 平 年 并 幷 幸 幹 幺 幻 幼 幽
幾 广 庁 広 庄 庆 庇 床 序 庐 庑 库 应 底 庖 店 庙 庚 府 庞 废 庠 庥 度 座 庫 庭 庵 庶 康 庸 庹 庾 廁 廂 廃 廄 廆 廈 廉
廊 廍 廓 廖 廙 廚 廝 廞 廟 廠 廡 廢 廣 廨 廩 廪 廬 廳 延 廷 廸 建 廻 廼 廿 开 弁 异 弃 弄 弇 弈 弉 弊 弋 弌 式 弐 弑 弒
弓 弔 引 弖 弗 弘 弛 弟 张 弢 弥 弦 弧 弩 弭 弯 弱 張 強 弹 强 弼 弾 彀 彅 彈 彊 彌 彎 归 当 录 彗 彘 彙 彝 彡 形 彤 彥
彦 彧 彩 彪 彫 彬 彭 彰 影 彳 彷 役 彻 彼 彿 往 征 徂 径 待 徇 很 徊 律 後 徐 徑 徒 従 徕 得 徘 徙 從 徠 御 徨 復 循 徬
徭 微 徳 徴 徵 德 徹 徽 心 忄 必 忆 忉 忌 忍 忏 忒 志 忘 忙 応 忞 忠 忡 忤 忧 快 忱 念 忻 忽 忿 怀 态 怂 怎 怒 怕 怖 怙
怛 怜 思 怠 怡 急 怦 性 怨 怪 怯 总 怿 恂 恃 恆 恋 恍 恐 恒 恕 恙 恚 恢 恣 恤 恥 恨 恩 恪 恫 恬 恭 息 恰 恳 恵 恶 恸 恺
恼 恽 恿 悄 悅 悉 悌 悍 悔 悖 悚 悛 悝 悟 悠 患 悦 悧 您 悩 悪 悫 悬 悯 悰 悲 悳 悴 悶 悸 悼 悽 情 惇 惊 惋 惑 惕 惘 惚
惛 惜 惟 惠 惡 惣 惧 惨 惩 惫 惭 惮 惯 惰 惱 惲 想 惶 惹 惺 愁 愃 愆 愈 愉 愍 愎 意 愔 愕 愚 愛 感 愤 愧 愨 愫 愴 愷 愼
愾 愿 慄 慇 慈 態 慌 慎 慑 慕 慘 慚 慜 慟 慢 慣 慤 慧 慨 慫 慮 慰 慳 慵 慶 慷 慾 憂 憊 憍 憎 憐 憑 憔 憙 憚 憤 憧 憨 憩
憫 憬 憲 憶 憺 憾 懂 懃 懇 懈 應 懊 懋 懌 懐 懒 懦 懲 懵 懶 懷 懸 懺 懼 懾 懿 戀 戈 戊 戌 戍 戎 戏 成 我 戒 戔 戕 或 战
戚 戛 戟 戡 戢 戦 戩 截 戮 戯 戰 戱 戲 戳 戴 戶 户 戸 戻 戾 房 所 扁 扆 扇 扈 扉 手 扌 才 扎 扑 扒 打 扔 払 托 扛 扣 扦
执 扩 扫 扬 扭 扮 扯 扰 扱 扳 扶 批 扼 找 承 技 抄 抉 把 抑 抒 抓 投 抖 抗 折 抚 抛 抜 択 抟 抡 抢 护 报 抨 披 抬 抱 抵
抹 押 抽 拂 担 拆 拇 拈 拉 拋 拌 拍 拏 拐 拒 拓 拔 拖 拗 拘 拙 拚 招 拜 拝 拟 拠 拡 拢 拣 拥 拦 拨 择 括 拭 拮 拯 拱 拳
拴 拵 拶 拷 拼 拽 拾 拿 持 挂 指 挈 按 挑 挖 挙 挚 挛 挝 挞 挟 挠 挡 挣 挤 挥 挨 挪 挫 振 挹 挺 挽 挾 挿 捅 捆 捉 捌 捍
捎 捏 捐 捒 捕 捗 捜 捞 损 捡 换 捣 捧 捨 捩 据 捱 捲 捶 捷 捺 捻 掀 掃 授 掉 掌 掏 掐 排 掖 掘 掙 掛 掟 掠 採 探 掣 接
控 推 掩 措 掬 掰 掲 掳 掴 掷 掸 掺 掻 掾 揀 揃 揄 揆 揉 揍 描 提 插 揖 揚 換 握 揣 揪 揭 揮 援 揶 揺 揽 揾 搁 搅 損 搏
搐 搓 搔 搖 搗 搜 搞 搦 搪 搬 搭 搵 搶 携 搾 摂 摄 摆 摇 摊 摑 摒 摔 摘 摠 摧 摩 摭 摯 摶 摸 摹 摺 摻 撃 撇 撈 撐 撑 撒
撓 撕 撚 撞 撣 撤 撥 撩 撫 撬 播 撮 撰 撲 撷 撹 撻 撼 撿 擁 擂 擄 擅 擇 擊 擋 操 擎 擒 擔 擘 據 擠 擢 擦 擧 擬 擱 擲 擴
擷 擺 擾 攀 攏 攒 攔 攘 攜 攝 攢 攣 攤 攪 攫 攬 支 收 攷 攸 改 攻 放 政 故 效 敌 敍 敎 敏 救 敕 敖 敗 敘 教 敛 敝 敞 敢
散 敦 敬 数 敲 整 敵 敷 數 斂 斃 文 斉 斋 斌 斎 斐 斑 斗 料 斛 斜 斟 斡 斤 斥 斧 斩 斫 斬 断 斯 新 斷 方 於 施 旁 旃 旅
旋 旌 族 旒 旗 旛 无 既 旣 日 旦 旧 旨 早 旬 旭 旱 时 旷 旸 旺 旻 旼 昀 昂 昆 昇 昉 昊 昌 明 昏 昐 易 昔 昕 昙 昝 昞 星
映 春 昧 昨 昪 昭 是 昰 昱 昴 昵 昶 昺 昼 显 晁 時 晃 晄 晉 晋 晏 晒 晓 晔 晕 晖 晗 晙 晚 晝 晞 晟 晤 晦 晧 晨 晩 普 景
晰 晳 晴 晶 晷 晸 智 晾 暁 暂 暄 暇 暈 暉 暌 暎 暐 暑 暖 暗 暘 暝 暠 暢 暦 暧 暨 暫 暮 暱 暲 暴 暹 暻 暾 曄 曆 曇 曉 曖
曙 曜 曝 曠 曦 曩 曬 曰 曲 曳 更 曷 書 曹 曺 曼 曽 曾 替 最 會 月 有 朋 服 朐 朓 朔 朕 朗 望 朝 期 朦 朧 木 未 末 本 札
朮 术 朱 朴 朵 朶 机 朽 杀 杂 权 杆 杉 杌 李 杏 材 村 杓 杖 杙 杜 杞 束 杠 条 杢 杣 来 杨 杭 杮 杯 杰 東 杲 杳 杵 杷 杼
松 板 极 构 枇 枉 枋 析 枕 林 枚 果 枝 枞 枠 枡 枢 枣 枪 枫 枭 枯 枳 架 枷 枸 枹 柁 柃 柄 柊 柏 某 柑 柒 染 柔 柘 柚 柜
柝 柞 柠 柢 查 柩 柬 柯 柱 柳 柴 柵 査 柽 柾 柿 栂 栃 栄 栅 标 栈 栉 栋 栎 栏 树 栒 栓 栖 栗 栞 校 栢 栩 株 栱 栲 栴 样
核 根 栻 格 栽 栾 桀 桁 桂 桃 桅 框 案 桉 桌 桎 桐 桑 桓 桔 桜 桝 桟 桡 桢 档 桥 桦 桧 桨 桩 桫 桶 桷 桿 梁 梃 梅 梆 梓
梔 梗 條 梟 梠 梢 梦 梧 梨 梭 梯 械 梱 梳 梵 梶 检 棂 棄 棉 棋 棍 棒 棕 棗 棘 棚 棟 棠 棣 棧 棨 棫 森 棱 棲 棵 棹 棺 棻
椀 椁 椅 椋 植 椎 椏 椒 椙 椚 椛 検 椭 椰 椴 椹 椽 椿 楂 楊 楓 楔 楕 楙 楚 楝 楞 楠 楡 楢 楣 楨 楫 業 楮 楯 楳 極 楷 楸
楹 楼 楽 概 榄 榆 榈 榉 榊 榎 榑 榔 榕 榖 榘 榛 榜 榧 榨 榫 榭 榮 榴 榷 榻 槁 槃 槇 槊 構 槌 槍 槎 槐 槓 様 槙 槛 槟 槤
槨 槪 槭 槲 槳 槺 槻 槽 槿 樁 樂 樅 樊 樋 樑 樓 樗 標 樞 樟 模 樣 樨 権 横 樫 樱 樵 樸 樹 樺 樽 樾 橄 橇 橈 橋 橐 橘 橙
機 橡 橢 橫 橱 橹 橿 檀 檄 檉 檎 檐 檔 檗 檜 檢 檣 檬 檯 檳 檸 檻 櫂 櫃 櫓 櫚 櫛 櫟 櫥 櫨 櫸 櫻 欄 欅 欉 權 欒 欖 欞 欠
次 欢 欣 欤 欧 欲 欺 欽 款 歆 歇 歉 歌 歎 歐 歓 歙 歟 歡 止 正 此 步 武 歧 歩 歪 歯 歲 歳 歴 歷 歸 歹 死 歼 歿 殁 殃 殆
殇 殉 殊 残 殒 殓 殖 殘 殞 殡 殤 殭 殮 殯 殲 殴 段 殷 殺 殻 殼 殿 毀 毁 毂 毅 毆 毋 毌 母 毎 每 毒 毓 比 毕 毖 毗 毘 毙
毛 毡 毫 毬 毯 毽 氂 氈 氏 氐 民 氓 气 氖 気 氘 氙 氚 氛 氟 氡 氢 氣 氦 氧 氨 氩 氪 氫 氬 氮 氯 氰 水 氵 氷 永 氹 氾 汀
汁 求 汇 汉 汊 汎 汐 汕 汗 汙 汚 汛 汜 汝 汞 江 池 污 汤 汧 汨 汪 汭 汰 汲 汴 汶 汹 決 汽 汾 沁 沂 沃 沄 沅 沆 沈 沉 沌
沐 沒 沓 沔 沖 沙 沛 沟 没 沢 沣 沥 沦 沧 沪 沫 沭 沮 沱 河 沸 油 治 沼 沽 沾 沿 況 泄 泉 泊 泌 泓 法 泗 泚 泛 泞 泠 泡
波 泣 泥 注 泪 泫 泮 泯 泰 泱 泳 泵 泷 泸 泻 泼 泽 泾 洁 洄 洋 洎 洒 洗 洙 洛 洞 津 洧 洩 洪 洮 洱 洲 洵 洶 洸 洹 洺 活
洼 洽 派 流 浄 浅 浆 浇 浊 测 济 浏 浑 浒 浓 浔 浙 浚 浜 浞 浠 浣 浤 浦 浩 浪 浬 浮 浯 浴 海 浸 涂 涅 涇 消 涉 涌 涎 涓
涔 涕 涙 涛 涜 涝 涞 涟 涡 涣 涤 润 涧 涨 涩 涪 涮 涯 液 涵 涸 涼 涿 淀 淄 淅 淆 淇 淋 淌 淑 淒 淖 淘 淙 淚 淝 淞 淡 淤
淦 淨 淩 淪 淫 淬 淮 淯 深 淳 淵 淶 混 淸 淹 淺 添 淼 清 渇 済 渉 渊 渋 渍 渎 渐 渓 渔 渕 渗 渙 渚 減 渝 渟 渠 渡 渣 渤
渥 渦 温 渫 測 渭 港 渲 渴 游 渺 渾 湃 湄 湊 湍 湖 湘 湛 湜 湟 湣 湧 湫 湮 湯 湳 湾 湿 満 溃 溅 溆 溉 溏 源 準 溜 溝 溟
溢 溥 溧 溪 溫 溯 溱 溲 溴 溶 溺 溼 滁 滂 滄 滅 滇 滉 滋 滌 滎 滏 滑 滓 滔 滕 滘 滙 滚 滝 滞 满 滢 滤 滥 滦 滨 滩 滬 滯
滲 滴 滷 滸 滹 滾 滿 漁 漂 漆 漉 漏 漑 漓 演 漕 漠 漢 漣 漩 漪 漫 漬 漯 漱 漲 漳 漵 漸 漾 漿 潁 潇 潍 潑 潔 潘 潛 潜 潞
潟 潢 潤 潦 潭 潮 潯 潰 潴 潺 潼 潾 澀 澁 澂 澄 澆 澇 澈 澍 澎 澗 澜 澠 澡 澤 澧 澪 澮 澱 澳 澶 澹 激 濁 濂 濃 濉 濊 濑
濒 濕 濘 濛 濞 濟 濠 濡 濤 濫 濬 濮 濯 濰 濱 濵 濺 濾 瀅 瀆 瀉 瀋 瀏 瀑 瀕 瀘 瀚 瀛 瀝 瀞 瀟 瀧 瀨 瀬 瀰 瀾 灃 灌 灏 灑
灕 灘 灝 灞 灣 灤 火 灭 灯 灰 灵 灶 灸 灼 災 灾 灿 炀 炁 炅 炆 炉 炊 炎 炒 炔 炕 炖 炘 炙 炜 炤 炫 炬 炭 炮 炯 炱 炳 炸
点 為 炼 炽 烁 烂 烃 烈 烏 烘 烙 烛 烜 烝 烟 烤 烦 烧 烨 烩 烫 烬 热 烯 烴 烷 烹 烺 烽 焉 焊 焓 焔 焕 焗 焘 焙 焚 焜 無
焦 焯 焰 焱 然 焼 煇 煉 煊 煌 煎 煒 煕 煖 煙 煚 煜 煞 煤 煥 煦 照 煨 煩 煬 煮 煲 煽 熄 熈 熊 熏 熒 熔 熙 熟 熠 熨 熬 熱
熲 熵 熹 熾 燁 燃 燄 燈 燉 燊 燎 燐 燒 燔 燕 燗 燙 營 燥 燦 燧 燬 燭 燮 燴 燹 燻 燼 燾 燿 爀 爆 爍 爐 爛 爨 爪 爬 爭 爰
爱 爲 爵 父 爷 爸 爹 爺 爻 爽 爾 牂 牆 片 版 牌 牍 牒 牘 牙 牛 牝 牟 牠 牡 牢 牧 物 牯 牲 牴 牵 特 牺 牻 牽 犀 犁 犂 犄
犊 犍 犒 犛 犠 犢 犧 犬 犯 犰 状 犷 犸 犹 犽 狀 狂 狄 狈 狍 狎 狐 狒 狗 狙 狛 狠 狡 狨 狩 独 狭 狮 狱 狳 狷 狸 狹 狼 狽
猁 猊 猎 猕 猖 猗 猛 猜 猝 猞 猟 猥 猩 猪 猫 猬 献 猴 猶 猷 猾 猿 獁 獄 獅 獎 獏 獐 獒 獗 獠 獣 獨 獬 獭 獰 獲 獴 獵 獷
獸 獺 獻 獼 獾 玄 玆 率 玉 王 玑 玕 玖 玘 玛 玟 玠 玢 玥 玩 玫 玮 环 现 玲 玳 玷 玹 玺 玻 珀 珂 珅 珈 珉 珊 珍 珏 珐 珑
珙 珝 珞 珠 珣 珥 珦 珩 珪 班 珮 珲 珺 珽 現 球 琅 理 琇 琉 琊 琍 琏 琐 琚 琛 琢 琤 琥 琦 琨 琪 琬 琮 琯 琰 琲 琳 琴 琵
琶 琺 琼 琿 瑀 瑁 瑄 瑈 瑊 瑋 瑕 瑗 瑙 瑚 瑛 瑜 瑞 瑟 瑠 瑢 瑣 瑤 瑩 瑪 瑭 瑯 瑰 瑱 瑳 瑶 瑷 瑾 璀 璁 璃 璆 璇 璈 璉 璋
璐 璘 璜 璞 璟 璠 璣 璥 璦 璧 璨 璩 璪 環 璲 璹 璽 璿 瓊 瓌 瓏 瓒 瓔 瓘 瓚 瓛 瓜 瓠 瓢 瓣 瓦 瓩 瓮 瓯 瓶 瓷 甄 甌 甑 甕
甘 甚 甜 生 甡 產 産 甥 甦 用 甩 甫 甬 甯 田 由 甲 申 电 男 甸 町 画 甾 畀 畅 畈 畋 界 畏 畑 畔 留 畜 畝 畠 畢 畤 略 畦
番 畫 畬 畯 異 畲 畳 畴 畵 當 畷 畸 畹 畿 疃 疆 疇 疊 疋 疍 疎 疏 疑 疒 疗 疙 疚 疝 疟 疡 疣 疤 疥 疫 疮 疯 疱 疲 疵 疸
疹 疼 疽 疾 痂 病 症 痉 痊 痍 痒 痔 痕 痘 痙 痛 痞 痢 痣 痤 痩 痪 痫 痰 痲 痴 痹 痺 瘀 瘁 瘋 瘍 瘓 瘙 瘟 瘠 瘡 瘢 瘤 瘦
瘧 瘩 瘫 瘴 瘾 療 癇 癌 癒 癖 癡 癢 癣 癥 癩 癪 癫 癬 癮 癱 癲 癸 発 登 發 白 百 皂 的 皆 皇 皈 皋 皎 皐 皓 皖 皙 皝 皮
皰 皱 皺 皿 盂 盃 盅 盆 盈 益 盎 盏 盐 监 盒 盔 盖 盗 盘 盛 盜 盞 盟 盡 監 盤 盥 盧 盩 盪 目 盯 盱 盲 直 相 盼 盾 省 眈
眉 看 県 眙 眞 真 眠 眨 眩 眭 眶 眷 眸 眺 眼 眾 着 睁 睇 睐 睑 睛 睜 睞 睡 睢 督 睦 睨 睪 睫 睬 睹 睺 睽 睾 睿 瞄 瞋 瞌
瞎 瞑 瞒 瞞 瞥 瞧 瞩 瞪 瞬 瞭 瞰 瞳 瞻 瞼 瞽 瞿 矇 矍 矗 矚 矛 矜 矢 矣 知 矧 矩 矫 短 矮 矯 石 矶 矽 矾 矿 砀 码 砂 砌
砍 砒 研 砕 砖 砚 砜 砥 砦 砧 砬 砭 砰 砲 破 砵 砷 砸 砺 砾 砿 础 硃 硅 硏 硐 硒 硕 硖 硝 硤 硫 硬 确 硯 硼 碁 碇 碉 碌
碍 碎 碑 碓 碕 碗 碘 碚 碛 碟 碣 碧 碩 碭 碰 碱 碲 碳 碴 碶 碸 確 碼 碾 磁 磅 磊 磋 磐 磔 磕 磘 磚 磡 磧 磨 磬 磯 磴 磷
磺 磻 磾 礁 礎 礒 礙 礦 礪 礫 礬 礴 示 礼 礽 社 祀 祁 祂 祆 祇 祈 祉 祎 祏 祐 祓 祔 祕 祖 祗 祚 祛 祜 祝 神 祟 祠 祢 祥
票 祭 祯 祷 祸 祺 祿 禀 禁 禄 禅 禊 禍 禎 福 禑 禔 禕 禛 禦 禧 禩 禪 禮 禰 禱 禹 禺 离 禽 禾 禿 秀 私 秃 秆 秉 秋 种 科
秒 秘 租 秣 秤 秦 秧 秩 秭 积 称 秸 移 秽 稀 稃 稅 稈 程 稍 税 稔 稗 稙 稚 稜 稟 稠 稣 種 稱 稲 稳 稷 稹 稻 稼 稽 稿 穀
穂 穆 穌 積 穎 穏 穐 穗 穢 穣 穩 穫 穰 穴 究 穷 穹 空 穿 突 窃 窄 窈 窍 窑 窒 窓 窕 窖 窗 窘 窜 窝 窟 窠 窣 窥 窦 窩 窪
窮 窯 窺 窿 竄 竅 竇 竈 竊 立 竑 竖 站 竜 竝 竞 竟 章 竣 童 竦 竪 竭 端 競 竹 竺 竿 笃 笄 笆 笈 笋 笏 笑 笔 笕 笙 笛 笞
笠 笥 符 笨 笪 第 笮 笳 笹 笺 笼 筅 筆 筈 等 筊 筋 筌 筍 筏 筐 筑 筒 答 策 筛 筝 筠 筥 筧 筮 筰 筱 筲 筵 筷 筹 筺 签 简
箇 箋 箍 箏 箐 箒 箓 箔 箕 算 箚 箝 管 箪 箫 箬 箭 箱 箴 箸 節 篁 範 篆 篇 築 篋 篙 篝 篠 篡 篤 篦 篩 篪 篭 篮 篱 篷 篾
簀 簃 簇 簋 簑 簒 簕 簗 簡 簧 簪 簫 簷 簸 簽 簾 簿 籀 籁 籃 籌 籍 籐 籓 籔 籙 籟 籠 籤 籬 籲 米 类 籽 籾 粁 粂 粄 粉 粋
粍 粑 粒 粕 粗 粘 粛 粟 粤 粥 粧 粪 粮 粱 粲 粳 粵 粹 粽 精 粿 糀 糅 糊 糍 糎 糕 糖 糗 糙 糜 糞 糟 糠 糧 糬 糯 糰 糸 糹
糺 系 糾 紀 紂 約 紅 紆 紇 紉 紊 紋 納 紐 紓 純 紗 紘 紙 級 紛 紜 素 紡 索 紧 紫 紬 紮 累 細 紳 紹 紺 終 絃 組 絅 絆 経
結 絕 絜 絞 絡 絢 絣 給 絨 絮 統 絲 絳 絵 絶 絹 綁 綃 綉 綏 綑 經 継 続 綜 綝 綠 綢 綦 綫 綬 維 綰 綱 網 綴 綵 綸 綺 綻
綽 綾 綿 緊 緋 総 緑 緒 緖 緘 線 緝 緞 締 緡 緣 編 緩 緬 緯 緲 練 緹 緻 縁 縄 縈 縉 縊 縛 縝 縞 縣 縦 縫 縮 縯 縱 縷 縹
縻 總 績 繁 繃 繆 繇 繊 繋 繍 繒 織 繕 繙 繚 繞 繡 繩 繪 繫 繭 繰 繳 繹 繼 繽 纂 纈 續 纍 纏 纓 纖 纘 纛 纜 纠 红 纣 纤
纥 约 级 纪 纫 纬 纭 纮 纯 纱 纲 纳 纵 纶 纷 纸 纹 纺 纽 纾 线 绀 绂 练 组 绅 细 织 终 绊 绍 绎 经 绑 绒 结 绕 绘 给 绚
绛 络 绝 绞 统 绡 绢 绣 绥 继 绩 绪 绫 续 绮 绯 绰 绳 维 绵 绶 绷 绸 综 绽 绾 绿 缀 缄 缅 缆 缇 缉 缎 缓 缔 缕 编 缘 缙
缚 缜 缝 缟 缠 缢 缤 缨 缩 缪 缬 缭 缮 缯 缴 缵 缶 缸 缺 缽 罂 罄 罅 罈 罌 罐 网 罔 罕 罗 罘 罚 罟 罠 罡 罢 罩 罪 置 罰
署 罵 罷 罹 罽 羁 羅 羆 羈 羊 羋 羌 美 羔 羚 羞 羟 羡 羣 群 羥 羧 羨 義 羯 羰 羲 羸 羹 羽 羿 翀 翁 翅 翊 翌 翎 習 翔 翕
翘 翟 翠 翡 翥 翦 翩 翫 翮 翰 翱 翳 翹 翻 翼 耀 老 考 者 耆 而 耍 耐 耒 耕 耗 耘 耙 耜 耦 耨 耳 耶 耸 耻 耽 耿 聂 聃 聆
聊 聋 职 联 聖 聘 聚 聞 聡 聪 聯 聰 聲 聳 聴 聶 職 聽 聾 聿 肃 肄 肅 肆 肇 肉 肋 肌 肓 肖 肘 肚 肛 肜 肝 肟 肠 股 肢 肤
肥 肩 肪 肮 肯 肱 育 肴 肺 肼 肽 肾 肿 胀 胁 胃 胄 胆 背 胍 胎 胖 胚 胛 胜 胝 胞 胡 胤 胥 胧 胪 胫 胭 胯 胰 胱 胳 胴 胶
胸 胺 胼 能 脂 脅 脆 脇 脈 脉 脊 脍 脏 脐 脑 脓 脖 脚 脛 脣 脩 脫 脯 脱 脲 脳 脷 脸 脹 脾 腆 腈 腊 腋 腌 腎 腐 腑 腓 腔
腕 腥 腦 腩 腫 腭 腮 腰 腱 腳 腴 腸 腹 腺 腻 腾 腿 膀 膂 膈 膊 膏 膑 膚 膛 膜 膝 膠 膣 膦 膨 膩 膳 膵 膺 膽 膾 膿 臀 臂
臆 臈 臉 臍 臏 臓 臘 臚 臟 臣 臥 臧 臨 自 臬 臭 至 致 臺 臻 臼 臾 舁 舂 舅 舆 與 興 舉 舊 舌 舍 舎 舐 舒 舔 舖 舗 舘 舛
舜 舞 舟 舢 舩 航 舫 般 舯 舰 舱 舳 舵 舶 舷 舸 船 舺 舾 艀 艇 艉 艋 艏 艘 艙 艛 艤 艦 艮 良 艰 艱 色 艳 艶 艷 艸 艹 艺
艾 节 芃 芈 芊 芋 芍 芎 芒 芗 芙 芜 芝 芡 芥 芦 芨 芩 芪 芫 芬 芭 芮 芯 花 芳 芷 芸 芹 芻 芽 芾 苄 苅 苇 苋 苌 苍 苎 苏
苑 苓 苔 苕 苗 苛 苜 苞 苟 苡 苣 若 苦 苧 苫 苯 英 苳 苴 苷 苹 苺 苻 苾 茁 茂 范 茄 茅 茉 茌 茎 茔 茗 茛 茜 茧 茨 茫 茯
茱 茲 茴 茵 茶 茸 茹 荀 荃 荆 草 荊 荏 荐 荒 荔 荖 荘 荚 荞 荟 荠 荡 荣 荤 荥 荧 荨 荩 荪 荫 药 荳 荷 荸 荻 荼 荽 莅 莆
莉 莊 莎 莒 莓 莖 莘 莞 莠 莢 莧 莨 莪 莫 莱 莲 莳 莴 获 莹 莺 莽 莿 菀 菁 菅 菇 菈 菊 菌 菏 菓 菖 菘 菜 菝 菟 菠 菡 菩
菫 華 菰 菱 菲 菴 菸 菽 萁 萃 萄 萇 萊 萌 萍 萎 萘 萜 萝 萠 萣 萤 营 萦 萧 萨 萩 萬 萱 萵 萸 萼 落 葆 葉 葎 著 葛 葜 葡
董 葦 葩 葫 葬 葭 葯 葱 葳 葵 葶 葷 葺 蒂 蒋 蒐 蒔 蒙 蒜 蒞 蒟 蒡 蒨 蒯 蒲 蒴 蒸 蒺 蒻 蒼 蒽 蒾 蒿 蓀 蓁 蓄 蓆 蓉 蓋 蓍
蓑 蓓 蓖 蓝 蓟 蓣 蓬 蓮 蓼 蓿 蔀 蔑 蔓 蔔 蔗 蔘 蔚 蔡 蔣 蔥 蔦 蔬 蔭 蔴 蔵 蔷 蔺 蔻 蔼 蔽 蕁 蕃 蕈 蕉 蕊 蕎 蕗 蕙 蕤 蕨
蕩 蕪 蕭 蕲 蕴 蕷 蕾 薀 薄 薇 薈 薊 薌 薏 薑 薔 薗 薙 薛 薜 薦 薨 薩 薪 薫 薬 薮 薯 薰 薷 薹 薺 藁 藉 藍 藎 藏 藐 藓 藔
藕 藜 藝 藤 藥 藨 藩 藪 藷 藹 藺 藻 藿 蘂 蘄 蘅 蘆 蘇 蘊 蘋 蘑 蘚 蘭 蘸 蘿 虎 虏 虐 虑 虓 虔 處 虚 虛 虜 虞 號 虢 虧 虫
虬 虯 虱 虹 虻 虽 虾 蚀 蚁 蚂 蚊 蚋 蚌 蚓 蚕 蚜 蚝 蚣 蚤 蚩 蚪 蚬 蚯 蚱 蚵 蚶 蚺 蛀 蛄 蛆 蛇 蛉 蛊 蛋 蛍 蛎 蛏 蛔 蛙 蛛
蛞 蛟 蛤 蛭 蛮 蛯 蛰 蛱 蛳 蛸 蛹 蛺 蛻 蛾 蜀 蜂 蜃 蜆 蜈 蜉 蜊 蜍 蜑 蜒 蜓 蜕 蜗 蜘 蜚 蜜 蜡 蜢 蜥 蜱 蜴 蜷 蜻 蜿 蝇 蝉
蝋 蝌 蝎 蝓 蝕 蝗 蝙 蝟 蝠 蝣 蝦 蝨 蝮 蝰 蝴 蝶 蝸 蝽 蝾 蝿 螂 螃 螄 螈 融 螞 螟 螢 螨 螫 螭 螯 螳 螺 螽 蟀 蟄 蟆 蟇 蟋
蟎 蟑 蟒 蟜 蟠 蟬 蟲 蟷 蟹 蟻 蟾 蠅 蠊 蠍 蠑 蠔 蠕 蠟 蠡 蠢 蠣 蠱 蠲 蠵 蠶 蠹 蠻 血 衅 衆 衊 行 衍 衒 術 衔 衕 街 衙 衚
衛 衝 衞 衡 衢 衣 补 表 衫 衬 衮 衰 衷 衹 衽 衾 衿 袁 袂 袄 袈 袋 袍 袒 袖 袛 袜 袞 袢 袤 被 袭 袱 袴 裁 裂 装 裏 裒 裔
裕 裘 裙 補 裝 裟 裡 裤 裨 裱 裳 裴 裵 裸 裹 製 裾 褂 複 褌 褐 褒 褓 褔 褚 褥 褪 褫 褲 褶 褻 襁 襄 襖 襞 襟 襦 襪 襯 襲
襷 西 要 覃 覆 覇 見 規 覓 視 覗 覚 覦 覧 親 覬 覲 観 覺 覽 觀 见 观 规 觅 视 览 觉 觊 觎 觐 角 觚 觞 解 触 觴 觸 言 訁
訂 訃 訇 計 訊 訌 討 訓 訕 訖 託 記 訛 訝 訟 訢 訣 訥 訪 設 許 訳 訴 訶 診 註 証 訾 詁 詆 詈 詐 詒 詔 評 詛 詞 詠 詡 詢
詣 試 詧 詩 詫 詬 詭 詮 詰 話 該 詳 詵 詹 詼 誅 誇 誉 誊 誌 認 誑 誓 誕 誘 語 誠 誡 誣 誤 誥 誦 誨 說 説 読 誰 課 誹 誼
誾 調 諂 諄 談 請 諌 諍 諏 諒 論 諛 諜 諡 諤 諦 諧 諫 諭 諮 諱 諲 諳 諴 諶 諷 諸 諺 諾 謀 謁 謂 謄 謇 謊 謎 謐 謔 謖 謗
謙 謚 講 謝 謠 謡 謨 謫 謬 謳 謹 謾 譁 證 譏 識 譙 譚 譜 警 譬 譯 議 譲 譴 護 譽 讀 讃 變 讎 讐 讒 讓 讖 讚 讞 计 订 讣
认 讥 讦 讧 讨 让 讪 讫 训 议 讯 记 讲 讳 讴 讶 讷 许 讹 论 讼 讽 设 访 诀 证 诂 诃 评 诅 识 诈 诉 诊 诋 词 诏 译 诒 试
诗 诘 诙 诚 诛 诜 话 诞 诟 诠 诡 询 诣 诤 该 详 诧 诩 诫 诬 语 误 诰 诱 诲 说 诵 请 诸 诹 诺 读 诽 课 谀 谁 调 谄 谅 谈
谊 谋 谌 谍 谎 谏 谐 谑 谒 谓 谔 谕 谗 谘 谙 谚 谛 谜 谟 谢 谣 谤 谥 谦 谧 谨 谩 谪 谬 谭 谯 谱 谳 谴 谶 谷 谿 豁 豆 豈
豉 豊 豌 豎 豐 豔 豕 豚 象 豢 豨 豪 豫 豬 豳 豸 豹 豺 貂 貉 貊 貌 貓 貘 貝 貞 負 財 貢 貧 貨 販 貪 貫 責 貯 貰 貳 貴 貶
買 貸 費 貼 貽 貿 賀 賁 賂 賃 賄 資 賈 賊 賎 賑 賓 賚 賛 賜 賞 賠 賡 賢 賣 賤 賦 質 賬 賭 賴 賺 購 賽 賾 贄 贅 贇 贈 贊
贋 贍 贏 贓 贔 贖 贛 贝 贞 负 贡 财 责 贤 败 账 货 质 贩 贪 贫 贬 购 贮 贯 贰 贱 贲 贴 贵 贷 贸 费 贺 贻 贼 贽 贾 贿 赀
赁 赂 赃 资 赈 赉 赋 赌 赍 赎 赏 赐 赓 赔 赖 赘 赚 赛 赝 赞 赟 赠 赡 赢 赣 赤 赦 赧 赫 赭 走 赳 赴 赵 赶 起 趁 超 越 趋
趕 趙 趟 趣 趨 足 趴 趸 趺 趾 跃 跆 跋 跌 跏 跑 跖 跗 跛 距 跟 跡 跤 跨 跪 路 跳 践 跷 跸 跻 踊 踏 踐 踝 踞 踢 踩 踪 踰
踴 踵 踹 蹂 蹄 蹇 蹈 蹉 蹊 蹋 蹕 蹙 蹟 蹠 蹤 蹦 蹬 蹲 蹴 蹶 蹺 蹼 躁 躄 躅 躇 躉 躊 躋 躍 躏 躑 躓 躙 躪 身 躬 躯 躰 躲
躺 躾 軀 車 軋 軌 軍 軒 軔 軛 軟 転 軫 軸 軻 軼 軽 軾 較 輅 載 輋 輌 輒 輓 輔 輕 輛 輜 輝 輟 輥 輦 輩 輪 輯 輳 輸 輻 輾
輿 轂 轄 轅 轆 轉 轍 轎 轟 轡 轢 轤 车 轧 轨 轩 轫 转 轭 轮 软 轰 轲 轴 轶 轸 轻 轼 载 轿 辂 较 辄 辅 辆 辇 辈 辉 辊 辍
辎 辐 辑 输 辕 辖 辗 辘 辙 辛 辜 辞 辟 辣 辦 辨 辩 辫 辭 辮 辯 辰 辱 農 辶 边 辺 辻 込 辽 达 辿 迁 迂 迄 迅 过 迈 迎 运
近 返 还 这 进 远 违 连 迟 迢 迤 迥 迦 迨 迩 迪 迫 迭 述 迳 迴 迷 迸 迹 迺 追 退 送 适 逃 逄 逅 逆 选 逊 逋 逍 透 逐 逑
递 逓 途 逕 逖 逗 這 通 逛 逝 逞 速 造 逡 逢 連 逮 逯 週 進 逵 逸 逹 逻 逼 逾 遁 遂 遅 遇 遊 運 遍 過 遏 遐 遑 遒 道 達
違 遗 遘 遙 遜 遞 遠 遡 遣 遥 遨 適 遭 遮 遲 遴 遵 遶 遷 選 遹 遺 遼 遽 避 邀 邁 邂 邃 還 邇 邈 邉 邊 邏 邑 邓 邕 邗 邙
邛 邝 邠 邡 邢 那 邦 邨 邪 邬 邮 邯 邰 邱 邳 邴 邵 邸 邹 邺 邻 邽 邾 郁 郃 郅 郇 郊 郎 郑 郓 郕 郗 郛 郜 郝 郞 郡 郢 郤
郦 郧 部 郫 郭 郯 郴 郵 郷 郸 都 郾 郿 鄂 鄄 鄆 鄉 鄒 鄔 鄕 鄖 鄙 鄚 鄜 鄞 鄠 鄢 鄣 鄧 鄭 鄯 鄰 鄱 鄲 鄴 鄺 酆 酈 酉 酊
酋 酌 配 酎 酐 酒 酔 酗 酚 酝 酞 酢 酣 酥 酩 酪 酬 酮 酯 酰 酱 酵 酶 酷 酸 酿 醃 醇 醉 醋 醌 醍 醐 醒 醗 醚 醛 醜 醞 醣
醤 醪 醫 醬 醮 醯 醴 醸 醺 釀 釁 采 釈 釉 释 釋 里 重 野 量 釐 金 釔 釗 釘 釜 針 釣 釤 釧 釩 釵 釷 鈀 鈉 鈍 鈎 鈐 鈑 鈔
鈕 鈞 鈣 鈦 鈮 鈰 鈴 鈷 鈸 鈹 鈺 鈽 鈾 鈿 鉀 鉄 鉅 鉈 鉉 鉋 鉍 鉏 鉑 鉗 鉚 鉛 鉞 鉢 鉤 鉦 鉬 鉭 鉱 鉴 鉸 鉻 鉾 銀 銃 銅
銑 銓 銕 銖 銘 銚 銛 銜 銣 銥 銦 銨 銫 銬 銭 銮 銳 銷 銹 銻 銼 鋁 鋅 鋆 鋇 鋌 鋏 鋐 鋒 鋤 鋪 鋭 鋯 鋰 鋲 鋳 鋸 鋹 鋼 錄
錆 錐 錒 錕 錘 錚 錠 錡 錢 錦 錨 錫 錬 錮 錯 録 錳 錶 錸 鍇 鍊 鍋 鍍 鍔 鍛 鍬 鍮 鍰 鍱 鍵 鍶 鍺 鍼 鍾 鎂 鎊 鎌 鎏 鎔 鎖
鎗 鎘 鎚 鎝 鎢 鎧 鎬 鎭 鎮 鎰 鎳 鎵 鏃 鏈 鏊 鏐 鏑 鏖 鏗 鏘 鏜 鏞 鏟 鏡 鏢 鏤 鏵 鏽 鐐 鐘 鐙 鐡 鐫 鐮 鐲 鐳 鐵 鐸 鐺 鑄
鑅 鑑 鑒 鑓 鑛 鑠 鑣 鑫 鑭 鑰 鑲 鑼 鑽 鑾 鑿 钇 针 钉 钊 钍 钏 钒 钓 钗 钙 钚 钛 钜 钝 钞 钟 钠 钡 钢 钤 钥 钦 钧 钨 钩
钫 钮 钯 钰 钱 钳 钴 钵 钹 钺 钻 钼 钽 钾 钿 铀 铁 铂 铃 铅 铆 铈 铉 铊 铋 铌 铍 铎 铐 铛 铜 铝 铟 铠 铢 铣 铤 铧 铨 铫
铬 铭 铮 铯 铰 铱 铲 铳 铵 银 铷 铸 铺 铼 链 铿 销 锁 锂 锄 锅 锆 锈 锉 锋 锌 锐 锑 锕 锗 错 锚 锜 锝 锟 锡 锢 锣 锤 锥
锦 锭 键 锯 锰 锴 锵 锶 锷 锹 锺 锻 锽 锾 镀 镁 镂 镇 镉 镌 镍 镐 镑 镒 镓 镕 镖 镗 镛 镜 镝 镠 镧 镫 镭 镯 镰 镳 镶 長
长 門 閂 閃 閆 閉 開 閎 閏 閑 閒 間 閔 閘 閡 関 閣 閤 閥 閨 閩 閬 閭 閱 閲 閹 閻 閼 閾 闆 闇 闈 闊 闋 闌 闍 闐 闓 闔 闕
闖 闘 關 闞 闡 闢 闥 门 闩 闪 闫 闭 问 闯 闰 闱 闲 闳 间 闵 闷 闸 闹 闺 闻 闼 闽 闾 闿 阀 阁 阂 阅 阆 阇 阈 阉 阎 阏 阐
阑 阔 阕 阖 阗 阙 阚 阜 阝 队 阡 阪 阮 阯 阱 防 阳 阴 阵 阶 阻 阿 陀 陂 附 际 陆 陇 陈 陉 陋 陌 降 限 陔 陕 陘 陛 陜 陝
陞 陟 陡 院 陣 除 陥 陨 险 陪 陰 陲 陳 陵 陶 陷 陸 険 陽 隅 隆 隈 隊 隋 隍 階 随 隐 隔 隕 隗 隘 隙 際 障 隠 隣 隧 隨 險
隰 隱 隴 隶 隷 隸 隹 隻 隼 隽 难 雀 雁 雄 雅 集 雇 雉 雋 雌 雍 雎 雏 雑 雒 雕 雖 雙 雛 雜 雞 雠 離 難 雨 雩 雪 雫 雯 雰
雲 雳 零 雷 雹 電 雾 需 霁 霄 霆 震 霈 霉 霊 霍 霎 霏 霑 霓 霖 霜 霞 霧 霭 霰 露 霸 霹 霽 霾 靂 靄 靈 靑 青 靓 靖 静 靚
靛 靜 非 靠 靡 面 革 靭 靱 靳 靴 靶 靺 靼 鞄 鞅 鞆 鞋 鞍 鞏 鞑 鞘 鞞 鞠 鞣 鞦 鞨 鞫 鞬 鞭 鞮 鞴 韃 韆 韋 韌 韓 韜 韞 韦
韧 韩 韫 韬 韭 韮 音 韵 韶 韻 響 頁 頂 頃 項 順 須 頊 頌 預 頑 頒 頓 頗 領 頚 頜 頠 頡 頤 頦 頫 頬 頭 頰 頴 頵 頷 頸 頹
頻 頼 頽 顆 題 額 顎 顏 顒 顓 顔 顕 顗 願 顛 類 顥 顧 顫 顯 顰 顱 顳 页 顶 顷 项 顺 须 顼 顽 顾 顿 颁 颂 预 颅 领 颇 颈
颉 颊 颌 颍 颏 颐 频 颓 颔 颖 颗 题 颙 颚 颛 颜 额 颞 颠 颢 颤 風 颯 颱 颳 颶 飄 飆 飈 风 飒 飓 飘 飙 飚 飛 飜 飞 食 飡
飢 飨 飩 飪 飫 飭 飮 飯 飲 飴 飼 飽 飾 餃 餅 餉 養 餌 餐 餓 餘 餚 餞 餡 館 餮 餵 餾 饅 饉 饋 饌 饑 饒 饕 饗 饥 饪 饬 饭
饮 饯 饰 饱 饲 饵 饶 饷 饺 饼 饿 馀 馅 馆 馈 馏 馑 馒 馔 首 馗 馘 香 馥 馨 馬 馭 馮 馱 馳 馴 駁 駄 駅 駆 駈 駐 駒 駕 駙
駛 駝 駟 駢 駭 駱 駸 駿 騁 騎 騏 騒 験 騙 騨 騫 騭 騮 騰 騶 騷 騾 驃 驅 驊 驍 驒 驕 驗 驚 驛 驟 驢 驤 驥 驩 驪 马 驭 驮
驯 驰 驱 驳 驴 驶 驷 驸 驹 驺 驻 驼 驾 驿 骁 骂 骄 骅 骆 骇 骈 骊 骋 验 骏 骐 骑 骗 骘 骚 骝 骞 骠 骡 骤 骥 骧 骨 骯 骰
骶 骷 骸 骼 髀 髂 髄 髅 髋 髏 髑 髒 髓 體 髖 高 髙 髡 髣 髦 髪 髭 髮 髯 髴 髷 髹 髻 鬃 鬆 鬍 鬓 鬘 鬚 鬟 鬢 鬣 鬥 鬧 鬨
鬪 鬬 鬯 鬱 鬲 鬻 鬼 魁 魂 魃 魄 魅 魇 魉 魋 魍 魎 魏 魑 魔 魘 魚 魟 魣 魨 魮 魯 魴 魷 鮃 鮄 鮈 鮋 鮎 鮑 鮒 鮗 鮟 鮠 鮡
鮨 鮪 鮫 鮭 鮮 鯉 鯊 鯒 鯓 鯔 鯖 鯙 鯛 鯡 鯤 鯧 鯨 鯪 鯰 鯱 鯵 鯷 鯽 鰂 鰈 鰍 鰐 鰓 鰕 鰧 鰨 鰩 鰭 鰯 鰲 鰹 鰺 鰻 鰾 鱂
鱇 鱈 鱉 鱒 鱔 鱗 鱘 鱚 鱟 鱥 鱧 鱨 鱲 鱵 鱷 鱸 鱺 鱼 鱿 鲀 鲁 鲂 鲆 鲇 鲈 鲉 鲍 鲎 鲑 鲔 鲗 鲛 鲜 鲟 鲡 鲣 鲤 鲨 鲫 鲬
鲭 鲮 鲱 鲲 鲳 鲶 鲷 鲸 鲹 鲻 鲽 鲿 鳀 鳃 鳄 鳅 鳉 鳌 鳍 鳎 鳐 鳔 鳕 鳖 鳗 鳚 鳝 鳞 鳟 鳢 鳥 鳧 鳩 鳳 鳴 鳶 鴇 鴈 鴉 鴎
鴒 鴕 鴛 鴝 鴞 鴟 鴣 鴦 鴨 鴫 鴻 鴿 鵄 鵑 鵙 鵜 鵝 鵞 鵟 鵠 鵡 鵬 鵯 鵰 鵲 鵺 鶇 鶉 鶏 鶘 鶚 鶥 鶯 鶲 鶴 鶺 鶻 鶿 鷂 鷄
鷓 鷗 鷯 鷲 鷸 鷹 鷺 鸕 鸚 鸛 鸞 鸟 鸠 鸡 鸢 鸣 鸥 鸦 鸨 鸪 鸫 鸬 鸭 鸮 鸯 鸰 鸲 鸳 鸵 鸽 鸾 鸿 鹀 鹃 鹄 鹅 鹈 鹉 鹊 鹌
鹎 鹏 鹑 鹕 鹗 鹘 鹚 鹛 鹞 鹟 鹡 鹤 鹦 鹧 鹩 鹫 鹬 鹭 鹮 鹰 鹳 鹵 鹸 鹹 鹼 鹽 鹿 麁 麂 麋 麒 麓 麗 麝 麟 麤 麥 麦 麩 麴
麵 麸 麹 麺 麻 麼 麽 麾 麿 黃 黄 黌 黍 黎 黏 黐 黑 黒 黔 默 黙 黛 黜 黝 點 黟 黠 黥 黨 黯 黴 黻 黼 鼈 鼎 鼐 鼓 鼠 鼩 鼬
鼯 鼱 鼴 鼹 鼻 齊 齋 齎 齐 齒 齟 齡 齢 齣 齦 齧 齬 齶 齿 龄 龈 龍 龐 龑 龔 龕 龙 龚 龛 龜 龟 龢 龸 ꧉ 﨑 ﭖ ﭘ ﮏ ﮐ ﮟ
ﮧ ﮨ ﮭ ﮯ ﯽ ﯾ ﯿ ﴾ ﴿ ﷲ ﷺ ︰ ﹐ ﹑ ﹒ ﹔ ﹕ ﹙ ﹚ ﹝ ﹞ ﹣ ﺍ ﺎ ﺑ ﺗ ﺟ ﺭ ﺮ ﺱ ﺳ ﻣ ﻦ ﻧ ﻭ ﻮ ！ ＂ ＃ ％
＆ （ ） ＊ ＋ ， － ． ／ ０ １ ２ ３ ４ ５ ６ ７ ８ ９ ： ； ＜ ＝ ＞ ？ ＠ ［ ＼ ］ ＿ ａ ｂ ｃ ｄ ｅ ｆ ｇ ｈ ｉ ｊ
ｋ ｌ ｍ ｎ ｏ ｐ ｒ ｓ ｔ ｖ ｗ ｘ ｚ ｛ ｜ ｝ ～ ｡ ｢ ｣ ､ ･ ｰ ￥ ￼ 𩽾 𩾌 𪨶 𫖮 𫚉 𫚒 𬀩 𬘭 𬞟 𬶋
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Are the two sets identical?&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_chars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Are the two sets identical? True
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Subwords-vs.-Whole-words&quot;&gt;Subwords vs. Whole-words&lt;a class=&quot;anchor-link&quot; href=&quot;#Subwords-vs.-Whole-words&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Let's gather some statistics on the vocabulary.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; pip install seaborn -q
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sns&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;darkgrid&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Increase the plot size and font size.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;font_scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;figure.figsize&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Measure the length of every token in the vocab.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;token_lengths&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plot the number of tokens of each length.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;countplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_lengths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Vocab Token Lengths&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Token Length&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;# of Tokens&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Maximum token length:&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_lengths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Maximum token length: 22
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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
&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let's look at just the tokens which begin with '##'.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_subwords&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;subword_lengths&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For each token in the vocabulary,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# If it&amp;#39;s a subword,&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;##&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Tally all wubwords&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_subwords&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Measure the subword length (without the hashes)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Record the lengths.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;subword_lengths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;How many '##' tokens are there vs. the full vocab?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Number of subwords: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; of &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_subwords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Calculate the percentage of words that are &amp;#39;##&amp;#39; subwords.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prcnt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_subwords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;100.0&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.1f%%&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prcnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Number of subwords: 36,436 of 105,879
34.4%
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Plot the subword lengths (not including the two '##' characters).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;countplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subword_lengths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Subword Token Lengths (w/o &amp;quot;##&amp;quot;)&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Subword Length&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;# of ## Subwords&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;Text(0,0.5,&amp;#39;# of ## Subwords&amp;#39;)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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
&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Misspellings&quot;&gt;Misspellings&lt;a class=&quot;anchor-link&quot; href=&quot;#Misspellings&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;misspelled&amp;#39;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Right&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;False&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;mispelled&amp;#39;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Wrong&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;False&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;government&amp;#39;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Right&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;goverment&amp;#39;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Wrong&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;False&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;beginning&amp;#39;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Right&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;begining&amp;#39;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Wrong&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;False&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;separate&amp;#39;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Right&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;seperate&amp;#39;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Wrong&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;False&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;What about contractions?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;can&amp;#39;t&amp;quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;False&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;cant&amp;quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Start-vs.-Mid-Subwords&quot;&gt;Start vs. Mid Subwords&lt;a class=&quot;anchor-link&quot; href=&quot;#Start-vs.-Mid-Subwords&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;For single characters, there are both the individual character and the '##' version for every character. Is the same true of subwords?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# For each token in the vocabulary,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# If it&amp;#39;s a subword,&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;##&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Did not find a token for&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Did not find a token for ом
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;##ом&amp;#39;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;ом&amp;#39;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;False&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Names&quot;&gt;Names&lt;a class=&quot;anchor-link&quot; href=&quot;#Names&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install wget -q
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;wget&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Beginning file download with wget module&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;http://www.gutenberg.org/files/3201/files/NAMES.TXT&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wget&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;first-names.txt&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Beginning file download with wget module
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;&amp;#39;first-names (1).txt&amp;#39;&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Read them in.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;first-names.txt&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;names_encoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;readlines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
&lt;span class=&quot;n&quot;&gt;names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Decode the names, convert to lowercase, and strip newlines.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names_encoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rstrip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Number of nmaes: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Example:&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Number of nmaes: 21,985
Example: geis
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For each name in our list,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# If it&amp;#39;s in the vocab,&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Tally it.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; names in the vocabulary&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;4,493 names in the vocabulary
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Numbers&quot;&gt;Numbers&lt;a class=&quot;anchor-link&quot; href=&quot;#Numbers&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Count how many numbers are in the vocabulary.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For each token in the vocabulary,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Tally if it&amp;#39;s a number.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isdigit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Any numbers &amp;gt;= 10,000?&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Vocab includes &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; numbers.&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;092917
511228
87237
0815752199
1403962588
1884964311
1579583377
0415285569
0821419161
0691008000
0136120512
0062700553
0195082095
0684805332
0691152071
0816071365
300000
933346
0521820486
1852337520
10000
६३८५९६
000291
१६८२६
84433
9780521872386
9789004244870
00238
89800
12148
99947
Vocab includes 2,278 numbers.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Count how many dates between 1600 and 2021 are included.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2021&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Vocab includes &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; of 421 dates from 1600 - 2021&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Vocab includes 421 of 421 dates from 1600 - 2021
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Bert Notebook Blog Post(2)</title><link href="https://sparklingness.github.io/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html" rel="alternate" type="text/html" title="Bert Notebook Blog Post(2)" /><published>2020-04-11T00:00:00-05:00</published><updated>2020-04-11T00:00:00-05:00</updated><id>https://sparklingness.github.io/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial</id><content type="html" xml:base="https://sparklingness.github.io/blog/bert/jupyter/2020/04/11/Bert-Word-Embedding-Tutorial.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-11-Bert-Word-Embedding-Tutorial.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;ref: &lt;a href=&quot;http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/&quot;&gt;http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-Loading-Pre-Trained-BERT&quot;&gt;1. Loading Pre-Trained BERT&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-Loading-Pre-Trained-BERT&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; pip install pytorch-pretrained-bert
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Collecting pytorch-pretrained-bert
  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)
     |████████████████████████████████| 133kB 2.8MB/s 
Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.2)
Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.12.38)
Requirement already satisfied: torch&amp;gt;=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)
Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)
Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.38.0)
Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)
Requirement already satisfied: jmespath&amp;lt;1.0.0,&amp;gt;=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3-&amp;gt;pytorch-pretrained-bert) (0.9.5)
Requirement already satisfied: botocore&amp;lt;1.16.0,&amp;gt;=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3-&amp;gt;pytorch-pretrained-bert) (1.15.38)
Requirement already satisfied: s3transfer&amp;lt;0.4.0,&amp;gt;=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3-&amp;gt;pytorch-pretrained-bert) (0.3.3)
Requirement already satisfied: urllib3&amp;lt;1.25,&amp;gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&amp;gt;pytorch-pretrained-bert) (1.24.3)
Requirement already satisfied: idna&amp;lt;2.9,&amp;gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&amp;gt;pytorch-pretrained-bert) (2.8)
Requirement already satisfied: certifi&amp;gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&amp;gt;pytorch-pretrained-bert) (2020.4.5.1)
Requirement already satisfied: chardet&amp;lt;3.1.0,&amp;gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&amp;gt;pytorch-pretrained-bert) (3.0.4)
Requirement already satisfied: python-dateutil&amp;lt;3.0.0,&amp;gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore&amp;lt;1.16.0,&amp;gt;=1.15.38-&amp;gt;boto3-&amp;gt;pytorch-pretrained-bert) (2.8.1)
Requirement already satisfied: docutils&amp;lt;0.16,&amp;gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore&amp;lt;1.16.0,&amp;gt;=1.15.38-&amp;gt;boto3-&amp;gt;pytorch-pretrained-bert) (0.15.2)
Requirement already satisfied: six&amp;gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&amp;lt;3.0.0,&amp;gt;=2.1-&amp;gt;botocore&amp;lt;1.16.0,&amp;gt;=1.15.38-&amp;gt;boto3-&amp;gt;pytorch-pretrained-bert) (1.12.0)
Installing collected packages: pytorch-pretrained-bert
Successfully installed pytorch-pretrained-bert-0.6.2
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pytorch_pretrained_bert&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForMaskedLM&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# OPTIONAL: if you want to have more information on what&amp;#39;s happening, activate the logger as follows&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;logging&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# logging.basicConfig(level=logging.INFO)&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Load pre-trained model tokenizer (vocabulary)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;bert-base-uncased&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;100%|██████████| 231508/231508 [00:00&amp;lt;00:00, 315280.35B/s]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-Input-Formatting&quot;&gt;2. Input Formatting&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-Input-Formatting&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;2.1.-Special-Tokens&quot;&gt;2.1. Special Tokens&lt;a class=&quot;anchor-link&quot; href=&quot;#2.1.-Special-Tokens&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Special tokens to mark the beginning ([CLS]) and seprataion/end of sentences ([SEP])&lt;/p&gt;
&lt;p&gt;BERT can take as input either one or two sentences, and expects special tokens to mark the beginning and end of each one:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2 Sentence Input:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[CLS] The man wnent to the store. [SEP] He bought a gallon of milk. [SEP]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1 Sentence Input:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[CLS] The man went to the store. [SEP]&lt;/code&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;2.2.-Tokenization&quot;&gt;2.2. Tokenization&lt;a class=&quot;anchor-link&quot; href=&quot;#2.2.-Tokenization&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Here is the sentence I want embeddings for.&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;marked_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;[CLS] &amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot; [SEP]&amp;quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Tokenize our sentence with the BERT tokenizer.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenized_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marked_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print out the tokens.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenized_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;[&amp;#39;[CLS]&amp;#39;, &amp;#39;here&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;the&amp;#39;, &amp;#39;sentence&amp;#39;, &amp;#39;i&amp;#39;, &amp;#39;want&amp;#39;, &amp;#39;em&amp;#39;, &amp;#39;##bed&amp;#39;, &amp;#39;##ding&amp;#39;, &amp;#39;##s&amp;#39;, &amp;#39;for&amp;#39;, &amp;#39;.&amp;#39;, &amp;#39;[SEP]&amp;#39;]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Define a new example sentence with multiple meanings of the word &amp;quot;bank&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.&amp;quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add the special tokens.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;marked_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;[CLS] &amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot; [SEP]&amp;quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Split the sentence into tokens.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenized_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marked_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Map the token strings to their vocabulary indeces.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;indexed_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_tokens_to_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenized_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Display the words with their indeces.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tup&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenized_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indexed_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;lt;12}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{:&amp;gt;6,}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;[CLS]           101
after         2,044
stealing     11,065
money         2,769
from          2,013
the           1,996
bank          2,924
vault        11,632
,             1,010
the           1,996
bank          2,924
robber       27,307
was           2,001
seen          2,464
fishing       5,645
on            2,006
the           1,996
mississippi   5,900
river         2,314
bank          2,924
.             1,012
[SEP]           102
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;2.3.-Segment-ID&quot;&gt;2.3. Segment ID&lt;a class=&quot;anchor-link&quot; href=&quot;#2.3.-Segment-ID&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Mark each of the 22 tokens as belonging to sentence &amp;quot;1&amp;quot;.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;segments_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenized_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;segments_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-Extracting-Embeddings&quot;&gt;3. Extracting Embeddings&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-Extracting-Embeddings&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;3.1.-Running-BERT-on-our-text&quot;&gt;3.1. Running BERT on our text&lt;a class=&quot;anchor-link&quot; href=&quot;#3.1.-Running-BERT-on-our-text&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;code&gt;model.eval()&lt;/code&gt; evaluation mode turns off dropout regularization which is used in training&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Convert inputs to PyTorch tensors&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokens_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indexed_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;segments_tensors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;segments_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Load pre-trained model (weights)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;bert-base-uncased&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Put the model in &amp;quot;evaluation&amp;quot; mode, meaning feed-forward operation.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;tensor([[  101,  2044, 11065,  2769,  2013,  1996,  2924, 11632,  1010,  1996,
          2924, 27307,  2001,  2464,  5645,  2006,  1996,  5900,  2314,  2924,
          1012,   102]])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;100%|██████████| 407873900/407873900 [00:35&amp;lt;00:00, 11382845.79B/s]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): BertLayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (1): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (2): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (3): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (4): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (5): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (6): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (7): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (8): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (9): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (10): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (11): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;code&gt;torch.no_grad&lt;/code&gt; deactivates the gradient calculations, saves memory, and speeds up computation. (we don't need gradients or backpropagation since we're just running a forward pass).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Predict hidden states features for each layer&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;encoded_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;segments_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;3.2.-Understanding-the-Output&quot;&gt;3.2. Understanding the Output&lt;a class=&quot;anchor-link&quot; href=&quot;#3.2.-Understanding-the-Output&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Number of layers:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;layer_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Number of batches:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batch_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Number of tokens:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;token_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Number of hidden units:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Number of layers: 12
Number of batches: 1
Number of tokens: 22
Number of hidden units: 768
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# For the 5th token in our sentence, select its feature values from layer 5.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;token_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;layer_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoded_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plot the values as a histogram to show their distribution.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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
&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# `encoded_layers` is a Python list.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;     Type of encoded_layers: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Each layer in the list is a torch tensor.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Tensor shape for each layer: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoded_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;     Type of encoded_layers:  &amp;lt;class &amp;#39;list&amp;#39;&amp;gt; 12
Tensor shape for each layer:  torch.Size([1, 22, 768])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Concatenate the tensors for all layers. We use `stack` here to &lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# create a new dimension in the tensor.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;token_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;token_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([12, 1, 22, 768])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Remove dimension 1, the &amp;quot;batches&amp;quot;.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;token_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;token_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([12, 22, 768])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Swap dimensions 0 and 1.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;token_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;token_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([22, 12, 768])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;3.3.-Creating-word-and-sentence-vectors-from-hidden-states&quot;&gt;3.3. Creating word and sentence vectors from hidden states&lt;a class=&quot;anchor-link&quot; href=&quot;#3.3.-Creating-word-and-sentence-vectors-from-hidden-states&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Word Vectors&lt;/strong&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Stores the token vectors, with shape [22 x 3,072] (4 x 768 = 3,072)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;token_vecs_cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# `token_embeddings` is a [22 x 12 x 768] tensor.&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For each token in the sentence...&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# `token` is a [12 x 768] tensor&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Concatenate the vectors (that is, append them together) from the last four layers.&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Each layer vector is 768 values, so `cat_vec` is length 3,072.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cat_vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Use `cat_vec` to represent `token`.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;token_vecs_cat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat_vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Shape is: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; x &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_vecs_cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_vecs_cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Shape is: 22 x 3072
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Stores the token vectors, with shape [22 x 768]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;token_vecs_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# `token_embeddings` is a [22 x 12 x 768] tensor.&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For each token in the sentence...&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# `token` is a [12 x 768] tensor&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Sum the vectors from the last four layers.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sum_vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Use `sum_vec` to represent `token`.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;token_vecs_sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum_vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Shape is: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; x &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_vecs_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_vecs_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Shape is: 22 x 768
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Sentence Vectors&lt;/strong&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# `encoded_layers` has shape [12 x 1 x 22 x 768]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# `token_vecs` is a tensor with shape [22 x 768]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;token_vecs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoded_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Calculate the average of all 22 token vectors.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sentence_embedding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_vecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Our final sentence embedding vector of shape:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence_embedding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Our final sentence embedding vector of shape: torch.Size([768])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;3.4.-Confirming-contextually-dependent-vectors&quot;&gt;3.4. Confirming contextually dependent vectors&lt;a class=&quot;anchor-link&quot; href=&quot;#3.4.-Confirming-contextually-dependent-vectors&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token_str&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenized_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token_str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;0 [CLS]
1 after
2 stealing
3 money
4 from
5 the
6 bank
7 vault
8 ,
9 the
10 bank
11 robber
12 was
13 seen
14 fishing
15 on
16 the
17 mississippi
18 river
19 bank
20 .
21 [SEP]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;First 5 vector values for each instance of &amp;quot;bank&amp;quot;.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;bank vault   &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_vecs_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;bank robber  &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_vecs_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;river bank   &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_vecs_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;First 5 vector values for each instance of &amp;#34;bank&amp;#34;.

bank vault    tensor([ 2.1319, -2.1413, -1.6260,  0.8638,  3.3173])
bank robber   tensor([ 1.1868, -1.5298, -1.3770,  1.0648,  3.1446])
river bank    tensor([ 1.1295, -1.4725, -0.7296, -0.0901,  2.4970])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.spatial.distance&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Calculate the cosine similarity between the word bank&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# in &amp;quot;bank robber&amp;quot; vs &amp;quot;river bank&amp;quot; (different meanings).&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;diff_bank&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_vecs_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token_vecs_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Calculate the cosine similarity between the word bank&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# in &amp;quot;bank robber&amp;quot; vs &amp;quot;bank vault&amp;quot; (same meaning).&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;same_bank&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token_vecs_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token_vecs_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Vector similarity for  *similar*  meanings: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;same_bank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Vector similarity for *different* meanings: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2f&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff_bank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Vector similarity for  *similar*  meanings: 0.95
Vector similarity for *different* meanings: 0.68
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>